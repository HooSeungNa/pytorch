{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch.autograd import Variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "latent_size = 100\n",
    "number_g_size =64\n",
    "number_d_size=64\n",
    "num_epochs = 200\n",
    "batch_size = 200\n",
    "sample_dir = 'samples/SGAN/'\n",
    "saver_dir = 'saved_data/SGAN/'\n",
    "\n",
    "\n",
    "# Create a directory if not exists\n",
    "if not os.path.exists(sample_dir):\n",
    "    os.makedirs(sample_dir)\n",
    "    print(\"created folder\")\n",
    "if not os.path.exists(saver_dir):\n",
    "    os.makedirs(saver_dir)\n",
    "    print(\"created folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "#Image processing\n",
    "transform = transforms.Compose([\n",
    "                transforms.Resize((32,32)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=(0.5, 0.5, 0.5),   # 3 for RGB channels\n",
    "                                     std=(0.5, 0.5, 0.5))])\n",
    "train_dataset=torchvision.datasets.MNIST(root='../../data',\n",
    "                                         train=True,\n",
    "                                         transform=transform)\n",
    "# Data loader (input pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "print(len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator,self).__init__()\n",
    "        self.layer1=nn.Sequential(\n",
    "            #layer1 32*32\n",
    "            nn.Conv2d(1,number_d_size,\n",
    "                      4, 2, 1,bias=False),   \n",
    "#             nn.BatchNorm2d(number_d_size),\n",
    "            nn.LeakyReLU(0.2,inplace=True),\n",
    "            \n",
    "            \n",
    "            #layer3 16*16\n",
    "            nn.Conv2d(number_d_size,number_d_size*2,\n",
    "                      4, 2, 1, bias=False),   \n",
    "            nn.BatchNorm2d(number_d_size*2),\n",
    "            nn.LeakyReLU(0.2,inplace=True),\n",
    "            \n",
    "            #layer4 8*8\n",
    "            nn.Conv2d(number_d_size*2,number_d_size*4,\n",
    "                      4, 2, 1, bias=False),   \n",
    "            nn.BatchNorm2d(number_d_size*4),\n",
    "            nn.LeakyReLU(0.2,inplace=True),\n",
    "            \n",
    "        )\n",
    "        self.adv_layer=nn.Sequential(\n",
    "            \n",
    "            nn.Linear(number_d_size*4*4*4,1),\n",
    "            nn.Sigmoid())\n",
    "        self.aux_layer=nn.Sequential(\n",
    "            nn.Linear(number_d_size*4*4*4,11),\n",
    "            nn.Softmax())\n",
    "    def forward(self,x):\n",
    "        out=self.layer1(x)\n",
    "        out=out.view(batch_size,-1)\n",
    "        validity=self.adv_layer(out)\n",
    "        label=self.aux_layer(out)\n",
    "        return validity,label\n",
    "    \n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator,self).__init__()\n",
    "        self.layer1=nn.Sequential(\n",
    "            #4*4\n",
    "            nn.ConvTranspose2d(latent_size,number_g_size*4,\n",
    "                               4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(number_g_size*4),\n",
    "            nn.ReLU(),\n",
    "            #8*8\n",
    "            nn.ConvTranspose2d(number_g_size*4,number_g_size*2,\n",
    "                               4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(number_g_size*2),\n",
    "            nn.ReLU(),\n",
    "            #16*16\n",
    "            nn.ConvTranspose2d(number_g_size*2,number_g_size,\n",
    "                               4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(number_g_size),\n",
    "            nn.ReLU(),\n",
    "            #32*32\n",
    "            nn.ConvTranspose2d(number_g_size,1,\n",
    "                               4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        out=self.layer1(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator=Discriminator().cuda()\n",
    "generator=Generator().cuda()\n",
    "#loss function\n",
    "adversarial_loss = torch.nn.BCELoss().cuda()\n",
    "auxiliary_loss = torch.nn.CrossEntropyLoss().cuda()\n",
    "# setup optimizer\n",
    "optimizerD = torch.optim.Adam(discriminator.parameters(), lr=2e-4)\n",
    "optimizerG = torch.optim.Adam(generator.parameters(), lr=2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denorm(x):\n",
    "    out=(x+1)/2\n",
    "    return out.clamp(0,1)\n",
    "def reset_grad():\n",
    "    optimizerD.zero_grad()\n",
    "    optimizerG.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Robotmedia9\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py:91: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/200], Step [200/300], d_loss: 0.8456, g_loss: 4.2643\n",
      "Epoch [1/200], Step [200/300], d_loss: 0.7939, g_loss: 5.7963\n",
      "Epoch [2/200], Step [200/300], d_loss: 0.7901, g_loss: 6.0817\n",
      "Epoch [3/200], Step [200/300], d_loss: 0.7953, g_loss: 4.6891\n",
      "Epoch [4/200], Step [200/300], d_loss: 0.7829, g_loss: 5.3314\n",
      "Epoch [5/200], Step [200/300], d_loss: 0.7857, g_loss: 4.8865\n",
      "Epoch [6/200], Step [200/300], d_loss: 0.7792, g_loss: 6.7758\n",
      "Epoch [7/200], Step [200/300], d_loss: 0.7956, g_loss: 4.3497\n",
      "Epoch [8/200], Step [200/300], d_loss: 0.7882, g_loss: 5.2386\n",
      "Epoch [9/200], Step [200/300], d_loss: 0.7984, g_loss: 3.5530\n",
      "Epoch [10/200], Step [200/300], d_loss: 0.7927, g_loss: 5.2008\n",
      "Epoch [11/200], Step [200/300], d_loss: 0.8236, g_loss: 2.8420\n",
      "Epoch [12/200], Step [200/300], d_loss: 0.8040, g_loss: 4.7949\n",
      "Epoch [13/200], Step [200/300], d_loss: 0.8302, g_loss: 2.6568\n",
      "Epoch [14/200], Step [200/300], d_loss: 0.8965, g_loss: 4.4326\n",
      "Epoch [15/200], Step [200/300], d_loss: 0.8118, g_loss: 4.5898\n",
      "Epoch [16/200], Step [200/300], d_loss: 0.8475, g_loss: 2.4233\n",
      "Epoch [17/200], Step [200/300], d_loss: 0.8173, g_loss: 2.9309\n",
      "Epoch [18/200], Step [200/300], d_loss: 0.9232, g_loss: 1.6502\n",
      "Epoch [19/200], Step [200/300], d_loss: 0.8060, g_loss: 4.2920\n",
      "Epoch [20/200], Step [200/300], d_loss: 0.8505, g_loss: 2.7536\n",
      "Epoch [21/200], Step [200/300], d_loss: 0.8131, g_loss: 3.0021\n",
      "Epoch [22/200], Step [200/300], d_loss: 0.7989, g_loss: 3.6765\n",
      "Epoch [23/200], Step [200/300], d_loss: 0.8359, g_loss: 3.1371\n",
      "Epoch [24/200], Step [200/300], d_loss: 0.8276, g_loss: 4.6513\n",
      "Epoch [25/200], Step [200/300], d_loss: 0.8419, g_loss: 3.2119\n",
      "Epoch [26/200], Step [200/300], d_loss: 0.8453, g_loss: 5.3328\n",
      "Epoch [27/200], Step [200/300], d_loss: 0.8109, g_loss: 3.6574\n",
      "Epoch [28/200], Step [200/300], d_loss: 0.8100, g_loss: 4.4843\n",
      "Epoch [29/200], Step [200/300], d_loss: 0.8045, g_loss: 3.6099\n",
      "Epoch [30/200], Step [200/300], d_loss: 0.7985, g_loss: 3.7655\n",
      "Epoch [31/200], Step [200/300], d_loss: 0.7823, g_loss: 4.7011\n",
      "Epoch [32/200], Step [200/300], d_loss: 0.8005, g_loss: 5.1205\n",
      "Epoch [33/200], Step [200/300], d_loss: 0.7954, g_loss: 4.6163\n",
      "Epoch [34/200], Step [200/300], d_loss: 0.8519, g_loss: 7.2898\n",
      "Epoch [35/200], Step [200/300], d_loss: 0.7930, g_loss: 4.9538\n",
      "Epoch [36/200], Step [200/300], d_loss: 0.8059, g_loss: 4.4722\n",
      "Epoch [37/200], Step [200/300], d_loss: 0.7979, g_loss: 3.3678\n",
      "Epoch [38/200], Step [200/300], d_loss: 0.7932, g_loss: 3.7366\n",
      "Epoch [39/200], Step [200/300], d_loss: 0.8147, g_loss: 3.6862\n",
      "Epoch [40/200], Step [200/300], d_loss: 0.7816, g_loss: 4.9277\n",
      "Epoch [41/200], Step [200/300], d_loss: 0.7956, g_loss: 5.4475\n",
      "Epoch [42/200], Step [200/300], d_loss: 0.7832, g_loss: 6.2359\n",
      "Epoch [43/200], Step [200/300], d_loss: 0.7790, g_loss: 6.0026\n",
      "Epoch [44/200], Step [200/300], d_loss: 0.7825, g_loss: 4.6926\n",
      "Epoch [45/200], Step [200/300], d_loss: 0.8418, g_loss: 3.4359\n",
      "Epoch [46/200], Step [200/300], d_loss: 0.8028, g_loss: 3.7506\n",
      "Epoch [47/200], Step [200/300], d_loss: 0.7805, g_loss: 4.6360\n",
      "Epoch [48/200], Step [200/300], d_loss: 0.7814, g_loss: 5.3076\n",
      "Epoch [49/200], Step [200/300], d_loss: 0.7786, g_loss: 5.3988\n",
      "Epoch [50/200], Step [200/300], d_loss: 0.7756, g_loss: 5.8559\n",
      "Epoch [51/200], Step [200/300], d_loss: 0.7788, g_loss: 5.3718\n",
      "Epoch [52/200], Step [200/300], d_loss: 0.7751, g_loss: 5.5039\n",
      "Epoch [53/200], Step [200/300], d_loss: 0.7744, g_loss: 6.3664\n",
      "Epoch [54/200], Step [200/300], d_loss: 0.7767, g_loss: 5.8015\n",
      "Epoch [55/200], Step [200/300], d_loss: 1.0322, g_loss: 6.2268\n",
      "Epoch [56/200], Step [200/300], d_loss: 0.8030, g_loss: 4.9611\n",
      "Epoch [57/200], Step [200/300], d_loss: 0.7828, g_loss: 5.7276\n",
      "Epoch [58/200], Step [200/300], d_loss: 0.7767, g_loss: 6.1258\n",
      "Epoch [59/200], Step [200/300], d_loss: 0.7764, g_loss: 5.2714\n",
      "Epoch [60/200], Step [200/300], d_loss: 0.7755, g_loss: 6.5839\n",
      "Epoch [61/200], Step [200/300], d_loss: 0.7796, g_loss: 4.8586\n",
      "Epoch [62/200], Step [200/300], d_loss: 0.7753, g_loss: 5.6242\n",
      "Epoch [63/200], Step [200/300], d_loss: 0.7756, g_loss: 6.7684\n",
      "Epoch [64/200], Step [200/300], d_loss: 0.7753, g_loss: 5.5003\n",
      "Epoch [65/200], Step [200/300], d_loss: 0.8006, g_loss: 4.8226\n",
      "Epoch [66/200], Step [200/300], d_loss: 0.8439, g_loss: 2.8010\n",
      "Epoch [67/200], Step [200/300], d_loss: 1.0040, g_loss: 2.4093\n",
      "Epoch [68/200], Step [200/300], d_loss: 0.8743, g_loss: 3.4879\n",
      "Epoch [69/200], Step [200/300], d_loss: 0.8064, g_loss: 2.9479\n",
      "Epoch [70/200], Step [200/300], d_loss: 0.7905, g_loss: 4.9050\n",
      "Epoch [71/200], Step [200/300], d_loss: 0.8103, g_loss: 4.6402\n",
      "Epoch [72/200], Step [200/300], d_loss: 0.7825, g_loss: 4.7917\n",
      "Epoch [73/200], Step [200/300], d_loss: 0.7784, g_loss: 5.3238\n",
      "Epoch [74/200], Step [200/300], d_loss: 0.7788, g_loss: 5.5532\n",
      "Epoch [75/200], Step [200/300], d_loss: 0.7754, g_loss: 5.6009\n",
      "Epoch [76/200], Step [200/300], d_loss: 0.8278, g_loss: 5.8681\n",
      "Epoch [77/200], Step [200/300], d_loss: 0.7825, g_loss: 5.2648\n",
      "Epoch [78/200], Step [200/300], d_loss: 0.7781, g_loss: 5.1704\n",
      "Epoch [79/200], Step [200/300], d_loss: 0.7758, g_loss: 6.2326\n",
      "Epoch [80/200], Step [200/300], d_loss: 0.7754, g_loss: 6.4681\n",
      "Epoch [81/200], Step [200/300], d_loss: 0.7735, g_loss: 6.5257\n",
      "Epoch [82/200], Step [200/300], d_loss: 0.8947, g_loss: 3.2236\n",
      "Epoch [83/200], Step [200/300], d_loss: 0.8107, g_loss: 3.5863\n",
      "Epoch [84/200], Step [200/300], d_loss: 0.7929, g_loss: 3.5445\n",
      "Epoch [85/200], Step [200/300], d_loss: 0.7808, g_loss: 5.7170\n",
      "Epoch [86/200], Step [200/300], d_loss: 0.7790, g_loss: 5.8903\n",
      "Epoch [87/200], Step [200/300], d_loss: 0.7744, g_loss: 6.8303\n",
      "Epoch [88/200], Step [200/300], d_loss: 0.7760, g_loss: 6.8444\n",
      "Epoch [89/200], Step [200/300], d_loss: 0.7744, g_loss: 6.3101\n",
      "Epoch [90/200], Step [200/300], d_loss: 0.7761, g_loss: 7.0908\n",
      "Epoch [91/200], Step [200/300], d_loss: 0.7747, g_loss: 6.7090\n",
      "Epoch [92/200], Step [200/300], d_loss: 0.7747, g_loss: 6.4813\n",
      "Epoch [93/200], Step [200/300], d_loss: 0.7736, g_loss: 8.1054\n",
      "Epoch [94/200], Step [200/300], d_loss: 0.7749, g_loss: 8.0028\n",
      "Epoch [95/200], Step [200/300], d_loss: 0.7727, g_loss: 7.3661\n",
      "Epoch [96/200], Step [200/300], d_loss: 0.8076, g_loss: 3.5773\n",
      "Epoch [97/200], Step [200/300], d_loss: 0.8523, g_loss: 5.2990\n",
      "Epoch [98/200], Step [200/300], d_loss: 0.9354, g_loss: 1.4273\n",
      "Epoch [99/200], Step [200/300], d_loss: 0.7807, g_loss: 4.6940\n",
      "Epoch [100/200], Step [200/300], d_loss: 0.7766, g_loss: 6.8689\n",
      "Epoch [101/200], Step [200/300], d_loss: 0.7745, g_loss: 5.9523\n",
      "Epoch [102/200], Step [200/300], d_loss: 0.7737, g_loss: 6.4098\n",
      "Epoch [103/200], Step [200/300], d_loss: 0.7770, g_loss: 6.8647\n",
      "Epoch [104/200], Step [200/300], d_loss: 0.7731, g_loss: 7.3601\n",
      "Epoch [105/200], Step [200/300], d_loss: 0.7739, g_loss: 6.2323\n",
      "Epoch [106/200], Step [200/300], d_loss: 1.1166, g_loss: 1.1856\n",
      "Epoch [107/200], Step [200/300], d_loss: 0.7910, g_loss: 4.3023\n",
      "Epoch [108/200], Step [200/300], d_loss: 0.7775, g_loss: 6.1470\n",
      "Epoch [109/200], Step [200/300], d_loss: 0.7737, g_loss: 7.1984\n",
      "Epoch [110/200], Step [200/300], d_loss: 0.7739, g_loss: 7.3509\n",
      "Epoch [111/200], Step [200/300], d_loss: 0.7741, g_loss: 7.7086\n",
      "Epoch [112/200], Step [200/300], d_loss: 0.7721, g_loss: 7.9326\n",
      "Epoch [113/200], Step [200/300], d_loss: 0.7724, g_loss: 7.0713\n",
      "Epoch [114/200], Step [200/300], d_loss: 0.7724, g_loss: 7.7170\n",
      "Epoch [115/200], Step [200/300], d_loss: 0.7721, g_loss: 7.9858\n",
      "Epoch [116/200], Step [200/300], d_loss: 0.7720, g_loss: 8.6230\n",
      "Epoch [117/200], Step [200/300], d_loss: 0.7720, g_loss: 9.2459\n",
      "Epoch [118/200], Step [200/300], d_loss: 0.7730, g_loss: 8.6662\n",
      "Epoch [119/200], Step [200/300], d_loss: 0.7720, g_loss: 8.5748\n",
      "Epoch [120/200], Step [200/300], d_loss: 7.9284, g_loss: 0.0000\n",
      "Epoch [121/200], Step [200/300], d_loss: 7.9292, g_loss: 0.0000\n",
      "Epoch [122/200], Step [200/300], d_loss: 7.9277, g_loss: 0.0000\n",
      "Epoch [123/200], Step [200/300], d_loss: 7.9268, g_loss: 0.0000\n",
      "Epoch [124/200], Step [200/300], d_loss: 7.9256, g_loss: 0.0000\n",
      "Epoch [125/200], Step [200/300], d_loss: 7.9254, g_loss: 0.0000\n",
      "Epoch [126/200], Step [200/300], d_loss: 7.9238, g_loss: 0.0000\n",
      "Epoch [127/200], Step [200/300], d_loss: 7.9240, g_loss: 0.0000\n",
      "Epoch [128/200], Step [200/300], d_loss: 7.9206, g_loss: 0.0000\n",
      "Epoch [129/200], Step [200/300], d_loss: 7.9201, g_loss: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [130/200], Step [200/300], d_loss: 7.9209, g_loss: 0.0000\n",
      "Epoch [131/200], Step [200/300], d_loss: 7.9206, g_loss: 0.0000\n",
      "Epoch [132/200], Step [200/300], d_loss: 7.9193, g_loss: 0.0000\n",
      "Epoch [133/200], Step [200/300], d_loss: 7.9186, g_loss: 0.0000\n",
      "Epoch [134/200], Step [200/300], d_loss: 7.9187, g_loss: 0.0000\n",
      "Epoch [135/200], Step [200/300], d_loss: 7.9184, g_loss: 0.0000\n",
      "Epoch [136/200], Step [200/300], d_loss: 7.9180, g_loss: 0.0000\n",
      "Epoch [137/200], Step [200/300], d_loss: 7.9183, g_loss: 0.0000\n",
      "Epoch [138/200], Step [200/300], d_loss: 7.9192, g_loss: 0.0000\n",
      "Epoch [139/200], Step [200/300], d_loss: 0.8308, g_loss: 4.7578\n",
      "Epoch [140/200], Step [200/300], d_loss: 0.7879, g_loss: 4.4261\n",
      "Epoch [141/200], Step [200/300], d_loss: 0.7884, g_loss: 4.0265\n",
      "Epoch [142/200], Step [200/300], d_loss: 0.7839, g_loss: 5.3882\n",
      "Epoch [143/200], Step [200/300], d_loss: 0.7779, g_loss: 5.5036\n",
      "Epoch [144/200], Step [200/300], d_loss: 0.7787, g_loss: 6.0919\n",
      "Epoch [145/200], Step [200/300], d_loss: 0.7741, g_loss: 5.8546\n",
      "Epoch [146/200], Step [200/300], d_loss: 0.7733, g_loss: 6.6966\n",
      "Epoch [147/200], Step [200/300], d_loss: 0.7751, g_loss: 5.8361\n",
      "Epoch [148/200], Step [200/300], d_loss: 0.8020, g_loss: 3.9246\n",
      "Epoch [149/200], Step [200/300], d_loss: 0.7768, g_loss: 5.7341\n",
      "Epoch [150/200], Step [200/300], d_loss: 0.7738, g_loss: 7.0141\n",
      "Epoch [151/200], Step [200/300], d_loss: 0.7734, g_loss: 6.7437\n",
      "Epoch [152/200], Step [200/300], d_loss: 0.7757, g_loss: 6.7204\n",
      "Epoch [153/200], Step [200/300], d_loss: 0.7733, g_loss: 7.2966\n",
      "Epoch [154/200], Step [200/300], d_loss: 0.7731, g_loss: 7.7888\n",
      "Epoch [155/200], Step [200/300], d_loss: 0.7729, g_loss: 7.4708\n",
      "Epoch [156/200], Step [200/300], d_loss: 0.8575, g_loss: 3.9094\n",
      "Epoch [157/200], Step [200/300], d_loss: 0.9211, g_loss: 6.5449\n",
      "Epoch [158/200], Step [200/300], d_loss: 0.8538, g_loss: 2.4953\n",
      "Epoch [159/200], Step [200/300], d_loss: 0.7973, g_loss: 4.1198\n",
      "Epoch [160/200], Step [200/300], d_loss: 0.7844, g_loss: 4.3731\n",
      "Epoch [161/200], Step [200/300], d_loss: 0.7767, g_loss: 5.4607\n",
      "Epoch [162/200], Step [200/300], d_loss: 0.7739, g_loss: 7.1738\n",
      "Epoch [163/200], Step [200/300], d_loss: 0.7727, g_loss: 7.2235\n",
      "Epoch [164/200], Step [200/300], d_loss: 0.7727, g_loss: 8.0017\n",
      "Epoch [165/200], Step [200/300], d_loss: 0.7738, g_loss: 7.8650\n",
      "Epoch [166/200], Step [200/300], d_loss: 0.7724, g_loss: 7.7660\n",
      "Epoch [167/200], Step [200/300], d_loss: 0.7721, g_loss: 7.6976\n",
      "Epoch [168/200], Step [200/300], d_loss: 0.7724, g_loss: 7.1664\n",
      "Epoch [169/200], Step [200/300], d_loss: 0.7719, g_loss: 8.6716\n",
      "Epoch [170/200], Step [200/300], d_loss: 0.8080, g_loss: 4.1945\n",
      "Epoch [171/200], Step [200/300], d_loss: 0.7917, g_loss: 5.8435\n",
      "Epoch [172/200], Step [200/300], d_loss: 0.7802, g_loss: 5.3541\n",
      "Epoch [173/200], Step [200/300], d_loss: 0.7764, g_loss: 6.1094\n",
      "Epoch [174/200], Step [200/300], d_loss: 0.7742, g_loss: 7.1965\n",
      "Epoch [175/200], Step [200/300], d_loss: 0.7728, g_loss: 7.6912\n",
      "Epoch [176/200], Step [200/300], d_loss: 0.7740, g_loss: 7.2513\n",
      "Epoch [177/200], Step [200/300], d_loss: 0.7722, g_loss: 8.7015\n",
      "Epoch [178/200], Step [200/300], d_loss: 0.7717, g_loss: 8.8681\n",
      "Epoch [179/200], Step [200/300], d_loss: 0.7718, g_loss: 8.9724\n",
      "Epoch [180/200], Step [200/300], d_loss: 0.7717, g_loss: 9.4567\n",
      "Epoch [181/200], Step [200/300], d_loss: 0.7718, g_loss: 8.9280\n",
      "Epoch [182/200], Step [200/300], d_loss: 0.7718, g_loss: 9.1146\n",
      "Epoch [183/200], Step [200/300], d_loss: 0.7895, g_loss: 5.4468\n",
      "Epoch [184/200], Step [200/300], d_loss: 0.7793, g_loss: 5.2364\n",
      "Epoch [185/200], Step [200/300], d_loss: 0.7741, g_loss: 7.0488\n",
      "Epoch [186/200], Step [200/300], d_loss: 0.7747, g_loss: 7.6601\n",
      "Epoch [187/200], Step [200/300], d_loss: 0.7731, g_loss: 6.6339\n",
      "Epoch [188/200], Step [200/300], d_loss: 0.7727, g_loss: 7.0255\n",
      "Epoch [189/200], Step [200/300], d_loss: 0.7723, g_loss: 7.9740\n",
      "Epoch [190/200], Step [200/300], d_loss: 0.7736, g_loss: 8.5209\n",
      "Epoch [191/200], Step [200/300], d_loss: 0.7722, g_loss: 8.3841\n",
      "Epoch [192/200], Step [200/300], d_loss: 0.7723, g_loss: 7.5745\n",
      "Epoch [193/200], Step [200/300], d_loss: 0.7720, g_loss: 8.7319\n",
      "Epoch [194/200], Step [200/300], d_loss: 0.7733, g_loss: 8.0171\n",
      "Epoch [195/200], Step [200/300], d_loss: 0.7722, g_loss: 10.0230\n",
      "Epoch [196/200], Step [200/300], d_loss: 0.7729, g_loss: 7.2972\n",
      "Epoch [197/200], Step [200/300], d_loss: 0.7719, g_loss: 9.0224\n",
      "Epoch [198/200], Step [200/300], d_loss: 0.7717, g_loss: 9.9157\n",
      "Epoch [199/200], Step [200/300], d_loss: 0.7717, g_loss: 9.5537\n",
      "training finished! 5402 minutes\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "# optimizerD.load_state_dict(torch.load('./saved_data/cat_dcgan/D_cifar_151.ckpt'))\n",
    "# optimizerG.load_state_dict(torch.load('./saved_data/cat_dcgan/G_cifar_151.ckpt'))\n",
    "start=time.time()\n",
    "total_step=len(train_loader)\n",
    "for epochs in range(num_epochs):\n",
    "    for i, (images,labels)in enumerate(train_loader):\n",
    "        images=Variable(images).cuda()\n",
    "        labels=Variable(labels.type(torch.LongTensor)).cuda()\n",
    "        \n",
    "        real_labels =Variable(torch.FloatTensor(batch_size, 1).fill_(1.0)).cuda()\n",
    "        fake_labels =Variable(torch.FloatTensor(batch_size, 1).fill_(0.0)).cuda()\n",
    "        fake_aux_gt =Variable(torch.LongTensor(batch_size).fill_(10)).cuda()\n",
    "        \n",
    "        ##generator##\n",
    "        optimizerG.zero_grad()\n",
    "        fixed_noise = torch.randn(batch_size,latent_size , 1, 1).cuda()\n",
    "        fake_images = generator.forward(fixed_noise)\n",
    "        validity,_ = discriminator.forward(fake_images)\n",
    "#         print(validity.shape,real_labels.shape)\n",
    "\n",
    "        #생성한 이미지를 참으로 받아들이도록 학습\n",
    "        g_loss = adversarial_loss(validity, real_labels)\n",
    "        \n",
    "        # Backprop and optimize\n",
    "        g_loss.backward(retain_graph=True )\n",
    "        optimizerG.step()\n",
    "        \n",
    "        \n",
    "        ##discriminator##\n",
    "        optimizerD.zero_grad()\n",
    "        \n",
    "        real_pred,real_aux=discriminator(images)\n",
    "#         print(\"real image : \",real_aux.type(),labels.type())\n",
    "        \n",
    "        d_real_loss =  (adversarial_loss(real_pred, real_labels) +auxiliary_loss(real_aux, labels)) / 2\n",
    "        # Loss for fake images\n",
    "        fake_pred, fake_aux = discriminator(fake_images)\n",
    "#         print(\"fake image : \",fake_pred.shape,fake_aux.shape,fake_aux_gt.shape)\n",
    "        d_fake_loss =  (adversarial_loss(fake_pred, fake_labels) +auxiliary_loss(fake_aux, fake_aux_gt)) / 2\n",
    "        \n",
    "        # Total discriminator loss\n",
    "        d_loss = (d_real_loss + d_fake_loss) / 2\n",
    "\n",
    "        d_loss.backward(retain_graph=True)\n",
    "        optimizerD.step()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        if (i+1) % 200 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], d_loss: {:.4f}, g_loss: {:.4f}'\n",
    "                  .format(epochs, num_epochs, i+1, total_step, d_loss.item(), g_loss.item()))\n",
    "    \n",
    "    # Save real images\n",
    "    if(epochs+1) == 1:\n",
    "        save_image(denorm(images), os.path.join(sample_dir, 'real_images.png'))\n",
    "    \n",
    "    # Save sampled images\n",
    "#     fake_images = fake_images.reshape(fake_images.size(0), 3, 32, 32)\n",
    "    save_image(denorm(fake_images), os.path.join(sample_dir, 'fake_images-{}.png'.format(epochs+1)))\n",
    "    if epochs%50==0:        \n",
    "        # Save the model checkpoints \n",
    "        torch.save(optimizerG.state_dict(), saver_dir+'/G_sgan_{}.ckpt'.format(epochs+1))\n",
    "        torch.save(optimizerD.state_dict(), saver_dir+'/D_sgan_{}.ckpt'.format(epochs+1)) \n",
    "finished=time.time()\n",
    "hours=finished-start\n",
    "print(\"training finished! %d minutes\"%hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset,DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=1e-5\n",
    "batch_size=1\n",
    "num_epochs=40\n",
    "num_classes=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training data test data loading\n",
    "train_dirr=\"D:/emotion_dataset/mmidataset/VideoWithImageLabels/face_detected/train/labels.csv\"\n",
    "train_dataset=ImageDataset(csv_file=train_dirr\n",
    "                           ,transform=ToTensor())\n",
    "train_loader=DataLoader(train_dataset,batch_size=1,shuffle=False)\n",
    "\n",
    "test_dirr=\"D:/emotion_dataset/mmidataset/VideoWithImageLabels/face_detected/test/labels.csv\"\n",
    "test_dataset=ImageDataset(csv_file=test_dirr\n",
    "                           ,transform=ToTensor())\n",
    "test_loader=DataLoader(test_dataset,batch_size=1,shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet,self).__init__()\n",
    "        self.conv1=nn.Sequential(nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=1),\n",
    "                                nn.BatchNorm2d(64),\n",
    "                                nn.ReLU(),\n",
    "                                nn.MaxPool2d(kernel_size=3,stride=2))\n",
    "        \n",
    "        #res2a\n",
    "        self.res2a_branch1=nn.Sequential(nn.Conv2d(64,256,kernel_size=1,stride=1,padding=0),\n",
    "                                         nn.BatchNorm2d(256))\n",
    "        self.res2a_branch2a=nn.Sequential(nn.Conv2d(64,64,kernel_size=1,stride=1,padding=0),\n",
    "                                         nn.BatchNorm2d(64),\n",
    "                                         nn.ReLU())\n",
    "        self.res2a_branch2b=nn.Sequential(nn.Conv2d(64,64,kernel_size=3,padding=1,stride=1),\n",
    "                                         nn.ReLU())\n",
    "        self.res2a_branch2c=nn.Sequential(nn.Conv2d(64,256,kernel_size=1,padding=0,stride=1),\n",
    "                                         nn.BatchNorm2d(256))\n",
    "        \n",
    "        #res2b\n",
    "        self.res2b_branch2a=nn.Sequential(nn.Conv2d(512,64,kernel_size=1,stride=1,padding=0),\n",
    "                                         nn.BatchNorm2d(64),\n",
    "                                         nn.ReLU())\n",
    "        self.res2b_branch2b=nn.Sequential(nn.Conv2d(64,64,kernel_size=3,padding=1,stride=1),\n",
    "                                         nn.BatchNorm2d(64),\n",
    "                                         nn.ReLU())\n",
    "        self.res2b_branch2c=nn.Sequential(nn.Conv2d(64,256,kernel_size=1,padding=0,stride=1),\n",
    "                                         nn.BatchNorm2d(256))\n",
    "        \n",
    "        #res2c\n",
    "        self.res2c_branch2a=nn.Sequential(nn.Conv2d(768,64,kernel_size=1,stride=1,padding=0),\n",
    "                                         nn.BatchNorm2d(64),\n",
    "                                         nn.ReLU())\n",
    "        self.res2c_branch2b=nn.Sequential(nn.Conv2d(64,64,kernel_size=3,padding=1,stride=1),\n",
    "                                         nn.BatchNorm2d(64),\n",
    "                                         nn.ReLU())\n",
    "        self.res2c_branch2c=nn.Sequential(nn.Conv2d(64,256,kernel_size=1,padding=0,stride=1),\n",
    "                                         nn.BatchNorm2d(256))\n",
    "        \n",
    "        #res3a\n",
    "        self.res3a_branch1=nn.Sequential(nn.Conv2d(1024,512,kernel_size=1,stride=2,padding=0),\n",
    "                                         nn.BatchNorm2d(512))\n",
    "        self.res3a_branch2a=nn.Sequential(nn.Conv2d(1024,128,kernel_size=1,stride=2,padding=0),\n",
    "                                         nn.BatchNorm2d(128),\n",
    "                                         nn.ReLU())\n",
    "        self.res3a_branch2b=nn.Sequential(nn.Conv2d(128,128,kernel_size=3,padding=1,stride=1),\n",
    "                                         nn.BatchNorm2d(128),\n",
    "                                         nn.ReLU())\n",
    "        self.res3a_branch2c=nn.Sequential(nn.Conv2d(128,512,kernel_size=1,padding=0,stride=1),\n",
    "                                         nn.BatchNorm2d(512))\n",
    "        \n",
    "        #res3b\n",
    "        self.res3b_branch2a=nn.Sequential(nn.Conv2d(1024,128,kernel_size=1,stride=1,padding=0),\n",
    "                                         nn.BatchNorm2d(128),\n",
    "                                         nn.ReLU())\n",
    "        self.res3b_branch2b=nn.Sequential(nn.Conv2d(128,128,kernel_size=3,padding=1,stride=1),\n",
    "                                         nn.BatchNorm2d(128),\n",
    "                                         nn.ReLU())\n",
    "        self.res3b_branch2c=nn.Sequential(nn.Conv2d(128,512,kernel_size=1,padding=0,stride=1),\n",
    "                                         nn.BatchNorm2d(512))\n",
    "        \n",
    "        #res3c\n",
    "        self.res3c_branch2a=nn.Sequential(nn.Conv2d(1536,128,kernel_size=1,stride=1,padding=0),\n",
    "                                         nn.BatchNorm2d(128),\n",
    "                                         nn.ReLU())\n",
    "        self.res3c_branch2b=nn.Sequential(nn.Conv2d(128,128,kernel_size=3,padding=1,stride=1),\n",
    "                                         nn.BatchNorm2d(128),\n",
    "                                         nn.ReLU())\n",
    "        self.res3c_branch2c=nn.Sequential(nn.Conv2d(128,512,kernel_size=1,padding=0,stride=1),\n",
    "                                         nn.BatchNorm2d(512))\n",
    "        \n",
    "        #res3d\n",
    "        self.res3d_branch2a=nn.Sequential(nn.Conv2d(2048,128,kernel_size=1,stride=1,padding=0),\n",
    "                                         nn.BatchNorm2d(128),\n",
    "                                         nn.ReLU())\n",
    "        self.res3d_branch2b=nn.Sequential(nn.Conv2d(128,128,kernel_size=3,padding=1,stride=1),\n",
    "                                         nn.BatchNorm2d(128),\n",
    "                                         nn.ReLU())\n",
    "        self.res3d_branch2c=nn.Sequential(nn.Conv2d(128,512,kernel_size=1,padding=0,stride=1),\n",
    "                                         nn.BatchNorm2d(512))\n",
    "        \n",
    "        #res4a\n",
    "        self.res4a_branch1=nn.Sequential(nn.Conv2d(2560,1024,kernel_size=1,stride=2,padding=0),\n",
    "                                         nn.BatchNorm2d(1024))\n",
    "        self.res4a_branch2a=nn.Sequential(nn.Conv2d(2560,256,kernel_size=1,stride=2,padding=0),\n",
    "                                         nn.BatchNorm2d(256),\n",
    "                                         nn.ReLU())\n",
    "        self.res4a_branch2b=nn.Sequential(nn.Conv2d(256,256,kernel_size=3,padding=1,stride=1),\n",
    "                                         nn.BatchNorm2d(256),\n",
    "                                         nn.ReLU())\n",
    "        self.res4a_branch2c=nn.Sequential(nn.Conv2d(256,1024,kernel_size=1,padding=0,stride=1),\n",
    "                                         nn.BatchNorm2d(1024))\n",
    "        \n",
    "        #res4b\n",
    "        self.res4b_branch2a=nn.Sequential(nn.Conv2d(2048,256,kernel_size=1,stride=1,padding=0),\n",
    "                                         nn.BatchNorm2d(256),\n",
    "                                         nn.ReLU())\n",
    "        self.res4b_branch2b=nn.Sequential(nn.Conv2d(256,256,kernel_size=3,padding=1,stride=1),\n",
    "                                         nn.BatchNorm2d(256),\n",
    "                                         nn.ReLU())\n",
    "        self.res4b_branch2c=nn.Sequential(nn.Conv2d(256,1024,kernel_size=1,padding=0,stride=1),\n",
    "                                         nn.BatchNorm2d(1024))\n",
    "        #res4c\n",
    "        self.res4c_branch2a=nn.Sequential(nn.Conv2d(3072,256,kernel_size=1,stride=1,padding=0),\n",
    "                                         nn.BatchNorm2d(256),\n",
    "                                         nn.ReLU())\n",
    "        self.res4c_branch2b=nn.Sequential(nn.Conv2d(256,256,kernel_size=3,padding=1,stride=1),\n",
    "                                         nn.BatchNorm2d(256),\n",
    "                                         nn.ReLU())\n",
    "        self.res4c_branch2c=nn.Sequential(nn.Conv2d(256,1024,kernel_size=1,padding=0,stride=1),\n",
    "                                         nn.BatchNorm2d(1024))\n",
    "        \n",
    "        #res4d\n",
    "        self.res4d_branch2a=nn.Sequential(nn.Conv2d(4096,256,kernel_size=1,stride=1,padding=0),\n",
    "                                         nn.BatchNorm2d(256),\n",
    "                                         nn.ReLU())\n",
    "        self.res4d_branch2b=nn.Sequential(nn.Conv2d(256,256,kernel_size=3,padding=1,stride=1),\n",
    "                                         nn.BatchNorm2d(256),\n",
    "                                         nn.ReLU())\n",
    "        self.res4d_branch2c=nn.Sequential(nn.Conv2d(256,1024,kernel_size=1,padding=0,stride=1),\n",
    "                                         nn.BatchNorm2d(1024))\n",
    "        \n",
    "        #res4e\n",
    "        self.res4e_branch2a=nn.Sequential(nn.Conv2d(5120,256,kernel_size=1,stride=1,padding=0),\n",
    "                                         nn.BatchNorm2d(256),\n",
    "                                         nn.ReLU())\n",
    "        self.res4e_branch2b=nn.Sequential(nn.Conv2d(256,256,kernel_size=3,padding=1,stride=1),\n",
    "                                         nn.BatchNorm2d(256),\n",
    "                                         nn.ReLU())\n",
    "        self.res4e_branch2c=nn.Sequential(nn.Conv2d(256,1024,kernel_size=1,padding=0,stride=1),\n",
    "                                         nn.BatchNorm2d(1024))\n",
    "        \n",
    "        #res4f\n",
    "        self.res4f_branch2a=nn.Sequential(nn.Conv2d(6144,256,kernel_size=1,stride=1,padding=0),\n",
    "                                         nn.BatchNorm2d(256),\n",
    "                                         nn.ReLU())\n",
    "        self.res4f_branch2b=nn.Sequential(nn.Conv2d(256,256,kernel_size=3,padding=1,stride=1),\n",
    "                                         nn.BatchNorm2d(256),\n",
    "                                         nn.ReLU())\n",
    "        self.res4f_branch2c=nn.Sequential(nn.Conv2d(256,1024,kernel_size=1,padding=0,stride=1),\n",
    "                                         nn.BatchNorm2d(1024))\n",
    "        \n",
    "        #res5a\n",
    "        self.res5a_branch1=nn.Sequential(nn.Conv2d(7168,2048,kernel_size=1,stride=2,padding=0),\n",
    "                                         nn.BatchNorm2d(2048))\n",
    "        self.res5a_branch2a=nn.Sequential(nn.Conv2d(7168,512,kernel_size=1,stride=2,padding=0),\n",
    "                                         nn.BatchNorm2d(512),\n",
    "                                         nn.ReLU())\n",
    "        self.res5a_branch2b=nn.Sequential(nn.Conv2d(512,512,kernel_size=3,padding=1,stride=1),\n",
    "                                         nn.BatchNorm2d(512),\n",
    "                                         nn.ReLU())\n",
    "        self.res5a_branch2c=nn.Sequential(nn.Conv2d(512,2048,kernel_size=1,padding=0,stride=1),\n",
    "                                         nn.BatchNorm2d(2048))\n",
    "        #res5b\n",
    "        self.res5b_branch2a=nn.Sequential(nn.Conv2d(4096,512,kernel_size=1,stride=1,padding=0),\n",
    "                                         nn.BatchNorm2d(512),\n",
    "                                         nn.ReLU())\n",
    "        self.res5b_branch2b=nn.Sequential(nn.Conv2d(512,512,kernel_size=3,padding=1,stride=1),\n",
    "                                         nn.BatchNorm2d(512),\n",
    "                                         nn.ReLU())\n",
    "        self.res5b_branch2c=nn.Sequential(nn.Conv2d(512,2048,kernel_size=1,padding=0,stride=1),\n",
    "                                         nn.BatchNorm2d(2048))\n",
    "        \n",
    "        #res5c\n",
    "        self.res5c_branch2a=nn.Sequential(nn.Conv2d(6144,512,kernel_size=1,stride=1,padding=0),\n",
    "                                         nn.BatchNorm2d(512),\n",
    "                                         nn.ReLU())\n",
    "        self.res5c_branch2b=nn.Sequential(nn.Conv2d(512,512,kernel_size=3,padding=1,stride=1),\n",
    "                                         nn.BatchNorm2d(512),\n",
    "                                         nn.ReLU())\n",
    "        self.res5c_branch2c=nn.Sequential(nn.Conv2d(512,2048,kernel_size=1,padding=0,stride=1),\n",
    "                                         nn.BatchNorm2d(2048))\n",
    "        \n",
    "        \n",
    "        #res2c_new\n",
    "        self.res2c_new=nn.Sequential(nn.Conv2d(1024,512,kernel_size=3,stride=2,padding=1),\n",
    "                                    nn.BatchNorm2d(512),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Conv2d(512,1024,kernel_size=3,stride=2,padding=1),\n",
    "                                    nn.BatchNorm2d(1024),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Conv2d(1024,2048,kernel_size=3,stride=2,padding=1),\n",
    "                                    nn.BatchNorm2d(2048),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.AvgPool2d(kernel_size=3,stride=1),\n",
    "                                    nn.Conv2d(2048,64,kernel_size=1,stride=1,padding=0))\n",
    "        #res3c_new\n",
    "        self.res3d_new=nn.Sequential(nn.Conv2d(2560,1024,kernel_size=3,stride=2,padding=1),\n",
    "                                    nn.BatchNorm2d(1024),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Conv2d(1024,2048,kernel_size=3,stride=2,padding=1),\n",
    "                                    nn.BatchNorm2d(2048),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.AvgPool2d(kernel_size=3,stride=1),\n",
    "                                    nn.Conv2d(2048,64,kernel_size=1,stride=1,padding=0))\n",
    "        \n",
    "        #res4f_new\n",
    "        self.res4f_new=nn.Sequential(nn.Conv2d(7168,2048,kernel_size=3,stride=2,padding=1),\n",
    "                                    nn.BatchNorm2d(2048),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.AvgPool2d(kernel_size=3,stride=1),\n",
    "                                    nn.Conv2d(2048,64,kernel_size=1,stride=1,padding=0))\n",
    "        #res4f_new\n",
    "        self.res5c_new=nn.Sequential(nn.AvgPool2d(kernel_size=3,stride=1),\n",
    "                                     nn.Conv2d(8192,64,kernel_size=1,stride=1,padding=0))\n",
    "        \n",
    "        self.relu=nn.ReLU()\n",
    "        self.fc1=nn.Linear(256,num_classes)\n",
    "        self.fc2=nn.Linear(num_classes*4,num_classes)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        conv1=self.conv1(x)\n",
    "        \n",
    "        #res2a\n",
    "        res2a_a=self.res2a_branch1(conv1)\n",
    "        res2a_b=self.res2a_branch2a(conv1)\n",
    "        res2a_b=self.res2a_branch2b(res2a_b)\n",
    "        res2a_b=self.res2a_branch2c(res2a_b)\n",
    "        res2a_out=torch.cat((res2a_a,res2a_b),1)\n",
    "        res2a_out=self.relu(res2a_out)\n",
    "        \n",
    "        #res2b\n",
    "        res2b_b=self.res2b_branch2a(res2a_out)\n",
    "        res2b_b=self.res2b_branch2b(res2b_b)\n",
    "        res2b_b=self.res2b_branch2c(res2b_b)\n",
    "        res2b_out=torch.cat((res2a_out,res2b_b),1)\n",
    "        res2b_out=self.relu(res2b_out)\n",
    "        \n",
    "        #res2c\n",
    "        res2c_b=self.res2c_branch2a(res2b_out)\n",
    "        res2c_b=self.res2c_branch2b(res2c_b)\n",
    "        res2c_b=self.res2c_branch2c(res2c_b)\n",
    "        res2c_out=torch.cat((res2b_out,res2c_b),1)\n",
    "        res2c_out=self.relu(res2c_out)\n",
    "        \n",
    "        #res3a\n",
    "        res3a_a=self.res3a_branch1(res2c_out)\n",
    "        res3a_b=self.res3a_branch2a(res2c_out)\n",
    "        res3a_b=self.res3a_branch2b(res3a_b)\n",
    "        res3a_b=self.res3a_branch2c(res3a_b)\n",
    "        res3a_out=torch.cat((res3a_a,res3a_b),1)\n",
    "        res3a_out=self.relu(res3a_out)\n",
    "        \n",
    "        #res3b\n",
    "        res3b_b=self.res3b_branch2a(res3a_out)\n",
    "        res3b_b=self.res3b_branch2b(res3b_b)\n",
    "        res3b_b=self.res3b_branch2c(res3b_b)\n",
    "        res3b_out=torch.cat((res3a_out,res3b_b),1)\n",
    "        res3b_out=self.relu(res3b_out)\n",
    "        \n",
    "        #res3c\n",
    "        res3c_b=self.res3c_branch2a(res3b_out)\n",
    "        res3c_b=self.res3c_branch2b(res3c_b)\n",
    "        res3c_b=self.res3c_branch2c(res3c_b)\n",
    "        res3c_out=torch.cat((res3b_out,res3c_b),1)\n",
    "        res3c_out=self.relu(res3c_out)\n",
    "        \n",
    "        #res3d\n",
    "        res3d_b=self.res3d_branch2a(res3c_out)\n",
    "        res3d_b=self.res3d_branch2b(res3d_b)\n",
    "        res3d_b=self.res3d_branch2c(res3d_b)\n",
    "        res3d_out=torch.cat((res3c_out,res3d_b),1)\n",
    "        res3d_out=self.relu(res3d_out)\n",
    "        \n",
    "        #res4a\n",
    "        res4a_a=self.res4a_branch1(res3d_out)\n",
    "        res4a_b=self.res4a_branch2a(res3d_out)\n",
    "        res4a_b=self.res4a_branch2b(res4a_b)\n",
    "        res4a_b=self.res4a_branch2c(res4a_b)\n",
    "        res4a_out=torch.cat((res4a_a,res4a_b),1)\n",
    "        res4a_out=self.relu(res4a_out)\n",
    "        \n",
    "        #res4b\n",
    "        res4b_b=self.res4b_branch2a(res4a_out)\n",
    "        res4b_b=self.res4b_branch2b(res4b_b)\n",
    "        res4b_b=self.res4b_branch2c(res4b_b)\n",
    "        res4b_out=torch.cat((res4a_out,res4b_b),1)\n",
    "        res4b_out=self.relu(res4b_out)\n",
    "        \n",
    "        #res4c\n",
    "        res4c_b=self.res4c_branch2a(res4b_out)\n",
    "        res4c_b=self.res4c_branch2b(res4c_b)\n",
    "        res4c_b=self.res4c_branch2c(res4c_b)\n",
    "        res4c_out=torch.cat((res4b_out,res4c_b),1)\n",
    "        res4c_out=self.relu(res4c_out)\n",
    "        \n",
    "        #res4d\n",
    "        res4d_b=self.res4d_branch2a(res4c_out)\n",
    "        res4d_b=self.res4d_branch2b(res4d_b)\n",
    "        res4d_b=self.res4d_branch2c(res4d_b)\n",
    "        res4d_out=torch.cat((res4c_out,res4d_b),1)\n",
    "        res4d_out=self.relu(res4d_out)\n",
    "        \n",
    "        #res4e\n",
    "        res4e_b=self.res4e_branch2a(res4d_out)\n",
    "        res4e_b=self.res4e_branch2b(res4e_b)\n",
    "        res4e_b=self.res4e_branch2c(res4e_b)\n",
    "        res4e_out=torch.cat((res4d_out,res4e_b),1)\n",
    "        res4e_out=self.relu(res4e_out)\n",
    "        \n",
    "        #res4f\n",
    "        res4f_b=self.res4f_branch2a(res4e_out)\n",
    "        res4f_b=self.res4f_branch2b(res4f_b)\n",
    "        res4f_b=self.res4f_branch2c(res4f_b)\n",
    "        res4f_out=torch.cat((res4e_out,res4f_b),1)\n",
    "        res4f_out=self.relu(res4f_out)\n",
    "        \n",
    "        #res5a\n",
    "        res5a_a=self.res5a_branch1(res4f_out)\n",
    "        res5a_b=self.res5a_branch2a(res4f_out)\n",
    "        res5a_b=self.res5a_branch2b(res5a_b)\n",
    "        res5a_b=self.res5a_branch2c(res5a_b)\n",
    "        res5a_out=torch.cat((res5a_a,res5a_b),1)\n",
    "        res5a_out=self.relu(res5a_out)\n",
    "        #res5b\n",
    "        res5b_b=self.res5b_branch2a(res5a_out)\n",
    "        res5b_b=self.res5b_branch2b(res5b_b)\n",
    "        res5b_b=self.res5b_branch2c(res5b_b)\n",
    "        res5b_out=torch.cat((res5a_out,res5b_b),1)\n",
    "        res5b_out=self.relu(res5b_out)\n",
    "\n",
    "        #res5c\n",
    "        res5c_b=self.res5c_branch2a(res5b_out)\n",
    "        res5c_b=self.res5c_branch2b(res5c_b)\n",
    "        res5c_b=self.res5c_branch2c(res5c_b)\n",
    "        res5c_out=torch.cat((res5b_out,res5c_b),1)\n",
    "        res5c_out=self.relu(res5c_out)\n",
    "        \n",
    "        ##########################block############################\n",
    "        #res2c_new (ss_block)\n",
    "        res2c_new=self.res2c_new(res2c_out)\n",
    "        res2c_new=res2c_new.reshape(res2c_new.size(0),-1)\n",
    "#         print(\"res2c_new.shape:\",res2c_new.shape)\n",
    "        res2c_new=self.fc1(res2c_new)\n",
    "        \n",
    "        #res3d_new (is_block)\n",
    "        res3d_new=self.res3d_new(res3d_out)\n",
    "        res3d_new=res3d_new.reshape(res3d_new.size(0),-1)\n",
    "        res3d_new=self.fc1(res3d_new)\n",
    "        \n",
    "        #res4f_new (is_block)\n",
    "        res4f_new=self.res4f_new(res4f_out)\n",
    "        res4f_new=res4f_new.reshape(res4f_new.size(0),-1)\n",
    "        res4f_new=self.fc1(res4f_new)\n",
    "        \n",
    "        #res5c_new (ds_block)\n",
    "        res5c_new=self.res5c_new(res5c_out)\n",
    "        res5c_new=res5c_new.reshape(res5c_new.size(0),-1)\n",
    "        res5c_new=self.fc1(res5c_new)\n",
    "        \n",
    "        concat_branch1=torch.cat((res2c_new,res3d_new,res4f_new,res5c_new),1)\n",
    "        concat_branch2=self.fc2(concat_branch1)\n",
    "        \n",
    "        return res2c_new,res3d_new,res4f_new,res5c_new,concat_branch2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model=ResNet().cuda()\n",
    "softmax=nn.Softmax()\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# a=torch.rand((1,3,128,128))\n",
    "# a=a.cuda()\n",
    "# outputs=model(a)\n",
    "# print(\"model_output : \",outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Step [100/12299],final_Loss: 0.3929\n",
      "Epoch [1/20], Step [200/12299],final_Loss: 8.6102\n",
      "Epoch [1/20], Step [300/12299],final_Loss: 0.6233\n",
      "Epoch [1/20], Step [400/12299],final_Loss: 2.0855\n",
      "Epoch [1/20], Step [500/12299],final_Loss: 0.6122\n",
      "Epoch [1/20], Step [600/12299],final_Loss: 0.8908\n",
      "Epoch [1/20], Step [700/12299],final_Loss: 30.9837\n",
      "Epoch [1/20], Step [800/12299],final_Loss: 0.9792\n",
      "Epoch [1/20], Step [900/12299],final_Loss: 0.4410\n",
      "Epoch [1/20], Step [1000/12299],final_Loss: 1.8991\n",
      "Epoch [1/20], Step [1100/12299],final_Loss: 0.4905\n",
      "Epoch [1/20], Step [1200/12299],final_Loss: 0.8468\n",
      "Epoch [1/20], Step [1300/12299],final_Loss: 2.0638\n",
      "Epoch [1/20], Step [1400/12299],final_Loss: 0.5117\n",
      "Epoch [1/20], Step [1500/12299],final_Loss: 5.0574\n",
      "Epoch [1/20], Step [1600/12299],final_Loss: 22.4239\n",
      "Epoch [1/20], Step [1700/12299],final_Loss: 16.2866\n",
      "Epoch [1/20], Step [1800/12299],final_Loss: 0.5029\n",
      "Epoch [1/20], Step [1900/12299],final_Loss: 0.6788\n",
      "Epoch [1/20], Step [2000/12299],final_Loss: 13.7053\n",
      "Epoch [1/20], Step [2100/12299],final_Loss: 2.2823\n",
      "Epoch [1/20], Step [2200/12299],final_Loss: 1.5063\n",
      "Epoch [1/20], Step [2300/12299],final_Loss: 1.0980\n",
      "Epoch [1/20], Step [2400/12299],final_Loss: 1.0694\n",
      "Epoch [1/20], Step [2500/12299],final_Loss: 1.7405\n",
      "Epoch [1/20], Step [2600/12299],final_Loss: 1.0764\n",
      "Epoch [1/20], Step [2700/12299],final_Loss: 2.3747\n",
      "Epoch [1/20], Step [2800/12299],final_Loss: 0.7472\n",
      "Epoch [1/20], Step [2900/12299],final_Loss: 0.6938\n",
      "Epoch [1/20], Step [3000/12299],final_Loss: 0.5514\n",
      "Epoch [1/20], Step [3100/12299],final_Loss: 1.8399\n",
      "Epoch [1/20], Step [3200/12299],final_Loss: 2.0595\n",
      "Epoch [1/20], Step [3300/12299],final_Loss: 13.5111\n",
      "Epoch [1/20], Step [3400/12299],final_Loss: 0.6183\n",
      "Epoch [1/20], Step [3500/12299],final_Loss: 0.7398\n",
      "Epoch [1/20], Step [3600/12299],final_Loss: 1.9303\n",
      "Epoch [1/20], Step [3700/12299],final_Loss: 0.4790\n",
      "Epoch [1/20], Step [3800/12299],final_Loss: 0.6496\n",
      "Epoch [1/20], Step [3900/12299],final_Loss: 0.7922\n",
      "Epoch [1/20], Step [4000/12299],final_Loss: 5.3882\n",
      "Epoch [1/20], Step [4100/12299],final_Loss: 0.3158\n",
      "Epoch [1/20], Step [4200/12299],final_Loss: 25.8036\n",
      "Epoch [1/20], Step [4300/12299],final_Loss: 1.0114\n",
      "Epoch [1/20], Step [4400/12299],final_Loss: 1.4505\n",
      "Epoch [1/20], Step [4500/12299],final_Loss: 0.9251\n",
      "Epoch [1/20], Step [4600/12299],final_Loss: 0.9304\n",
      "Epoch [1/20], Step [4700/12299],final_Loss: 2.2619\n",
      "Epoch [1/20], Step [4800/12299],final_Loss: 4.7012\n",
      "Epoch [1/20], Step [4900/12299],final_Loss: 9.0926\n",
      "Epoch [1/20], Step [5000/12299],final_Loss: 0.5888\n",
      "Epoch [1/20], Step [5100/12299],final_Loss: 8.1232\n",
      "Epoch [1/20], Step [5200/12299],final_Loss: 1.6483\n",
      "Epoch [1/20], Step [5300/12299],final_Loss: 1.2541\n",
      "Epoch [1/20], Step [5400/12299],final_Loss: 13.4624\n",
      "Epoch [1/20], Step [5500/12299],final_Loss: 2.6898\n",
      "Epoch [1/20], Step [5600/12299],final_Loss: 1.1647\n",
      "Epoch [1/20], Step [5700/12299],final_Loss: 12.0986\n",
      "Epoch [1/20], Step [5800/12299],final_Loss: 0.7186\n",
      "Epoch [1/20], Step [5900/12299],final_Loss: 1.6181\n",
      "Epoch [1/20], Step [6000/12299],final_Loss: 3.9274\n",
      "Epoch [1/20], Step [6100/12299],final_Loss: 2.2280\n",
      "Epoch [1/20], Step [6200/12299],final_Loss: 1.2688\n",
      "Epoch [1/20], Step [6300/12299],final_Loss: 0.2962\n",
      "Epoch [1/20], Step [6400/12299],final_Loss: 0.6770\n",
      "Epoch [1/20], Step [6500/12299],final_Loss: 0.6982\n",
      "Epoch [1/20], Step [6600/12299],final_Loss: 1.1340\n",
      "Epoch [1/20], Step [6700/12299],final_Loss: 1.5305\n",
      "Epoch [1/20], Step [6800/12299],final_Loss: 1.1947\n",
      "Epoch [1/20], Step [6900/12299],final_Loss: 0.8826\n",
      "Epoch [1/20], Step [7000/12299],final_Loss: 0.8182\n",
      "Epoch [1/20], Step [7100/12299],final_Loss: 1.0478\n",
      "Epoch [1/20], Step [7200/12299],final_Loss: 0.3293\n",
      "Epoch [1/20], Step [7300/12299],final_Loss: 1.0980\n",
      "Epoch [1/20], Step [7400/12299],final_Loss: 2.8971\n",
      "Epoch [1/20], Step [7500/12299],final_Loss: 5.8826\n",
      "Epoch [1/20], Step [7600/12299],final_Loss: 2.5864\n",
      "Epoch [1/20], Step [7700/12299],final_Loss: 4.0093\n",
      "Epoch [1/20], Step [7800/12299],final_Loss: 21.4170\n",
      "Epoch [1/20], Step [7900/12299],final_Loss: 0.3891\n",
      "Epoch [1/20], Step [8000/12299],final_Loss: 0.3026\n",
      "Epoch [1/20], Step [8100/12299],final_Loss: 0.6292\n",
      "Epoch [1/20], Step [8200/12299],final_Loss: 2.3793\n",
      "Epoch [1/20], Step [8300/12299],final_Loss: 0.5512\n",
      "Epoch [1/20], Step [8400/12299],final_Loss: 0.6385\n",
      "Epoch [1/20], Step [8500/12299],final_Loss: 0.2090\n",
      "Epoch [1/20], Step [8600/12299],final_Loss: 0.6847\n",
      "Epoch [1/20], Step [8700/12299],final_Loss: 0.3017\n",
      "Epoch [1/20], Step [8800/12299],final_Loss: 0.3786\n",
      "Epoch [1/20], Step [8900/12299],final_Loss: 0.3629\n",
      "Epoch [1/20], Step [9000/12299],final_Loss: 21.7048\n",
      "Epoch [1/20], Step [9100/12299],final_Loss: 2.9549\n",
      "Epoch [1/20], Step [9200/12299],final_Loss: 1.0316\n",
      "Epoch [1/20], Step [9300/12299],final_Loss: 0.6551\n",
      "Epoch [1/20], Step [9400/12299],final_Loss: 14.8693\n",
      "Epoch [1/20], Step [9500/12299],final_Loss: 0.2986\n",
      "Epoch [1/20], Step [9600/12299],final_Loss: 0.8070\n",
      "Epoch [1/20], Step [9700/12299],final_Loss: 0.2798\n",
      "Epoch [1/20], Step [9800/12299],final_Loss: 0.1858\n",
      "Epoch [1/20], Step [9900/12299],final_Loss: 0.1340\n",
      "Epoch [1/20], Step [10000/12299],final_Loss: 1.0406\n",
      "Epoch [1/20], Step [10100/12299],final_Loss: 25.6600\n",
      "Epoch [1/20], Step [10200/12299],final_Loss: 0.2607\n",
      "Epoch [1/20], Step [10300/12299],final_Loss: 7.3608\n",
      "Epoch [1/20], Step [10400/12299],final_Loss: 0.3863\n",
      "Epoch [1/20], Step [10500/12299],final_Loss: 14.9665\n",
      "Epoch [1/20], Step [10600/12299],final_Loss: 0.2786\n",
      "Epoch [1/20], Step [10700/12299],final_Loss: 0.9389\n",
      "Epoch [1/20], Step [10800/12299],final_Loss: 0.2411\n",
      "Epoch [1/20], Step [10900/12299],final_Loss: 0.5144\n",
      "Epoch [1/20], Step [11000/12299],final_Loss: 2.6147\n",
      "Epoch [1/20], Step [11100/12299],final_Loss: 0.7514\n",
      "Epoch [1/20], Step [11200/12299],final_Loss: 1.9590\n",
      "Epoch [1/20], Step [11300/12299],final_Loss: 0.6516\n",
      "Epoch [1/20], Step [11400/12299],final_Loss: 0.2488\n",
      "Epoch [1/20], Step [11500/12299],final_Loss: 0.4856\n",
      "Epoch [1/20], Step [11600/12299],final_Loss: 0.3200\n",
      "Epoch [1/20], Step [11700/12299],final_Loss: 0.1555\n",
      "Epoch [1/20], Step [11800/12299],final_Loss: 5.2885\n",
      "Epoch [1/20], Step [11900/12299],final_Loss: 0.2778\n",
      "Epoch [1/20], Step [12000/12299],final_Loss: 0.9365\n",
      "Epoch [1/20], Step [12100/12299],final_Loss: 0.3145\n",
      "Epoch [1/20], Step [12200/12299],final_Loss: 0.3983\n",
      "Epoch [2/20], Step [100/12299],final_Loss: 0.2351\n",
      "Epoch [2/20], Step [200/12299],final_Loss: 4.4428\n",
      "Epoch [2/20], Step [300/12299],final_Loss: 0.2879\n",
      "Epoch [2/20], Step [400/12299],final_Loss: 0.7881\n",
      "Epoch [2/20], Step [500/12299],final_Loss: 0.2602\n",
      "Epoch [2/20], Step [600/12299],final_Loss: 0.1984\n",
      "Epoch [2/20], Step [700/12299],final_Loss: 20.7397\n",
      "Epoch [2/20], Step [800/12299],final_Loss: 0.3167\n",
      "Epoch [2/20], Step [900/12299],final_Loss: 0.1734\n",
      "Epoch [2/20], Step [1000/12299],final_Loss: 0.4390\n",
      "Epoch [2/20], Step [1100/12299],final_Loss: 0.1717\n",
      "Epoch [2/20], Step [1200/12299],final_Loss: 0.3626\n",
      "Epoch [2/20], Step [1300/12299],final_Loss: 0.7225\n",
      "Epoch [2/20], Step [1400/12299],final_Loss: 0.1766\n",
      "Epoch [2/20], Step [1500/12299],final_Loss: 2.7017\n",
      "Epoch [2/20], Step [1600/12299],final_Loss: 32.0841\n",
      "Epoch [2/20], Step [1700/12299],final_Loss: 15.5605\n",
      "Epoch [2/20], Step [1800/12299],final_Loss: 0.2260\n",
      "Epoch [2/20], Step [1900/12299],final_Loss: 0.4606\n",
      "Epoch [2/20], Step [2000/12299],final_Loss: 9.8259\n",
      "Epoch [2/20], Step [2100/12299],final_Loss: 0.8167\n",
      "Epoch [2/20], Step [2200/12299],final_Loss: 0.5200\n",
      "Epoch [2/20], Step [2300/12299],final_Loss: 0.5648\n",
      "Epoch [2/20], Step [2400/12299],final_Loss: 0.4166\n",
      "Epoch [2/20], Step [2500/12299],final_Loss: 0.6663\n",
      "Epoch [2/20], Step [2600/12299],final_Loss: 0.3587\n",
      "Epoch [2/20], Step [2700/12299],final_Loss: 0.7126\n",
      "Epoch [2/20], Step [2800/12299],final_Loss: 0.3339\n",
      "Epoch [2/20], Step [2900/12299],final_Loss: 0.3363\n",
      "Epoch [2/20], Step [3000/12299],final_Loss: 0.4522\n",
      "Epoch [2/20], Step [3100/12299],final_Loss: 1.0532\n",
      "Epoch [2/20], Step [3200/12299],final_Loss: 1.0696\n",
      "Epoch [2/20], Step [3300/12299],final_Loss: 6.8782\n",
      "Epoch [2/20], Step [3400/12299],final_Loss: 0.2829\n",
      "Epoch [2/20], Step [3500/12299],final_Loss: 0.3502\n",
      "Epoch [2/20], Step [3600/12299],final_Loss: 1.1076\n",
      "Epoch [2/20], Step [3700/12299],final_Loss: 0.3928\n",
      "Epoch [2/20], Step [3800/12299],final_Loss: 0.3632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20], Step [3900/12299],final_Loss: 0.4180\n",
      "Epoch [2/20], Step [4000/12299],final_Loss: 4.0955\n",
      "Epoch [2/20], Step [4100/12299],final_Loss: 0.2690\n",
      "Epoch [2/20], Step [4200/12299],final_Loss: 31.1882\n",
      "Epoch [2/20], Step [4300/12299],final_Loss: 0.7614\n",
      "Epoch [2/20], Step [4400/12299],final_Loss: 0.5774\n",
      "Epoch [2/20], Step [4500/12299],final_Loss: 0.4494\n",
      "Epoch [2/20], Step [4600/12299],final_Loss: 0.6919\n",
      "Epoch [2/20], Step [4700/12299],final_Loss: 2.5945\n",
      "Epoch [2/20], Step [4800/12299],final_Loss: 5.1873\n",
      "Epoch [2/20], Step [4900/12299],final_Loss: 4.6677\n",
      "Epoch [2/20], Step [5000/12299],final_Loss: 0.3845\n",
      "Epoch [2/20], Step [5100/12299],final_Loss: 6.1813\n",
      "Epoch [2/20], Step [5200/12299],final_Loss: 1.0403\n",
      "Epoch [2/20], Step [5300/12299],final_Loss: 0.7258\n",
      "Epoch [2/20], Step [5400/12299],final_Loss: 9.5191\n",
      "Epoch [2/20], Step [5500/12299],final_Loss: 1.9649\n",
      "Epoch [2/20], Step [5600/12299],final_Loss: 0.8405\n",
      "Epoch [2/20], Step [5700/12299],final_Loss: 12.6274\n",
      "Epoch [2/20], Step [5800/12299],final_Loss: 0.6212\n",
      "Epoch [2/20], Step [5900/12299],final_Loss: 0.8936\n",
      "Epoch [2/20], Step [6000/12299],final_Loss: 2.8738\n",
      "Epoch [2/20], Step [6100/12299],final_Loss: 1.2903\n",
      "Epoch [2/20], Step [6200/12299],final_Loss: 1.3453\n",
      "Epoch [2/20], Step [6300/12299],final_Loss: 0.3108\n",
      "Epoch [2/20], Step [6400/12299],final_Loss: 0.5939\n",
      "Epoch [2/20], Step [6500/12299],final_Loss: 0.5966\n",
      "Epoch [2/20], Step [6600/12299],final_Loss: 0.8401\n",
      "Epoch [2/20], Step [6700/12299],final_Loss: 1.2739\n",
      "Epoch [2/20], Step [6800/12299],final_Loss: 0.6565\n",
      "Epoch [2/20], Step [6900/12299],final_Loss: 0.5664\n",
      "Epoch [2/20], Step [7000/12299],final_Loss: 0.6406\n",
      "Epoch [2/20], Step [7100/12299],final_Loss: 0.8074\n",
      "Epoch [2/20], Step [7200/12299],final_Loss: 0.2297\n",
      "Epoch [2/20], Step [7300/12299],final_Loss: 1.0435\n",
      "Epoch [2/20], Step [7400/12299],final_Loss: 2.8130\n",
      "Epoch [2/20], Step [7500/12299],final_Loss: 4.0193\n",
      "Epoch [2/20], Step [7600/12299],final_Loss: 1.9317\n",
      "Epoch [2/20], Step [7700/12299],final_Loss: 3.6187\n",
      "Epoch [2/20], Step [7800/12299],final_Loss: 18.2067\n",
      "Epoch [2/20], Step [7900/12299],final_Loss: 0.3939\n",
      "Epoch [2/20], Step [8000/12299],final_Loss: 0.3111\n",
      "Epoch [2/20], Step [8100/12299],final_Loss: 0.5612\n",
      "Epoch [2/20], Step [8200/12299],final_Loss: 2.1852\n",
      "Epoch [2/20], Step [8300/12299],final_Loss: 0.6683\n",
      "Epoch [2/20], Step [8400/12299],final_Loss: 0.7844\n",
      "Epoch [2/20], Step [8500/12299],final_Loss: 0.1533\n",
      "Epoch [2/20], Step [8600/12299],final_Loss: 0.4633\n",
      "Epoch [2/20], Step [8700/12299],final_Loss: 0.2099\n",
      "Epoch [2/20], Step [8800/12299],final_Loss: 0.3223\n",
      "Epoch [2/20], Step [8900/12299],final_Loss: 0.4340\n",
      "Epoch [2/20], Step [9000/12299],final_Loss: 28.0148\n",
      "Epoch [2/20], Step [9100/12299],final_Loss: 3.5099\n",
      "Epoch [2/20], Step [9200/12299],final_Loss: 1.3484\n",
      "Epoch [2/20], Step [9300/12299],final_Loss: 1.0083\n",
      "Epoch [2/20], Step [9400/12299],final_Loss: 11.0472\n",
      "Epoch [2/20], Step [9500/12299],final_Loss: 0.2836\n",
      "Epoch [2/20], Step [9600/12299],final_Loss: 0.7437\n",
      "Epoch [2/20], Step [9700/12299],final_Loss: 0.2312\n",
      "Epoch [2/20], Step [9800/12299],final_Loss: 0.1456\n",
      "Epoch [2/20], Step [9900/12299],final_Loss: 0.1006\n",
      "Epoch [2/20], Step [10000/12299],final_Loss: 0.8411\n",
      "Epoch [2/20], Step [10100/12299],final_Loss: 22.0904\n",
      "Epoch [2/20], Step [10200/12299],final_Loss: 0.2926\n",
      "Epoch [2/20], Step [10300/12299],final_Loss: 6.7441\n",
      "Epoch [2/20], Step [10400/12299],final_Loss: 0.3258\n",
      "Epoch [2/20], Step [10500/12299],final_Loss: 16.0444\n",
      "Epoch [2/20], Step [10600/12299],final_Loss: 0.2737\n",
      "Epoch [2/20], Step [10700/12299],final_Loss: 0.7359\n",
      "Epoch [2/20], Step [10800/12299],final_Loss: 0.1619\n",
      "Epoch [2/20], Step [10900/12299],final_Loss: 0.4461\n",
      "Epoch [2/20], Step [11000/12299],final_Loss: 2.9640\n",
      "Epoch [2/20], Step [11100/12299],final_Loss: 0.6695\n",
      "Epoch [2/20], Step [11200/12299],final_Loss: 2.9446\n",
      "Epoch [2/20], Step [11300/12299],final_Loss: 1.0869\n",
      "Epoch [2/20], Step [11400/12299],final_Loss: 0.3209\n",
      "Epoch [2/20], Step [11500/12299],final_Loss: 0.5285\n",
      "Epoch [2/20], Step [11600/12299],final_Loss: 0.4418\n",
      "Epoch [2/20], Step [11700/12299],final_Loss: 0.2055\n",
      "Epoch [2/20], Step [11800/12299],final_Loss: 6.2750\n",
      "Epoch [2/20], Step [11900/12299],final_Loss: 0.2644\n",
      "Epoch [2/20], Step [12000/12299],final_Loss: 0.6783\n",
      "Epoch [2/20], Step [12100/12299],final_Loss: 0.2533\n",
      "Epoch [2/20], Step [12200/12299],final_Loss: 0.4144\n",
      "Epoch [3/20], Step [100/12299],final_Loss: 0.2596\n",
      "Epoch [3/20], Step [200/12299],final_Loss: 3.7268\n",
      "Epoch [3/20], Step [300/12299],final_Loss: 0.2347\n",
      "Epoch [3/20], Step [400/12299],final_Loss: 0.6223\n",
      "Epoch [3/20], Step [500/12299],final_Loss: 0.2107\n",
      "Epoch [3/20], Step [600/12299],final_Loss: 0.1792\n",
      "Epoch [3/20], Step [700/12299],final_Loss: 22.7199\n",
      "Epoch [3/20], Step [800/12299],final_Loss: 0.2133\n",
      "Epoch [3/20], Step [900/12299],final_Loss: 0.1138\n",
      "Epoch [3/20], Step [1000/12299],final_Loss: 0.3734\n",
      "Epoch [3/20], Step [1100/12299],final_Loss: 0.1522\n",
      "Epoch [3/20], Step [1200/12299],final_Loss: 0.3121\n",
      "Epoch [3/20], Step [1300/12299],final_Loss: 0.7193\n",
      "Epoch [3/20], Step [1400/12299],final_Loss: 0.1734\n",
      "Epoch [3/20], Step [1500/12299],final_Loss: 2.6106\n",
      "Epoch [3/20], Step [1600/12299],final_Loss: 27.9476\n",
      "Epoch [3/20], Step [1700/12299],final_Loss: 12.8018\n",
      "Epoch [3/20], Step [1800/12299],final_Loss: 0.1790\n",
      "Epoch [3/20], Step [1900/12299],final_Loss: 0.4015\n",
      "Epoch [3/20], Step [2000/12299],final_Loss: 11.4236\n",
      "Epoch [3/20], Step [2100/12299],final_Loss: 0.7816\n",
      "Epoch [3/20], Step [2200/12299],final_Loss: 0.4007\n",
      "Epoch [3/20], Step [2300/12299],final_Loss: 0.4749\n",
      "Epoch [3/20], Step [2400/12299],final_Loss: 0.3911\n",
      "Epoch [3/20], Step [2500/12299],final_Loss: 0.5707\n",
      "Epoch [3/20], Step [2600/12299],final_Loss: 0.3161\n",
      "Epoch [3/20], Step [2700/12299],final_Loss: 0.6064\n",
      "Epoch [3/20], Step [2800/12299],final_Loss: 0.3129\n",
      "Epoch [3/20], Step [2900/12299],final_Loss: 0.2648\n",
      "Epoch [3/20], Step [3000/12299],final_Loss: 0.4536\n",
      "Epoch [3/20], Step [3100/12299],final_Loss: 1.0687\n",
      "Epoch [3/20], Step [3200/12299],final_Loss: 0.8085\n",
      "Epoch [3/20], Step [3300/12299],final_Loss: 6.8866\n",
      "Epoch [3/20], Step [3400/12299],final_Loss: 0.2596\n",
      "Epoch [3/20], Step [3500/12299],final_Loss: 0.3196\n",
      "Epoch [3/20], Step [3600/12299],final_Loss: 0.9809\n",
      "Epoch [3/20], Step [3700/12299],final_Loss: 0.4135\n",
      "Epoch [3/20], Step [3800/12299],final_Loss: 0.3262\n",
      "Epoch [3/20], Step [3900/12299],final_Loss: 0.3908\n",
      "Epoch [3/20], Step [4000/12299],final_Loss: 4.7063\n",
      "Epoch [3/20], Step [4100/12299],final_Loss: 0.3103\n",
      "Epoch [3/20], Step [4200/12299],final_Loss: 34.2274\n",
      "Epoch [3/20], Step [4300/12299],final_Loss: 1.0555\n",
      "Epoch [3/20], Step [4400/12299],final_Loss: 0.7121\n",
      "Epoch [3/20], Step [4500/12299],final_Loss: 0.3877\n",
      "Epoch [3/20], Step [4600/12299],final_Loss: 0.5666\n",
      "Epoch [3/20], Step [4700/12299],final_Loss: 2.1687\n",
      "Epoch [3/20], Step [4800/12299],final_Loss: 4.0379\n",
      "Epoch [3/20], Step [4900/12299],final_Loss: 6.2451\n",
      "Epoch [3/20], Step [5000/12299],final_Loss: 0.3254\n",
      "Epoch [3/20], Step [5100/12299],final_Loss: 6.4090\n",
      "Epoch [3/20], Step [5200/12299],final_Loss: 1.0163\n",
      "Epoch [3/20], Step [5300/12299],final_Loss: 0.6892\n",
      "Epoch [3/20], Step [5400/12299],final_Loss: 13.8580\n",
      "Epoch [3/20], Step [5500/12299],final_Loss: 1.8878\n",
      "Epoch [3/20], Step [5600/12299],final_Loss: 0.5567\n",
      "Epoch [3/20], Step [5700/12299],final_Loss: 10.8471\n",
      "Epoch [3/20], Step [5800/12299],final_Loss: 0.7429\n",
      "Epoch [3/20], Step [5900/12299],final_Loss: 1.3700\n",
      "Epoch [3/20], Step [6000/12299],final_Loss: 3.8799\n",
      "Epoch [3/20], Step [6100/12299],final_Loss: 1.8317\n",
      "Epoch [3/20], Step [6200/12299],final_Loss: 1.5491\n",
      "Epoch [3/20], Step [6300/12299],final_Loss: 0.3527\n",
      "Epoch [3/20], Step [6400/12299],final_Loss: 0.5328\n",
      "Epoch [3/20], Step [6500/12299],final_Loss: 0.6179\n",
      "Epoch [3/20], Step [6600/12299],final_Loss: 0.9642\n",
      "Epoch [3/20], Step [6700/12299],final_Loss: 1.1073\n",
      "Epoch [3/20], Step [6800/12299],final_Loss: 0.6279\n",
      "Epoch [3/20], Step [6900/12299],final_Loss: 0.5301\n",
      "Epoch [3/20], Step [7000/12299],final_Loss: 0.5293\n",
      "Epoch [3/20], Step [7100/12299],final_Loss: 0.8471\n",
      "Epoch [3/20], Step [7200/12299],final_Loss: 0.2658\n",
      "Epoch [3/20], Step [7300/12299],final_Loss: 0.8602\n",
      "Epoch [3/20], Step [7400/12299],final_Loss: 3.8648\n",
      "Epoch [3/20], Step [7500/12299],final_Loss: 3.5403\n",
      "Epoch [3/20], Step [7600/12299],final_Loss: 1.5068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/20], Step [7700/12299],final_Loss: 4.1975\n",
      "Epoch [3/20], Step [7800/12299],final_Loss: 29.4147\n",
      "Epoch [3/20], Step [7900/12299],final_Loss: 0.4882\n",
      "Epoch [3/20], Step [8000/12299],final_Loss: 0.5906\n",
      "Epoch [3/20], Step [8100/12299],final_Loss: 0.5432\n",
      "Epoch [3/20], Step [8200/12299],final_Loss: 2.9443\n",
      "Epoch [3/20], Step [8300/12299],final_Loss: 0.4942\n",
      "Epoch [3/20], Step [8400/12299],final_Loss: 0.8712\n",
      "Epoch [3/20], Step [8500/12299],final_Loss: 0.1909\n",
      "Epoch [3/20], Step [8600/12299],final_Loss: 0.4345\n",
      "Epoch [3/20], Step [8700/12299],final_Loss: 0.2148\n",
      "Epoch [3/20], Step [8800/12299],final_Loss: 0.2136\n",
      "Epoch [3/20], Step [8900/12299],final_Loss: 0.3971\n",
      "Epoch [3/20], Step [9000/12299],final_Loss: 20.4474\n",
      "Epoch [3/20], Step [9100/12299],final_Loss: 3.2344\n",
      "Epoch [3/20], Step [9200/12299],final_Loss: 0.9314\n",
      "Epoch [3/20], Step [9300/12299],final_Loss: 0.6721\n",
      "Epoch [3/20], Step [9400/12299],final_Loss: 12.2693\n",
      "Epoch [3/20], Step [9500/12299],final_Loss: 0.3015\n",
      "Epoch [3/20], Step [9600/12299],final_Loss: 0.5929\n",
      "Epoch [3/20], Step [9700/12299],final_Loss: 0.1962\n",
      "Epoch [3/20], Step [9800/12299],final_Loss: 0.1216\n",
      "Epoch [3/20], Step [9900/12299],final_Loss: 0.0893\n",
      "Epoch [3/20], Step [10000/12299],final_Loss: 0.7334\n",
      "Epoch [3/20], Step [10100/12299],final_Loss: 22.0808\n",
      "Epoch [3/20], Step [10200/12299],final_Loss: 0.3058\n",
      "Epoch [3/20], Step [10300/12299],final_Loss: 5.6216\n",
      "Epoch [3/20], Step [10400/12299],final_Loss: 0.2973\n",
      "Epoch [3/20], Step [10500/12299],final_Loss: 14.2449\n",
      "Epoch [3/20], Step [10600/12299],final_Loss: 0.2679\n",
      "Epoch [3/20], Step [10700/12299],final_Loss: 0.9933\n",
      "Epoch [3/20], Step [10800/12299],final_Loss: 0.1609\n",
      "Epoch [3/20], Step [10900/12299],final_Loss: 0.3987\n",
      "Epoch [3/20], Step [11000/12299],final_Loss: 3.0275\n",
      "Epoch [3/20], Step [11100/12299],final_Loss: 0.6153\n",
      "Epoch [3/20], Step [11200/12299],final_Loss: 3.3583\n",
      "Epoch [3/20], Step [11300/12299],final_Loss: 0.8726\n",
      "Epoch [3/20], Step [11400/12299],final_Loss: 0.2890\n",
      "Epoch [3/20], Step [11500/12299],final_Loss: 0.4082\n",
      "Epoch [3/20], Step [11600/12299],final_Loss: 0.3926\n",
      "Epoch [3/20], Step [11700/12299],final_Loss: 0.1856\n",
      "Epoch [3/20], Step [11800/12299],final_Loss: 5.2393\n",
      "Epoch [3/20], Step [11900/12299],final_Loss: 0.2361\n",
      "Epoch [3/20], Step [12000/12299],final_Loss: 0.6470\n",
      "Epoch [3/20], Step [12100/12299],final_Loss: 0.2338\n",
      "Epoch [3/20], Step [12200/12299],final_Loss: 0.3580\n",
      "Epoch [4/20], Step [100/12299],final_Loss: 0.2063\n",
      "Epoch [4/20], Step [200/12299],final_Loss: 4.0739\n",
      "Epoch [4/20], Step [300/12299],final_Loss: 0.2222\n",
      "Epoch [4/20], Step [400/12299],final_Loss: 0.6409\n",
      "Epoch [4/20], Step [500/12299],final_Loss: 0.2070\n",
      "Epoch [4/20], Step [600/12299],final_Loss: 0.1492\n",
      "Epoch [4/20], Step [700/12299],final_Loss: 18.2324\n",
      "Epoch [4/20], Step [800/12299],final_Loss: 0.1664\n",
      "Epoch [4/20], Step [900/12299],final_Loss: 0.1081\n",
      "Epoch [4/20], Step [1000/12299],final_Loss: 0.3071\n",
      "Epoch [4/20], Step [1100/12299],final_Loss: 0.1303\n",
      "Epoch [4/20], Step [1200/12299],final_Loss: 0.2745\n",
      "Epoch [4/20], Step [1300/12299],final_Loss: 0.8688\n",
      "Epoch [4/20], Step [1400/12299],final_Loss: 0.1642\n",
      "Epoch [4/20], Step [1500/12299],final_Loss: 2.3964\n",
      "Epoch [4/20], Step [1600/12299],final_Loss: 40.0441\n",
      "Epoch [4/20], Step [1700/12299],final_Loss: 13.0576\n",
      "Epoch [4/20], Step [1800/12299],final_Loss: 0.1889\n",
      "Epoch [4/20], Step [1900/12299],final_Loss: 0.3832\n",
      "Epoch [4/20], Step [2000/12299],final_Loss: 6.0430\n",
      "Epoch [4/20], Step [2100/12299],final_Loss: 0.7872\n",
      "Epoch [4/20], Step [2200/12299],final_Loss: 0.3928\n",
      "Epoch [4/20], Step [2300/12299],final_Loss: 0.6338\n",
      "Epoch [4/20], Step [2400/12299],final_Loss: 0.3943\n",
      "Epoch [4/20], Step [2500/12299],final_Loss: 0.7740\n",
      "Epoch [4/20], Step [2600/12299],final_Loss: 0.4173\n",
      "Epoch [4/20], Step [2700/12299],final_Loss: 0.6161\n",
      "Epoch [4/20], Step [2800/12299],final_Loss: 0.4212\n",
      "Epoch [4/20], Step [2900/12299],final_Loss: 0.2805\n",
      "Epoch [4/20], Step [3000/12299],final_Loss: 0.5495\n",
      "Epoch [4/20], Step [3100/12299],final_Loss: 1.1241\n",
      "Epoch [4/20], Step [3200/12299],final_Loss: 0.8503\n",
      "Epoch [4/20], Step [3300/12299],final_Loss: 8.3625\n",
      "Epoch [4/20], Step [3400/12299],final_Loss: 0.3329\n",
      "Epoch [4/20], Step [3500/12299],final_Loss: 0.3083\n",
      "Epoch [4/20], Step [3600/12299],final_Loss: 1.1462\n",
      "Epoch [4/20], Step [3700/12299],final_Loss: 0.3969\n",
      "Epoch [4/20], Step [3800/12299],final_Loss: 0.3676\n",
      "Epoch [4/20], Step [3900/12299],final_Loss: 0.3622\n",
      "Epoch [4/20], Step [4000/12299],final_Loss: 6.4529\n",
      "Epoch [4/20], Step [4100/12299],final_Loss: 0.3747\n",
      "Epoch [4/20], Step [4200/12299],final_Loss: 24.3951\n",
      "Epoch [4/20], Step [4300/12299],final_Loss: 1.2553\n",
      "Epoch [4/20], Step [4400/12299],final_Loss: 1.1254\n",
      "Epoch [4/20], Step [4500/12299],final_Loss: 0.4170\n",
      "Epoch [4/20], Step [4600/12299],final_Loss: 0.6431\n",
      "Epoch [4/20], Step [4700/12299],final_Loss: 3.0098\n",
      "Epoch [4/20], Step [4800/12299],final_Loss: 4.3038\n",
      "Epoch [4/20], Step [4900/12299],final_Loss: 6.9575\n",
      "Epoch [4/20], Step [5000/12299],final_Loss: 0.3897\n",
      "Epoch [4/20], Step [5100/12299],final_Loss: 5.7082\n",
      "Epoch [4/20], Step [5200/12299],final_Loss: 1.1631\n",
      "Epoch [4/20], Step [5300/12299],final_Loss: 0.8514\n",
      "Epoch [4/20], Step [5400/12299],final_Loss: 11.2900\n",
      "Epoch [4/20], Step [5500/12299],final_Loss: 2.4083\n",
      "Epoch [4/20], Step [5600/12299],final_Loss: 0.9164\n",
      "Epoch [4/20], Step [5700/12299],final_Loss: 12.5669\n",
      "Epoch [4/20], Step [5800/12299],final_Loss: 0.6663\n",
      "Epoch [4/20], Step [5900/12299],final_Loss: 1.5156\n",
      "Epoch [4/20], Step [6000/12299],final_Loss: 3.4424\n",
      "Epoch [4/20], Step [6100/12299],final_Loss: 1.7572\n",
      "Epoch [4/20], Step [6200/12299],final_Loss: 1.0484\n",
      "Epoch [4/20], Step [6300/12299],final_Loss: 0.3888\n",
      "Epoch [4/20], Step [6400/12299],final_Loss: 0.5963\n",
      "Epoch [4/20], Step [6500/12299],final_Loss: 0.5523\n",
      "Epoch [4/20], Step [6600/12299],final_Loss: 0.9343\n",
      "Epoch [4/20], Step [6700/12299],final_Loss: 1.1043\n",
      "Epoch [4/20], Step [6800/12299],final_Loss: 0.5938\n",
      "Epoch [4/20], Step [6900/12299],final_Loss: 0.4700\n",
      "Epoch [4/20], Step [7000/12299],final_Loss: 0.6439\n",
      "Epoch [4/20], Step [7100/12299],final_Loss: 0.9608\n",
      "Epoch [4/20], Step [7200/12299],final_Loss: 0.2972\n",
      "Epoch [4/20], Step [7300/12299],final_Loss: 1.0503\n",
      "Epoch [4/20], Step [7400/12299],final_Loss: 3.1864\n",
      "Epoch [4/20], Step [7500/12299],final_Loss: 3.4197\n",
      "Epoch [4/20], Step [7600/12299],final_Loss: 1.4303\n",
      "Epoch [4/20], Step [7700/12299],final_Loss: 3.8771\n",
      "Epoch [4/20], Step [7800/12299],final_Loss: 19.6487\n",
      "Epoch [4/20], Step [7900/12299],final_Loss: 0.4214\n",
      "Epoch [4/20], Step [8000/12299],final_Loss: 0.4870\n",
      "Epoch [4/20], Step [8100/12299],final_Loss: 0.3902\n",
      "Epoch [4/20], Step [8200/12299],final_Loss: 2.3239\n",
      "Epoch [4/20], Step [8300/12299],final_Loss: 0.5136\n",
      "Epoch [4/20], Step [8400/12299],final_Loss: 0.8840\n",
      "Epoch [4/20], Step [8500/12299],final_Loss: 0.1838\n",
      "Epoch [4/20], Step [8600/12299],final_Loss: 0.3752\n",
      "Epoch [4/20], Step [8700/12299],final_Loss: 0.1804\n",
      "Epoch [4/20], Step [8800/12299],final_Loss: 0.2126\n",
      "Epoch [4/20], Step [8900/12299],final_Loss: 0.3871\n",
      "Epoch [4/20], Step [9000/12299],final_Loss: 21.0375\n",
      "Epoch [4/20], Step [9100/12299],final_Loss: 3.4769\n",
      "Epoch [4/20], Step [9200/12299],final_Loss: 0.8290\n",
      "Epoch [4/20], Step [9300/12299],final_Loss: 0.9153\n",
      "Epoch [4/20], Step [9400/12299],final_Loss: 11.2917\n",
      "Epoch [4/20], Step [9500/12299],final_Loss: 0.3069\n",
      "Epoch [4/20], Step [9600/12299],final_Loss: 0.5753\n",
      "Epoch [4/20], Step [9700/12299],final_Loss: 0.1877\n",
      "Epoch [4/20], Step [9800/12299],final_Loss: 0.1163\n",
      "Epoch [4/20], Step [9900/12299],final_Loss: 0.0779\n",
      "Epoch [4/20], Step [10000/12299],final_Loss: 0.6100\n",
      "Epoch [4/20], Step [10100/12299],final_Loss: 21.6495\n",
      "Epoch [4/20], Step [10200/12299],final_Loss: 0.2955\n",
      "Epoch [4/20], Step [10300/12299],final_Loss: 5.9176\n",
      "Epoch [4/20], Step [10400/12299],final_Loss: 0.2796\n",
      "Epoch [4/20], Step [10500/12299],final_Loss: 16.4310\n",
      "Epoch [4/20], Step [10600/12299],final_Loss: 0.2947\n",
      "Epoch [4/20], Step [10700/12299],final_Loss: 1.5313\n",
      "Epoch [4/20], Step [10800/12299],final_Loss: 0.1623\n",
      "Epoch [4/20], Step [10900/12299],final_Loss: 0.4133\n",
      "Epoch [4/20], Step [11000/12299],final_Loss: 3.8977\n",
      "Epoch [4/20], Step [11100/12299],final_Loss: 0.8135\n",
      "Epoch [4/20], Step [11200/12299],final_Loss: 5.3998\n",
      "Epoch [4/20], Step [11300/12299],final_Loss: 0.8683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/20], Step [11400/12299],final_Loss: 0.3477\n",
      "Epoch [4/20], Step [11500/12299],final_Loss: 0.5691\n",
      "Epoch [4/20], Step [11600/12299],final_Loss: 0.4835\n",
      "Epoch [4/20], Step [11700/12299],final_Loss: 0.2262\n",
      "Epoch [4/20], Step [11800/12299],final_Loss: 6.1421\n",
      "Epoch [4/20], Step [11900/12299],final_Loss: 0.2681\n",
      "Epoch [4/20], Step [12000/12299],final_Loss: 0.8434\n",
      "Epoch [4/20], Step [12100/12299],final_Loss: 0.2764\n",
      "Epoch [4/20], Step [12200/12299],final_Loss: 0.3379\n",
      "Epoch [5/20], Step [100/12299],final_Loss: 0.2591\n",
      "Epoch [5/20], Step [200/12299],final_Loss: 4.6823\n",
      "Epoch [5/20], Step [300/12299],final_Loss: 0.2501\n",
      "Epoch [5/20], Step [400/12299],final_Loss: 0.6221\n",
      "Epoch [5/20], Step [500/12299],final_Loss: 0.2146\n",
      "Epoch [5/20], Step [600/12299],final_Loss: 0.2056\n",
      "Epoch [5/20], Step [700/12299],final_Loss: 23.3798\n",
      "Epoch [5/20], Step [800/12299],final_Loss: 0.1759\n",
      "Epoch [5/20], Step [900/12299],final_Loss: 0.1094\n",
      "Epoch [5/20], Step [1000/12299],final_Loss: 0.4315\n",
      "Epoch [5/20], Step [1100/12299],final_Loss: 0.1635\n",
      "Epoch [5/20], Step [1200/12299],final_Loss: 0.3439\n",
      "Epoch [5/20], Step [1300/12299],final_Loss: 0.6506\n",
      "Epoch [5/20], Step [1400/12299],final_Loss: 0.1880\n",
      "Epoch [5/20], Step [1500/12299],final_Loss: 2.8769\n",
      "Epoch [5/20], Step [1600/12299],final_Loss: 39.7228\n",
      "Epoch [5/20], Step [1700/12299],final_Loss: 11.2138\n",
      "Epoch [5/20], Step [1800/12299],final_Loss: 0.1719\n",
      "Epoch [5/20], Step [1900/12299],final_Loss: 0.4719\n",
      "Epoch [5/20], Step [2000/12299],final_Loss: 7.4881\n",
      "Epoch [5/20], Step [2100/12299],final_Loss: 0.7693\n",
      "Epoch [5/20], Step [2200/12299],final_Loss: 0.2996\n",
      "Epoch [5/20], Step [2300/12299],final_Loss: 0.4134\n",
      "Epoch [5/20], Step [2400/12299],final_Loss: 0.3907\n",
      "Epoch [5/20], Step [2500/12299],final_Loss: 0.5332\n",
      "Epoch [5/20], Step [2600/12299],final_Loss: 0.3454\n",
      "Epoch [5/20], Step [2700/12299],final_Loss: 0.4506\n",
      "Epoch [5/20], Step [2800/12299],final_Loss: 0.3054\n",
      "Epoch [5/20], Step [2900/12299],final_Loss: 0.2478\n",
      "Epoch [5/20], Step [3000/12299],final_Loss: 0.4880\n",
      "Epoch [5/20], Step [3100/12299],final_Loss: 1.0386\n",
      "Epoch [5/20], Step [3200/12299],final_Loss: 0.7105\n",
      "Epoch [5/20], Step [3300/12299],final_Loss: 7.0282\n",
      "Epoch [5/20], Step [3400/12299],final_Loss: 0.2891\n",
      "Epoch [5/20], Step [3500/12299],final_Loss: 0.2986\n",
      "Epoch [5/20], Step [3600/12299],final_Loss: 1.0543\n",
      "Epoch [5/20], Step [3700/12299],final_Loss: 0.3944\n",
      "Epoch [5/20], Step [3800/12299],final_Loss: 0.3027\n",
      "Epoch [5/20], Step [3900/12299],final_Loss: 0.3016\n",
      "Epoch [5/20], Step [4000/12299],final_Loss: 4.4299\n",
      "Epoch [5/20], Step [4100/12299],final_Loss: 0.2987\n",
      "Epoch [5/20], Step [4200/12299],final_Loss: 29.3318\n",
      "Epoch [5/20], Step [4300/12299],final_Loss: 0.8868\n",
      "Epoch [5/20], Step [4400/12299],final_Loss: 0.6694\n",
      "Epoch [5/20], Step [4500/12299],final_Loss: 0.3796\n",
      "Epoch [5/20], Step [4600/12299],final_Loss: 0.8086\n",
      "Epoch [5/20], Step [4700/12299],final_Loss: 2.8011\n",
      "Epoch [5/20], Step [4800/12299],final_Loss: 3.9636\n",
      "Epoch [5/20], Step [4900/12299],final_Loss: 4.5146\n",
      "Epoch [5/20], Step [5000/12299],final_Loss: 0.3201\n",
      "Epoch [5/20], Step [5100/12299],final_Loss: 7.3153\n",
      "Epoch [5/20], Step [5200/12299],final_Loss: 1.1441\n",
      "Epoch [5/20], Step [5300/12299],final_Loss: 0.7216\n",
      "Epoch [5/20], Step [5400/12299],final_Loss: 13.3281\n",
      "Epoch [5/20], Step [5500/12299],final_Loss: 2.5459\n",
      "Epoch [5/20], Step [5600/12299],final_Loss: 0.8110\n",
      "Epoch [5/20], Step [5700/12299],final_Loss: 10.6897\n",
      "Epoch [5/20], Step [5800/12299],final_Loss: 0.8676\n",
      "Epoch [5/20], Step [5900/12299],final_Loss: 1.7990\n",
      "Epoch [5/20], Step [6000/12299],final_Loss: 3.4403\n",
      "Epoch [5/20], Step [6100/12299],final_Loss: 2.6382\n",
      "Epoch [5/20], Step [6200/12299],final_Loss: 1.2536\n",
      "Epoch [5/20], Step [6300/12299],final_Loss: 0.3422\n",
      "Epoch [5/20], Step [6400/12299],final_Loss: 0.7688\n",
      "Epoch [5/20], Step [6500/12299],final_Loss: 0.6576\n",
      "Epoch [5/20], Step [6600/12299],final_Loss: 0.8964\n",
      "Epoch [5/20], Step [6700/12299],final_Loss: 1.1104\n",
      "Epoch [5/20], Step [6800/12299],final_Loss: 0.6276\n",
      "Epoch [5/20], Step [6900/12299],final_Loss: 0.5358\n",
      "Epoch [5/20], Step [7000/12299],final_Loss: 0.6665\n",
      "Epoch [5/20], Step [7100/12299],final_Loss: 0.8884\n",
      "Epoch [5/20], Step [7200/12299],final_Loss: 0.2836\n",
      "Epoch [5/20], Step [7300/12299],final_Loss: 1.0263\n",
      "Epoch [5/20], Step [7400/12299],final_Loss: 2.3409\n",
      "Epoch [5/20], Step [7500/12299],final_Loss: 3.7193\n",
      "Epoch [5/20], Step [7600/12299],final_Loss: 1.5601\n",
      "Epoch [5/20], Step [7700/12299],final_Loss: 3.8421\n",
      "Epoch [5/20], Step [7800/12299],final_Loss: 28.0981\n",
      "Epoch [5/20], Step [7900/12299],final_Loss: 0.4744\n",
      "Epoch [5/20], Step [8000/12299],final_Loss: 0.5875\n",
      "Epoch [5/20], Step [8100/12299],final_Loss: 0.4591\n",
      "Epoch [5/20], Step [8200/12299],final_Loss: 1.9820\n",
      "Epoch [5/20], Step [8300/12299],final_Loss: 0.5989\n",
      "Epoch [5/20], Step [8400/12299],final_Loss: 1.1108\n",
      "Epoch [5/20], Step [8500/12299],final_Loss: 0.1817\n",
      "Epoch [5/20], Step [8600/12299],final_Loss: 0.3788\n",
      "Epoch [5/20], Step [8700/12299],final_Loss: 0.1901\n",
      "Epoch [5/20], Step [8800/12299],final_Loss: 0.2362\n",
      "Epoch [5/20], Step [8900/12299],final_Loss: 0.5588\n",
      "Epoch [5/20], Step [9000/12299],final_Loss: 27.3051\n",
      "Epoch [5/20], Step [9100/12299],final_Loss: 2.9635\n",
      "Epoch [5/20], Step [9200/12299],final_Loss: 1.0416\n",
      "Epoch [5/20], Step [9300/12299],final_Loss: 0.9609\n",
      "Epoch [5/20], Step [9400/12299],final_Loss: 15.7843\n",
      "Epoch [5/20], Step [9500/12299],final_Loss: 0.3484\n",
      "Epoch [5/20], Step [9600/12299],final_Loss: 0.5076\n",
      "Epoch [5/20], Step [9700/12299],final_Loss: 0.1583\n",
      "Epoch [5/20], Step [9800/12299],final_Loss: 0.1083\n",
      "Epoch [5/20], Step [9900/12299],final_Loss: 0.0742\n",
      "Epoch [5/20], Step [10000/12299],final_Loss: 0.7641\n",
      "Epoch [5/20], Step [10100/12299],final_Loss: 19.3137\n",
      "Epoch [5/20], Step [10200/12299],final_Loss: 0.2693\n",
      "Epoch [5/20], Step [10300/12299],final_Loss: 5.3472\n",
      "Epoch [5/20], Step [10400/12299],final_Loss: 0.2973\n",
      "Epoch [5/20], Step [10500/12299],final_Loss: 13.2483\n",
      "Epoch [5/20], Step [10600/12299],final_Loss: 0.2807\n",
      "Epoch [5/20], Step [10700/12299],final_Loss: 1.2190\n",
      "Epoch [5/20], Step [10800/12299],final_Loss: 0.1541\n",
      "Epoch [5/20], Step [10900/12299],final_Loss: 0.5327\n",
      "Epoch [5/20], Step [11000/12299],final_Loss: 4.1935\n",
      "Epoch [5/20], Step [11100/12299],final_Loss: 0.7694\n",
      "Epoch [5/20], Step [11200/12299],final_Loss: 6.1675\n",
      "Epoch [5/20], Step [11300/12299],final_Loss: 0.9792\n",
      "Epoch [5/20], Step [11400/12299],final_Loss: 0.3786\n",
      "Epoch [5/20], Step [11500/12299],final_Loss: 0.5859\n",
      "Epoch [5/20], Step [11600/12299],final_Loss: 0.4660\n",
      "Epoch [5/20], Step [11700/12299],final_Loss: 0.2210\n",
      "Epoch [5/20], Step [11800/12299],final_Loss: 6.5726\n",
      "Epoch [5/20], Step [11900/12299],final_Loss: 0.2870\n",
      "Epoch [5/20], Step [12000/12299],final_Loss: 0.9375\n",
      "Epoch [5/20], Step [12100/12299],final_Loss: 0.3088\n",
      "Epoch [5/20], Step [12200/12299],final_Loss: 0.3462\n",
      "Epoch [6/20], Step [100/12299],final_Loss: 0.2500\n",
      "Epoch [6/20], Step [200/12299],final_Loss: 4.0998\n",
      "Epoch [6/20], Step [300/12299],final_Loss: 0.2366\n",
      "Epoch [6/20], Step [400/12299],final_Loss: 0.7056\n",
      "Epoch [6/20], Step [500/12299],final_Loss: 0.2399\n",
      "Epoch [6/20], Step [600/12299],final_Loss: 0.1595\n",
      "Epoch [6/20], Step [700/12299],final_Loss: 22.7119\n",
      "Epoch [6/20], Step [800/12299],final_Loss: 0.1388\n",
      "Epoch [6/20], Step [900/12299],final_Loss: 0.0870\n",
      "Epoch [6/20], Step [1000/12299],final_Loss: 0.3242\n",
      "Epoch [6/20], Step [1100/12299],final_Loss: 0.1374\n",
      "Epoch [6/20], Step [1200/12299],final_Loss: 0.3352\n",
      "Epoch [6/20], Step [1300/12299],final_Loss: 0.6304\n",
      "Epoch [6/20], Step [1400/12299],final_Loss: 0.1807\n",
      "Epoch [6/20], Step [1500/12299],final_Loss: 2.1249\n",
      "Epoch [6/20], Step [1600/12299],final_Loss: 41.6963\n",
      "Epoch [6/20], Step [1700/12299],final_Loss: 16.2210\n",
      "Epoch [6/20], Step [1800/12299],final_Loss: 0.1730\n",
      "Epoch [6/20], Step [1900/12299],final_Loss: 0.5300\n",
      "Epoch [6/20], Step [2000/12299],final_Loss: 7.3679\n",
      "Epoch [6/20], Step [2100/12299],final_Loss: 0.9086\n",
      "Epoch [6/20], Step [2200/12299],final_Loss: 0.2929\n",
      "Epoch [6/20], Step [2300/12299],final_Loss: 0.5806\n",
      "Epoch [6/20], Step [2400/12299],final_Loss: 0.3585\n",
      "Epoch [6/20], Step [2500/12299],final_Loss: 0.5498\n",
      "Epoch [6/20], Step [2600/12299],final_Loss: 0.2730\n",
      "Epoch [6/20], Step [2700/12299],final_Loss: 0.4168\n",
      "Epoch [6/20], Step [2800/12299],final_Loss: 0.3296\n",
      "Epoch [6/20], Step [2900/12299],final_Loss: 0.2667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/20], Step [3000/12299],final_Loss: 0.5528\n",
      "Epoch [6/20], Step [3100/12299],final_Loss: 0.9858\n",
      "Epoch [6/20], Step [3200/12299],final_Loss: 0.7607\n",
      "Epoch [6/20], Step [3300/12299],final_Loss: 7.7250\n",
      "Epoch [6/20], Step [3400/12299],final_Loss: 0.2435\n",
      "Epoch [6/20], Step [3500/12299],final_Loss: 0.2761\n",
      "Epoch [6/20], Step [3600/12299],final_Loss: 1.2497\n",
      "Epoch [6/20], Step [3700/12299],final_Loss: 0.4458\n",
      "Epoch [6/20], Step [3800/12299],final_Loss: 0.3008\n",
      "Epoch [6/20], Step [3900/12299],final_Loss: 0.2810\n",
      "Epoch [6/20], Step [4000/12299],final_Loss: 4.1078\n",
      "Epoch [6/20], Step [4100/12299],final_Loss: 0.3073\n",
      "Epoch [6/20], Step [4200/12299],final_Loss: 34.6979\n",
      "Epoch [6/20], Step [4300/12299],final_Loss: 0.9324\n",
      "Epoch [6/20], Step [4400/12299],final_Loss: 0.6570\n",
      "Epoch [6/20], Step [4500/12299],final_Loss: 0.3421\n",
      "Epoch [6/20], Step [4600/12299],final_Loss: 0.5026\n",
      "Epoch [6/20], Step [4700/12299],final_Loss: 2.4485\n",
      "Epoch [6/20], Step [4800/12299],final_Loss: 3.8352\n",
      "Epoch [6/20], Step [4900/12299],final_Loss: 5.8261\n",
      "Epoch [6/20], Step [5000/12299],final_Loss: 0.3185\n",
      "Epoch [6/20], Step [5100/12299],final_Loss: 5.1065\n",
      "Epoch [6/20], Step [5200/12299],final_Loss: 0.8712\n",
      "Epoch [6/20], Step [5300/12299],final_Loss: 0.6090\n",
      "Epoch [6/20], Step [5400/12299],final_Loss: 8.7177\n",
      "Epoch [6/20], Step [5500/12299],final_Loss: 2.1020\n",
      "Epoch [6/20], Step [5600/12299],final_Loss: 0.8443\n",
      "Epoch [6/20], Step [5700/12299],final_Loss: 11.4783\n",
      "Epoch [6/20], Step [5800/12299],final_Loss: 0.5711\n",
      "Epoch [6/20], Step [5900/12299],final_Loss: 1.0436\n",
      "Epoch [6/20], Step [6000/12299],final_Loss: 2.0333\n",
      "Epoch [6/20], Step [6100/12299],final_Loss: 1.2423\n",
      "Epoch [6/20], Step [6200/12299],final_Loss: 1.4327\n",
      "Epoch [6/20], Step [6300/12299],final_Loss: 0.3758\n",
      "Epoch [6/20], Step [6400/12299],final_Loss: 0.5694\n",
      "Epoch [6/20], Step [6500/12299],final_Loss: 0.6965\n",
      "Epoch [6/20], Step [6600/12299],final_Loss: 1.0238\n",
      "Epoch [6/20], Step [6700/12299],final_Loss: 1.3194\n",
      "Epoch [6/20], Step [6800/12299],final_Loss: 0.5435\n",
      "Epoch [6/20], Step [6900/12299],final_Loss: 0.4367\n",
      "Epoch [6/20], Step [7000/12299],final_Loss: 0.6053\n",
      "Epoch [6/20], Step [7100/12299],final_Loss: 0.7292\n",
      "Epoch [6/20], Step [7200/12299],final_Loss: 0.2455\n",
      "Epoch [6/20], Step [7300/12299],final_Loss: 0.8065\n",
      "Epoch [6/20], Step [7400/12299],final_Loss: 2.5966\n",
      "Epoch [6/20], Step [7500/12299],final_Loss: 3.4023\n",
      "Epoch [6/20], Step [7600/12299],final_Loss: 1.4133\n",
      "Epoch [6/20], Step [7700/12299],final_Loss: 2.3532\n",
      "Epoch [6/20], Step [7800/12299],final_Loss: 24.9778\n",
      "Epoch [6/20], Step [7900/12299],final_Loss: 0.5340\n",
      "Epoch [6/20], Step [8000/12299],final_Loss: 0.4081\n",
      "Epoch [6/20], Step [8100/12299],final_Loss: 0.4026\n",
      "Epoch [6/20], Step [8200/12299],final_Loss: 1.4633\n",
      "Epoch [6/20], Step [8300/12299],final_Loss: 0.4600\n",
      "Epoch [6/20], Step [8400/12299],final_Loss: 0.5719\n",
      "Epoch [6/20], Step [8500/12299],final_Loss: 0.1494\n",
      "Epoch [6/20], Step [8600/12299],final_Loss: 0.3899\n",
      "Epoch [6/20], Step [8700/12299],final_Loss: 0.1678\n",
      "Epoch [6/20], Step [8800/12299],final_Loss: 0.2549\n",
      "Epoch [6/20], Step [8900/12299],final_Loss: 0.4700\n",
      "Epoch [6/20], Step [9000/12299],final_Loss: 24.2325\n",
      "Epoch [6/20], Step [9100/12299],final_Loss: 2.8256\n",
      "Epoch [6/20], Step [9200/12299],final_Loss: 0.6253\n",
      "Epoch [6/20], Step [9300/12299],final_Loss: 0.8819\n",
      "Epoch [6/20], Step [9400/12299],final_Loss: 17.0433\n",
      "Epoch [6/20], Step [9500/12299],final_Loss: 0.3135\n",
      "Epoch [6/20], Step [9600/12299],final_Loss: 0.5827\n",
      "Epoch [6/20], Step [9700/12299],final_Loss: 0.1735\n",
      "Epoch [6/20], Step [9800/12299],final_Loss: 0.1093\n",
      "Epoch [6/20], Step [9900/12299],final_Loss: 0.0796\n",
      "Epoch [6/20], Step [10000/12299],final_Loss: 0.7222\n",
      "Epoch [6/20], Step [10100/12299],final_Loss: 19.5329\n",
      "Epoch [6/20], Step [10200/12299],final_Loss: 0.3274\n",
      "Epoch [6/20], Step [10300/12299],final_Loss: 4.8523\n",
      "Epoch [6/20], Step [10400/12299],final_Loss: 0.2879\n",
      "Epoch [6/20], Step [10500/12299],final_Loss: 15.9776\n",
      "Epoch [6/20], Step [10600/12299],final_Loss: 0.2937\n",
      "Epoch [6/20], Step [10700/12299],final_Loss: 0.9444\n",
      "Epoch [6/20], Step [10800/12299],final_Loss: 0.1465\n",
      "Epoch [6/20], Step [10900/12299],final_Loss: 0.5672\n",
      "Epoch [6/20], Step [11000/12299],final_Loss: 3.6677\n",
      "Epoch [6/20], Step [11100/12299],final_Loss: 0.6825\n",
      "Epoch [6/20], Step [11200/12299],final_Loss: 3.7483\n",
      "Epoch [6/20], Step [11300/12299],final_Loss: 0.7782\n",
      "Epoch [6/20], Step [11400/12299],final_Loss: 0.3343\n",
      "Epoch [6/20], Step [11500/12299],final_Loss: 0.4687\n",
      "Epoch [6/20], Step [11600/12299],final_Loss: 0.3816\n",
      "Epoch [6/20], Step [11700/12299],final_Loss: 0.1913\n",
      "Epoch [6/20], Step [11800/12299],final_Loss: 5.3089\n",
      "Epoch [6/20], Step [11900/12299],final_Loss: 0.2477\n",
      "Epoch [6/20], Step [12000/12299],final_Loss: 0.6548\n",
      "Epoch [6/20], Step [12100/12299],final_Loss: 0.2643\n",
      "Epoch [6/20], Step [12200/12299],final_Loss: 0.3396\n",
      "Epoch [7/20], Step [100/12299],final_Loss: 0.2377\n",
      "Epoch [7/20], Step [200/12299],final_Loss: 2.9117\n",
      "Epoch [7/20], Step [300/12299],final_Loss: 0.2146\n",
      "Epoch [7/20], Step [400/12299],final_Loss: 0.6086\n",
      "Epoch [7/20], Step [500/12299],final_Loss: 0.2145\n",
      "Epoch [7/20], Step [600/12299],final_Loss: 0.1526\n",
      "Epoch [7/20], Step [700/12299],final_Loss: 22.0553\n",
      "Epoch [7/20], Step [800/12299],final_Loss: 0.1518\n",
      "Epoch [7/20], Step [900/12299],final_Loss: 0.0932\n",
      "Epoch [7/20], Step [1000/12299],final_Loss: 0.3418\n",
      "Epoch [7/20], Step [1100/12299],final_Loss: 0.1377\n",
      "Epoch [7/20], Step [1200/12299],final_Loss: 0.3347\n",
      "Epoch [7/20], Step [1300/12299],final_Loss: 0.6455\n",
      "Epoch [7/20], Step [1400/12299],final_Loss: 0.2046\n",
      "Epoch [7/20], Step [1500/12299],final_Loss: 1.5870\n",
      "Epoch [7/20], Step [1600/12299],final_Loss: 47.3946\n",
      "Epoch [7/20], Step [1700/12299],final_Loss: 14.7236\n",
      "Epoch [7/20], Step [1800/12299],final_Loss: 0.1637\n",
      "Epoch [7/20], Step [1900/12299],final_Loss: 0.4440\n",
      "Epoch [7/20], Step [2000/12299],final_Loss: 9.8107\n",
      "Epoch [7/20], Step [2100/12299],final_Loss: 0.8209\n",
      "Epoch [7/20], Step [2200/12299],final_Loss: 0.3014\n",
      "Epoch [7/20], Step [2300/12299],final_Loss: 0.3867\n",
      "Epoch [7/20], Step [2400/12299],final_Loss: 0.3949\n",
      "Epoch [7/20], Step [2500/12299],final_Loss: 0.5724\n",
      "Epoch [7/20], Step [2600/12299],final_Loss: 0.3163\n",
      "Epoch [7/20], Step [2700/12299],final_Loss: 0.3903\n",
      "Epoch [7/20], Step [2800/12299],final_Loss: 0.2984\n",
      "Epoch [7/20], Step [2900/12299],final_Loss: 0.2349\n",
      "Epoch [7/20], Step [3000/12299],final_Loss: 0.4199\n",
      "Epoch [7/20], Step [3100/12299],final_Loss: 0.8898\n",
      "Epoch [7/20], Step [3200/12299],final_Loss: 0.7162\n",
      "Epoch [7/20], Step [3300/12299],final_Loss: 6.7888\n",
      "Epoch [7/20], Step [3400/12299],final_Loss: 0.2675\n",
      "Epoch [7/20], Step [3500/12299],final_Loss: 0.2346\n",
      "Epoch [7/20], Step [3600/12299],final_Loss: 1.1112\n",
      "Epoch [7/20], Step [3700/12299],final_Loss: 0.4098\n",
      "Epoch [7/20], Step [3800/12299],final_Loss: 0.3017\n",
      "Epoch [7/20], Step [3900/12299],final_Loss: 0.2456\n",
      "Epoch [7/20], Step [4000/12299],final_Loss: 6.4439\n",
      "Epoch [7/20], Step [4100/12299],final_Loss: 0.3957\n",
      "Epoch [7/20], Step [4200/12299],final_Loss: 29.9738\n",
      "Epoch [7/20], Step [4300/12299],final_Loss: 1.2728\n",
      "Epoch [7/20], Step [4400/12299],final_Loss: 0.6388\n",
      "Epoch [7/20], Step [4500/12299],final_Loss: 0.3617\n",
      "Epoch [7/20], Step [4600/12299],final_Loss: 0.6269\n",
      "Epoch [7/20], Step [4700/12299],final_Loss: 2.6314\n",
      "Epoch [7/20], Step [4800/12299],final_Loss: 6.2507\n",
      "Epoch [7/20], Step [4900/12299],final_Loss: 6.2966\n",
      "Epoch [7/20], Step [5000/12299],final_Loss: 0.2943\n",
      "Epoch [7/20], Step [5100/12299],final_Loss: 5.8377\n",
      "Epoch [7/20], Step [5200/12299],final_Loss: 0.9878\n",
      "Epoch [7/20], Step [5300/12299],final_Loss: 0.6339\n",
      "Epoch [7/20], Step [5400/12299],final_Loss: 11.1793\n",
      "Epoch [7/20], Step [5500/12299],final_Loss: 2.0174\n",
      "Epoch [7/20], Step [5600/12299],final_Loss: 0.8215\n",
      "Epoch [7/20], Step [5700/12299],final_Loss: 12.9124\n",
      "Epoch [7/20], Step [5800/12299],final_Loss: 0.7551\n",
      "Epoch [7/20], Step [5900/12299],final_Loss: 1.1243\n",
      "Epoch [7/20], Step [6000/12299],final_Loss: 2.3017\n",
      "Epoch [7/20], Step [6100/12299],final_Loss: 1.8545\n",
      "Epoch [7/20], Step [6200/12299],final_Loss: 1.4437\n",
      "Epoch [7/20], Step [6300/12299],final_Loss: 0.3442\n",
      "Epoch [7/20], Step [6400/12299],final_Loss: 0.8462\n",
      "Epoch [7/20], Step [6500/12299],final_Loss: 0.8867\n",
      "Epoch [7/20], Step [6600/12299],final_Loss: 1.1878\n",
      "Epoch [7/20], Step [6700/12299],final_Loss: 1.4775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/20], Step [6800/12299],final_Loss: 0.5280\n",
      "Epoch [7/20], Step [6900/12299],final_Loss: 0.4243\n",
      "Epoch [7/20], Step [7000/12299],final_Loss: 0.6927\n",
      "Epoch [7/20], Step [7100/12299],final_Loss: 0.8599\n",
      "Epoch [7/20], Step [7200/12299],final_Loss: 0.2707\n",
      "Epoch [7/20], Step [7300/12299],final_Loss: 0.8231\n",
      "Epoch [7/20], Step [7400/12299],final_Loss: 1.9001\n",
      "Epoch [7/20], Step [7500/12299],final_Loss: 3.1421\n",
      "Epoch [7/20], Step [7600/12299],final_Loss: 1.4166\n",
      "Epoch [7/20], Step [7700/12299],final_Loss: 2.6942\n",
      "Epoch [7/20], Step [7800/12299],final_Loss: 27.3979\n",
      "Epoch [7/20], Step [7900/12299],final_Loss: 0.5795\n",
      "Epoch [7/20], Step [8000/12299],final_Loss: 0.4621\n",
      "Epoch [7/20], Step [8100/12299],final_Loss: 0.4448\n",
      "Epoch [7/20], Step [8200/12299],final_Loss: 1.6096\n",
      "Epoch [7/20], Step [8300/12299],final_Loss: 0.4984\n",
      "Epoch [7/20], Step [8400/12299],final_Loss: 0.8508\n",
      "Epoch [7/20], Step [8500/12299],final_Loss: 0.1984\n",
      "Epoch [7/20], Step [8600/12299],final_Loss: 0.3865\n",
      "Epoch [7/20], Step [8700/12299],final_Loss: 0.1702\n",
      "Epoch [7/20], Step [8800/12299],final_Loss: 0.2647\n",
      "Epoch [7/20], Step [8900/12299],final_Loss: 0.5102\n",
      "Epoch [7/20], Step [9000/12299],final_Loss: 26.1544\n",
      "Epoch [7/20], Step [9100/12299],final_Loss: 3.4782\n",
      "Epoch [7/20], Step [9200/12299],final_Loss: 0.8305\n",
      "Epoch [7/20], Step [9300/12299],final_Loss: 1.6001\n",
      "Epoch [7/20], Step [9400/12299],final_Loss: 15.8872\n",
      "Epoch [7/20], Step [9500/12299],final_Loss: 0.3389\n",
      "Epoch [7/20], Step [9600/12299],final_Loss: 0.6274\n",
      "Epoch [7/20], Step [9700/12299],final_Loss: 0.2058\n",
      "Epoch [7/20], Step [9800/12299],final_Loss: 0.1184\n",
      "Epoch [7/20], Step [9900/12299],final_Loss: 0.0814\n",
      "Epoch [7/20], Step [10000/12299],final_Loss: 0.7553\n",
      "Epoch [7/20], Step [10100/12299],final_Loss: 18.5518\n",
      "Epoch [7/20], Step [10200/12299],final_Loss: 0.3346\n",
      "Epoch [7/20], Step [10300/12299],final_Loss: 4.7270\n",
      "Epoch [7/20], Step [10400/12299],final_Loss: 0.2883\n",
      "Epoch [7/20], Step [10500/12299],final_Loss: 17.8805\n",
      "Epoch [7/20], Step [10600/12299],final_Loss: 0.2879\n",
      "Epoch [7/20], Step [10700/12299],final_Loss: 0.9915\n",
      "Epoch [7/20], Step [10800/12299],final_Loss: 0.1707\n",
      "Epoch [7/20], Step [10900/12299],final_Loss: 0.7812\n",
      "Epoch [7/20], Step [11000/12299],final_Loss: 4.9665\n",
      "Epoch [7/20], Step [11100/12299],final_Loss: 0.8193\n",
      "Epoch [7/20], Step [11200/12299],final_Loss: 3.3987\n",
      "Epoch [7/20], Step [11300/12299],final_Loss: 0.7906\n",
      "Epoch [7/20], Step [11400/12299],final_Loss: 0.3625\n",
      "Epoch [7/20], Step [11500/12299],final_Loss: 0.6182\n",
      "Epoch [7/20], Step [11600/12299],final_Loss: 0.4607\n",
      "Epoch [7/20], Step [11700/12299],final_Loss: 0.2212\n",
      "Epoch [7/20], Step [11800/12299],final_Loss: 5.1499\n",
      "Epoch [7/20], Step [11900/12299],final_Loss: 0.2469\n",
      "Epoch [7/20], Step [12000/12299],final_Loss: 0.6966\n",
      "Epoch [7/20], Step [12100/12299],final_Loss: 0.2816\n",
      "Epoch [7/20], Step [12200/12299],final_Loss: 0.3102\n",
      "Epoch [8/20], Step [100/12299],final_Loss: 0.2337\n",
      "Epoch [8/20], Step [200/12299],final_Loss: 3.2923\n",
      "Epoch [8/20], Step [300/12299],final_Loss: 0.2377\n",
      "Epoch [8/20], Step [400/12299],final_Loss: 0.6983\n",
      "Epoch [8/20], Step [500/12299],final_Loss: 0.2352\n",
      "Epoch [8/20], Step [600/12299],final_Loss: 0.1643\n",
      "Epoch [8/20], Step [700/12299],final_Loss: 17.3688\n",
      "Epoch [8/20], Step [800/12299],final_Loss: 0.1532\n",
      "Epoch [8/20], Step [900/12299],final_Loss: 0.1124\n",
      "Epoch [8/20], Step [1000/12299],final_Loss: 0.5457\n",
      "Epoch [8/20], Step [1100/12299],final_Loss: 0.1924\n",
      "Epoch [8/20], Step [1200/12299],final_Loss: 0.3629\n",
      "Epoch [8/20], Step [1300/12299],final_Loss: 0.5555\n",
      "Epoch [8/20], Step [1400/12299],final_Loss: 0.1900\n",
      "Epoch [8/20], Step [1500/12299],final_Loss: 1.6570\n",
      "Epoch [8/20], Step [1600/12299],final_Loss: 42.9209\n",
      "Epoch [8/20], Step [1700/12299],final_Loss: 12.7627\n",
      "Epoch [8/20], Step [1800/12299],final_Loss: 0.2134\n",
      "Epoch [8/20], Step [1900/12299],final_Loss: 0.5060\n",
      "Epoch [8/20], Step [2000/12299],final_Loss: 6.9454\n",
      "Epoch [8/20], Step [2100/12299],final_Loss: 0.8301\n",
      "Epoch [8/20], Step [2200/12299],final_Loss: 0.4777\n",
      "Epoch [8/20], Step [2300/12299],final_Loss: 0.5876\n",
      "Epoch [8/20], Step [2400/12299],final_Loss: 0.4149\n",
      "Epoch [8/20], Step [2500/12299],final_Loss: 0.5743\n",
      "Epoch [8/20], Step [2600/12299],final_Loss: 0.2830\n",
      "Epoch [8/20], Step [2700/12299],final_Loss: 0.4811\n",
      "Epoch [8/20], Step [2800/12299],final_Loss: 0.3642\n",
      "Epoch [8/20], Step [2900/12299],final_Loss: 0.2488\n",
      "Epoch [8/20], Step [3000/12299],final_Loss: 0.4444\n",
      "Epoch [8/20], Step [3100/12299],final_Loss: 0.9821\n",
      "Epoch [8/20], Step [3200/12299],final_Loss: 0.7121\n",
      "Epoch [8/20], Step [3300/12299],final_Loss: 5.4737\n",
      "Epoch [8/20], Step [3400/12299],final_Loss: 0.2412\n",
      "Epoch [8/20], Step [3500/12299],final_Loss: 0.2836\n",
      "Epoch [8/20], Step [3600/12299],final_Loss: 1.1339\n",
      "Epoch [8/20], Step [3700/12299],final_Loss: 0.5585\n",
      "Epoch [8/20], Step [3800/12299],final_Loss: 0.3408\n",
      "Epoch [8/20], Step [3900/12299],final_Loss: 0.2304\n",
      "Epoch [8/20], Step [4000/12299],final_Loss: 4.5830\n",
      "Epoch [8/20], Step [4100/12299],final_Loss: 0.3545\n",
      "Epoch [8/20], Step [4200/12299],final_Loss: 35.4579\n",
      "Epoch [8/20], Step [4300/12299],final_Loss: 1.0274\n",
      "Epoch [8/20], Step [4400/12299],final_Loss: 0.6663\n",
      "Epoch [8/20], Step [4500/12299],final_Loss: 0.2584\n",
      "Epoch [8/20], Step [4600/12299],final_Loss: 0.7073\n",
      "Epoch [8/20], Step [4700/12299],final_Loss: 2.7463\n",
      "Epoch [8/20], Step [4800/12299],final_Loss: 4.9713\n",
      "Epoch [8/20], Step [4900/12299],final_Loss: 5.5344\n",
      "Epoch [8/20], Step [5000/12299],final_Loss: 0.2791\n",
      "Epoch [8/20], Step [5100/12299],final_Loss: 4.8911\n",
      "Epoch [8/20], Step [5200/12299],final_Loss: 0.8567\n",
      "Epoch [8/20], Step [5300/12299],final_Loss: 0.6561\n",
      "Epoch [8/20], Step [5400/12299],final_Loss: 8.7800\n",
      "Epoch [8/20], Step [5500/12299],final_Loss: 2.4325\n",
      "Epoch [8/20], Step [5600/12299],final_Loss: 0.8805\n",
      "Epoch [8/20], Step [5700/12299],final_Loss: 10.7481\n",
      "Epoch [8/20], Step [5800/12299],final_Loss: 1.0807\n",
      "Epoch [8/20], Step [5900/12299],final_Loss: 1.4720\n",
      "Epoch [8/20], Step [6000/12299],final_Loss: 2.2975\n",
      "Epoch [8/20], Step [6100/12299],final_Loss: 2.0357\n",
      "Epoch [8/20], Step [6200/12299],final_Loss: 1.4327\n",
      "Epoch [8/20], Step [6300/12299],final_Loss: 0.3669\n",
      "Epoch [8/20], Step [6400/12299],final_Loss: 0.9062\n",
      "Epoch [8/20], Step [6500/12299],final_Loss: 0.7545\n",
      "Epoch [8/20], Step [6600/12299],final_Loss: 1.0705\n",
      "Epoch [8/20], Step [6700/12299],final_Loss: 1.6254\n",
      "Epoch [8/20], Step [6800/12299],final_Loss: 0.7011\n",
      "Epoch [8/20], Step [6900/12299],final_Loss: 0.6446\n",
      "Epoch [8/20], Step [7000/12299],final_Loss: 0.8187\n",
      "Epoch [8/20], Step [7100/12299],final_Loss: 0.9013\n",
      "Epoch [8/20], Step [7200/12299],final_Loss: 0.2840\n",
      "Epoch [8/20], Step [7300/12299],final_Loss: 0.7701\n",
      "Epoch [8/20], Step [7400/12299],final_Loss: 2.3355\n",
      "Epoch [8/20], Step [7500/12299],final_Loss: 3.2334\n",
      "Epoch [8/20], Step [7600/12299],final_Loss: 1.1279\n",
      "Epoch [8/20], Step [7700/12299],final_Loss: 2.6261\n",
      "Epoch [8/20], Step [7800/12299],final_Loss: 23.8493\n",
      "Epoch [8/20], Step [7900/12299],final_Loss: 0.4772\n",
      "Epoch [8/20], Step [8000/12299],final_Loss: 0.4538\n",
      "Epoch [8/20], Step [8100/12299],final_Loss: 0.4754\n",
      "Epoch [8/20], Step [8200/12299],final_Loss: 1.4488\n",
      "Epoch [8/20], Step [8300/12299],final_Loss: 0.7407\n",
      "Epoch [8/20], Step [8400/12299],final_Loss: 0.7424\n",
      "Epoch [8/20], Step [8500/12299],final_Loss: 0.1570\n",
      "Epoch [8/20], Step [8600/12299],final_Loss: 0.3198\n",
      "Epoch [8/20], Step [8700/12299],final_Loss: 0.1626\n",
      "Epoch [8/20], Step [8800/12299],final_Loss: 0.2624\n",
      "Epoch [8/20], Step [8900/12299],final_Loss: 0.5232\n",
      "Epoch [8/20], Step [9000/12299],final_Loss: 28.1248\n",
      "Epoch [8/20], Step [9100/12299],final_Loss: 3.3170\n",
      "Epoch [8/20], Step [9200/12299],final_Loss: 0.7860\n",
      "Epoch [8/20], Step [9300/12299],final_Loss: 1.7138\n",
      "Epoch [8/20], Step [9400/12299],final_Loss: 16.8737\n",
      "Epoch [8/20], Step [9500/12299],final_Loss: 0.3316\n",
      "Epoch [8/20], Step [9600/12299],final_Loss: 1.1763\n",
      "Epoch [8/20], Step [9700/12299],final_Loss: 0.2090\n",
      "Epoch [8/20], Step [9800/12299],final_Loss: 0.1294\n",
      "Epoch [8/20], Step [9900/12299],final_Loss: 0.0847\n",
      "Epoch [8/20], Step [10000/12299],final_Loss: 1.0634\n",
      "Epoch [8/20], Step [10100/12299],final_Loss: 18.6154\n",
      "Epoch [8/20], Step [10200/12299],final_Loss: 0.3223\n",
      "Epoch [8/20], Step [10300/12299],final_Loss: 6.2610\n",
      "Epoch [8/20], Step [10400/12299],final_Loss: 0.3158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/20], Step [10500/12299],final_Loss: 17.6143\n",
      "Epoch [8/20], Step [10600/12299],final_Loss: 0.2484\n",
      "Epoch [8/20], Step [10700/12299],final_Loss: 0.8636\n",
      "Epoch [8/20], Step [10800/12299],final_Loss: 0.1574\n",
      "Epoch [8/20], Step [10900/12299],final_Loss: 0.7035\n",
      "Epoch [8/20], Step [11000/12299],final_Loss: 5.6727\n",
      "Epoch [8/20], Step [11100/12299],final_Loss: 0.9140\n",
      "Epoch [8/20], Step [11200/12299],final_Loss: 4.8019\n",
      "Epoch [8/20], Step [11300/12299],final_Loss: 0.8090\n",
      "Epoch [8/20], Step [11400/12299],final_Loss: 0.3858\n",
      "Epoch [8/20], Step [11500/12299],final_Loss: 0.5454\n",
      "Epoch [8/20], Step [11600/12299],final_Loss: 0.3825\n",
      "Epoch [8/20], Step [11700/12299],final_Loss: 0.1975\n",
      "Epoch [8/20], Step [11800/12299],final_Loss: 6.5003\n",
      "Epoch [8/20], Step [11900/12299],final_Loss: 0.2855\n",
      "Epoch [8/20], Step [12000/12299],final_Loss: 0.6012\n",
      "Epoch [8/20], Step [12100/12299],final_Loss: 0.2750\n",
      "Epoch [8/20], Step [12200/12299],final_Loss: 0.3585\n",
      "Epoch [9/20], Step [100/12299],final_Loss: 0.2624\n",
      "Epoch [9/20], Step [200/12299],final_Loss: 3.6136\n",
      "Epoch [9/20], Step [300/12299],final_Loss: 0.2507\n",
      "Epoch [9/20], Step [400/12299],final_Loss: 0.5714\n",
      "Epoch [9/20], Step [500/12299],final_Loss: 0.2077\n",
      "Epoch [9/20], Step [600/12299],final_Loss: 0.1350\n",
      "Epoch [9/20], Step [700/12299],final_Loss: 17.8593\n",
      "Epoch [9/20], Step [800/12299],final_Loss: 0.1564\n",
      "Epoch [9/20], Step [900/12299],final_Loss: 0.0992\n",
      "Epoch [9/20], Step [1000/12299],final_Loss: 0.3976\n",
      "Epoch [9/20], Step [1100/12299],final_Loss: 0.1743\n",
      "Epoch [9/20], Step [1200/12299],final_Loss: 0.3180\n",
      "Epoch [9/20], Step [1300/12299],final_Loss: 0.5710\n",
      "Epoch [9/20], Step [1400/12299],final_Loss: 0.1712\n",
      "Epoch [9/20], Step [1500/12299],final_Loss: 1.8039\n",
      "Epoch [9/20], Step [1600/12299],final_Loss: 47.7592\n",
      "Epoch [9/20], Step [1700/12299],final_Loss: 14.8389\n",
      "Epoch [9/20], Step [1800/12299],final_Loss: 0.2189\n",
      "Epoch [9/20], Step [1900/12299],final_Loss: 0.4449\n",
      "Epoch [9/20], Step [2000/12299],final_Loss: 6.2380\n",
      "Epoch [9/20], Step [2100/12299],final_Loss: 0.7912\n",
      "Epoch [9/20], Step [2200/12299],final_Loss: 0.3597\n",
      "Epoch [9/20], Step [2300/12299],final_Loss: 0.5282\n",
      "Epoch [9/20], Step [2400/12299],final_Loss: 0.3677\n",
      "Epoch [9/20], Step [2500/12299],final_Loss: 0.4409\n",
      "Epoch [9/20], Step [2600/12299],final_Loss: 0.2676\n",
      "Epoch [9/20], Step [2700/12299],final_Loss: 0.4246\n",
      "Epoch [9/20], Step [2800/12299],final_Loss: 0.2675\n",
      "Epoch [9/20], Step [2900/12299],final_Loss: 0.2753\n",
      "Epoch [9/20], Step [3000/12299],final_Loss: 0.4253\n",
      "Epoch [9/20], Step [3100/12299],final_Loss: 0.8870\n",
      "Epoch [9/20], Step [3200/12299],final_Loss: 0.6993\n",
      "Epoch [9/20], Step [3300/12299],final_Loss: 8.2033\n",
      "Epoch [9/20], Step [3400/12299],final_Loss: 0.2606\n",
      "Epoch [9/20], Step [3500/12299],final_Loss: 0.2596\n",
      "Epoch [9/20], Step [3600/12299],final_Loss: 0.9600\n",
      "Epoch [9/20], Step [3700/12299],final_Loss: 0.5234\n",
      "Epoch [9/20], Step [3800/12299],final_Loss: 0.3451\n",
      "Epoch [9/20], Step [3900/12299],final_Loss: 0.3052\n",
      "Epoch [9/20], Step [4000/12299],final_Loss: 5.2881\n",
      "Epoch [9/20], Step [4100/12299],final_Loss: 0.3811\n",
      "Epoch [9/20], Step [4200/12299],final_Loss: 34.6509\n",
      "Epoch [9/20], Step [4300/12299],final_Loss: 1.2975\n",
      "Epoch [9/20], Step [4400/12299],final_Loss: 0.6505\n",
      "Epoch [9/20], Step [4500/12299],final_Loss: 0.3514\n",
      "Epoch [9/20], Step [4600/12299],final_Loss: 0.9545\n",
      "Epoch [9/20], Step [4700/12299],final_Loss: 3.4807\n",
      "Epoch [9/20], Step [4800/12299],final_Loss: 5.1907\n",
      "Epoch [9/20], Step [4900/12299],final_Loss: 4.7243\n",
      "Epoch [9/20], Step [5000/12299],final_Loss: 0.3002\n",
      "Epoch [9/20], Step [5100/12299],final_Loss: 6.8182\n",
      "Epoch [9/20], Step [5200/12299],final_Loss: 1.1873\n",
      "Epoch [9/20], Step [5300/12299],final_Loss: 0.6845\n",
      "Epoch [9/20], Step [5400/12299],final_Loss: 10.9069\n",
      "Epoch [9/20], Step [5500/12299],final_Loss: 2.3570\n",
      "Epoch [9/20], Step [5600/12299],final_Loss: 1.0356\n",
      "Epoch [9/20], Step [5700/12299],final_Loss: 9.3995\n",
      "Epoch [9/20], Step [5800/12299],final_Loss: 0.9181\n",
      "Epoch [9/20], Step [5900/12299],final_Loss: 1.4399\n",
      "Epoch [9/20], Step [6000/12299],final_Loss: 2.7519\n",
      "Epoch [9/20], Step [6100/12299],final_Loss: 1.7522\n",
      "Epoch [9/20], Step [6200/12299],final_Loss: 1.4749\n",
      "Epoch [9/20], Step [6300/12299],final_Loss: 0.3657\n",
      "Epoch [9/20], Step [6400/12299],final_Loss: 1.5482\n",
      "Epoch [9/20], Step [6500/12299],final_Loss: 0.8432\n",
      "Epoch [9/20], Step [6600/12299],final_Loss: 0.9322\n",
      "Epoch [9/20], Step [6700/12299],final_Loss: 1.7497\n",
      "Epoch [9/20], Step [6800/12299],final_Loss: 0.7839\n",
      "Epoch [9/20], Step [6900/12299],final_Loss: 0.6712\n",
      "Epoch [9/20], Step [7000/12299],final_Loss: 0.9654\n",
      "Epoch [9/20], Step [7100/12299],final_Loss: 0.7893\n",
      "Epoch [9/20], Step [7200/12299],final_Loss: 0.2510\n",
      "Epoch [9/20], Step [7300/12299],final_Loss: 0.7742\n",
      "Epoch [9/20], Step [7400/12299],final_Loss: 1.6328\n",
      "Epoch [9/20], Step [7500/12299],final_Loss: 3.8803\n",
      "Epoch [9/20], Step [7600/12299],final_Loss: 1.2559\n",
      "Epoch [9/20], Step [7700/12299],final_Loss: 3.2783\n",
      "Epoch [9/20], Step [7800/12299],final_Loss: 22.0018\n",
      "Epoch [9/20], Step [7900/12299],final_Loss: 0.4998\n",
      "Epoch [9/20], Step [8000/12299],final_Loss: 0.3880\n",
      "Epoch [9/20], Step [8100/12299],final_Loss: 0.4734\n",
      "Epoch [9/20], Step [8200/12299],final_Loss: 1.1220\n",
      "Epoch [9/20], Step [8300/12299],final_Loss: 0.5915\n",
      "Epoch [9/20], Step [8400/12299],final_Loss: 0.6859\n",
      "Epoch [9/20], Step [8500/12299],final_Loss: 0.1817\n",
      "Epoch [9/20], Step [8600/12299],final_Loss: 0.3198\n",
      "Epoch [9/20], Step [8700/12299],final_Loss: 0.1331\n",
      "Epoch [9/20], Step [8800/12299],final_Loss: 0.2743\n",
      "Epoch [9/20], Step [8900/12299],final_Loss: 0.6080\n",
      "Epoch [9/20], Step [9000/12299],final_Loss: 24.7693\n",
      "Epoch [9/20], Step [9100/12299],final_Loss: 3.4381\n",
      "Epoch [9/20], Step [9200/12299],final_Loss: 0.7519\n",
      "Epoch [9/20], Step [9300/12299],final_Loss: 1.6437\n",
      "Epoch [9/20], Step [9400/12299],final_Loss: 15.8961\n",
      "Epoch [9/20], Step [9500/12299],final_Loss: 0.3584\n",
      "Epoch [9/20], Step [9600/12299],final_Loss: 0.7816\n",
      "Epoch [9/20], Step [9700/12299],final_Loss: 0.1652\n",
      "Epoch [9/20], Step [9800/12299],final_Loss: 0.1150\n",
      "Epoch [9/20], Step [9900/12299],final_Loss: 0.0787\n",
      "Epoch [9/20], Step [10000/12299],final_Loss: 1.0355\n",
      "Epoch [9/20], Step [10100/12299],final_Loss: 17.3445\n",
      "Epoch [9/20], Step [10200/12299],final_Loss: 0.3377\n",
      "Epoch [9/20], Step [10300/12299],final_Loss: 4.2695\n",
      "Epoch [9/20], Step [10400/12299],final_Loss: 0.2671\n",
      "Epoch [9/20], Step [10500/12299],final_Loss: 16.8251\n",
      "Epoch [9/20], Step [10600/12299],final_Loss: 0.2630\n",
      "Epoch [9/20], Step [10700/12299],final_Loss: 1.3162\n",
      "Epoch [9/20], Step [10800/12299],final_Loss: 0.1395\n",
      "Epoch [9/20], Step [10900/12299],final_Loss: 0.8196\n",
      "Epoch [9/20], Step [11000/12299],final_Loss: 4.0333\n",
      "Epoch [9/20], Step [11100/12299],final_Loss: 0.7959\n",
      "Epoch [9/20], Step [11200/12299],final_Loss: 6.0103\n",
      "Epoch [9/20], Step [11300/12299],final_Loss: 0.8260\n",
      "Epoch [9/20], Step [11400/12299],final_Loss: 0.4267\n",
      "Epoch [9/20], Step [11500/12299],final_Loss: 0.5171\n",
      "Epoch [9/20], Step [11600/12299],final_Loss: 0.5839\n",
      "Epoch [9/20], Step [11700/12299],final_Loss: 0.2153\n",
      "Epoch [9/20], Step [11800/12299],final_Loss: 8.2683\n",
      "Epoch [9/20], Step [11900/12299],final_Loss: 0.2983\n",
      "Epoch [9/20], Step [12000/12299],final_Loss: 0.5294\n",
      "Epoch [9/20], Step [12100/12299],final_Loss: 0.2554\n",
      "Epoch [9/20], Step [12200/12299],final_Loss: 0.3277\n",
      "Epoch [10/20], Step [100/12299],final_Loss: 0.2817\n",
      "Epoch [10/20], Step [200/12299],final_Loss: 3.0196\n",
      "Epoch [10/20], Step [300/12299],final_Loss: 0.2307\n",
      "Epoch [10/20], Step [400/12299],final_Loss: 0.5310\n",
      "Epoch [10/20], Step [500/12299],final_Loss: 0.2176\n",
      "Epoch [10/20], Step [600/12299],final_Loss: 0.1469\n",
      "Epoch [10/20], Step [700/12299],final_Loss: 22.5106\n",
      "Epoch [10/20], Step [800/12299],final_Loss: 0.1419\n",
      "Epoch [10/20], Step [900/12299],final_Loss: 0.0956\n",
      "Epoch [10/20], Step [1000/12299],final_Loss: 0.3478\n",
      "Epoch [10/20], Step [1100/12299],final_Loss: 0.1647\n",
      "Epoch [10/20], Step [1200/12299],final_Loss: 0.3161\n",
      "Epoch [10/20], Step [1300/12299],final_Loss: 0.4987\n",
      "Epoch [10/20], Step [1400/12299],final_Loss: 0.1660\n",
      "Epoch [10/20], Step [1500/12299],final_Loss: 2.1076\n",
      "Epoch [10/20], Step [1600/12299],final_Loss: 47.1734\n",
      "Epoch [10/20], Step [1700/12299],final_Loss: 18.3906\n",
      "Epoch [10/20], Step [1800/12299],final_Loss: 0.1887\n",
      "Epoch [10/20], Step [1900/12299],final_Loss: 0.4753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/20], Step [2000/12299],final_Loss: 6.9684\n",
      "Epoch [10/20], Step [2100/12299],final_Loss: 0.8297\n",
      "Epoch [10/20], Step [2200/12299],final_Loss: 0.5086\n",
      "Epoch [10/20], Step [2300/12299],final_Loss: 0.5981\n",
      "Epoch [10/20], Step [2400/12299],final_Loss: 0.4041\n",
      "Epoch [10/20], Step [2500/12299],final_Loss: 0.5105\n",
      "Epoch [10/20], Step [2600/12299],final_Loss: 0.3017\n",
      "Epoch [10/20], Step [2700/12299],final_Loss: 0.5042\n",
      "Epoch [10/20], Step [2800/12299],final_Loss: 0.3786\n",
      "Epoch [10/20], Step [2900/12299],final_Loss: 0.2943\n",
      "Epoch [10/20], Step [3000/12299],final_Loss: 0.4405\n",
      "Epoch [10/20], Step [3100/12299],final_Loss: 1.1274\n",
      "Epoch [10/20], Step [3200/12299],final_Loss: 0.8724\n",
      "Epoch [10/20], Step [3300/12299],final_Loss: 7.6079\n",
      "Epoch [10/20], Step [3400/12299],final_Loss: 0.2905\n",
      "Epoch [10/20], Step [3500/12299],final_Loss: 0.2670\n",
      "Epoch [10/20], Step [3600/12299],final_Loss: 1.3394\n",
      "Epoch [10/20], Step [3700/12299],final_Loss: 0.5623\n",
      "Epoch [10/20], Step [3800/12299],final_Loss: 0.3303\n",
      "Epoch [10/20], Step [3900/12299],final_Loss: 0.3196\n",
      "Epoch [10/20], Step [4000/12299],final_Loss: 7.6981\n",
      "Epoch [10/20], Step [4100/12299],final_Loss: 0.3590\n",
      "Epoch [10/20], Step [4200/12299],final_Loss: 30.8605\n",
      "Epoch [10/20], Step [4300/12299],final_Loss: 1.2231\n",
      "Epoch [10/20], Step [4400/12299],final_Loss: 0.7116\n",
      "Epoch [10/20], Step [4500/12299],final_Loss: 0.3557\n",
      "Epoch [10/20], Step [4600/12299],final_Loss: 1.1104\n",
      "Epoch [10/20], Step [4700/12299],final_Loss: 2.4580\n",
      "Epoch [10/20], Step [4800/12299],final_Loss: 3.1587\n",
      "Epoch [10/20], Step [4900/12299],final_Loss: 5.0111\n",
      "Epoch [10/20], Step [5000/12299],final_Loss: 0.3102\n",
      "Epoch [10/20], Step [5100/12299],final_Loss: 6.1475\n",
      "Epoch [10/20], Step [5200/12299],final_Loss: 1.0950\n",
      "Epoch [10/20], Step [5300/12299],final_Loss: 0.6690\n",
      "Epoch [10/20], Step [5400/12299],final_Loss: 12.0921\n",
      "Epoch [10/20], Step [5500/12299],final_Loss: 2.9650\n",
      "Epoch [10/20], Step [5600/12299],final_Loss: 0.8794\n",
      "Epoch [10/20], Step [5700/12299],final_Loss: 12.9158\n",
      "Epoch [10/20], Step [5800/12299],final_Loss: 1.0057\n",
      "Epoch [10/20], Step [5900/12299],final_Loss: 1.1600\n",
      "Epoch [10/20], Step [6000/12299],final_Loss: 2.3023\n",
      "Epoch [10/20], Step [6100/12299],final_Loss: 1.7546\n",
      "Epoch [10/20], Step [6200/12299],final_Loss: 1.1813\n",
      "Epoch [10/20], Step [6300/12299],final_Loss: 0.2891\n",
      "Epoch [10/20], Step [6400/12299],final_Loss: 0.8089\n",
      "Epoch [10/20], Step [6500/12299],final_Loss: 0.6834\n",
      "Epoch [10/20], Step [6600/12299],final_Loss: 1.0277\n",
      "Epoch [10/20], Step [6700/12299],final_Loss: 1.1822\n",
      "Epoch [10/20], Step [6800/12299],final_Loss: 0.5202\n",
      "Epoch [10/20], Step [6900/12299],final_Loss: 0.5569\n",
      "Epoch [10/20], Step [7000/12299],final_Loss: 0.7461\n",
      "Epoch [10/20], Step [7100/12299],final_Loss: 0.7778\n",
      "Epoch [10/20], Step [7200/12299],final_Loss: 0.2376\n",
      "Epoch [10/20], Step [7300/12299],final_Loss: 0.8296\n",
      "Epoch [10/20], Step [7400/12299],final_Loss: 2.0587\n",
      "Epoch [10/20], Step [7500/12299],final_Loss: 2.7227\n",
      "Epoch [10/20], Step [7600/12299],final_Loss: 1.6757\n",
      "Epoch [10/20], Step [7700/12299],final_Loss: 3.2713\n",
      "Epoch [10/20], Step [7800/12299],final_Loss: 23.8548\n",
      "Epoch [10/20], Step [7900/12299],final_Loss: 0.4021\n",
      "Epoch [10/20], Step [8000/12299],final_Loss: 0.4072\n",
      "Epoch [10/20], Step [8100/12299],final_Loss: 0.4616\n",
      "Epoch [10/20], Step [8200/12299],final_Loss: 1.2813\n",
      "Epoch [10/20], Step [8300/12299],final_Loss: 0.7432\n",
      "Epoch [10/20], Step [8400/12299],final_Loss: 0.7527\n",
      "Epoch [10/20], Step [8500/12299],final_Loss: 0.1607\n",
      "Epoch [10/20], Step [8600/12299],final_Loss: 0.3671\n",
      "Epoch [10/20], Step [8700/12299],final_Loss: 0.1602\n",
      "Epoch [10/20], Step [8800/12299],final_Loss: 0.3063\n",
      "Epoch [10/20], Step [8900/12299],final_Loss: 0.5085\n",
      "Epoch [10/20], Step [9000/12299],final_Loss: 21.5774\n",
      "Epoch [10/20], Step [9100/12299],final_Loss: 2.6070\n",
      "Epoch [10/20], Step [9200/12299],final_Loss: 0.7322\n",
      "Epoch [10/20], Step [9300/12299],final_Loss: 1.4088\n",
      "Epoch [10/20], Step [9400/12299],final_Loss: 16.0669\n",
      "Epoch [10/20], Step [9500/12299],final_Loss: 0.3201\n",
      "Epoch [10/20], Step [9600/12299],final_Loss: 0.6174\n",
      "Epoch [10/20], Step [9700/12299],final_Loss: 0.1806\n",
      "Epoch [10/20], Step [9800/12299],final_Loss: 0.1153\n",
      "Epoch [10/20], Step [9900/12299],final_Loss: 0.0800\n",
      "Epoch [10/20], Step [10000/12299],final_Loss: 1.0447\n",
      "Epoch [10/20], Step [10100/12299],final_Loss: 19.2750\n",
      "Epoch [10/20], Step [10200/12299],final_Loss: 0.3239\n",
      "Epoch [10/20], Step [10300/12299],final_Loss: 4.7391\n",
      "Epoch [10/20], Step [10400/12299],final_Loss: 0.2960\n",
      "Epoch [10/20], Step [10500/12299],final_Loss: 16.5404\n",
      "Epoch [10/20], Step [10600/12299],final_Loss: 0.2324\n",
      "Epoch [10/20], Step [10700/12299],final_Loss: 1.5388\n",
      "Epoch [10/20], Step [10800/12299],final_Loss: 0.1959\n",
      "Epoch [10/20], Step [10900/12299],final_Loss: 1.0253\n",
      "Epoch [10/20], Step [11000/12299],final_Loss: 4.6603\n",
      "Epoch [10/20], Step [11100/12299],final_Loss: 0.8037\n",
      "Epoch [10/20], Step [11200/12299],final_Loss: 5.5963\n",
      "Epoch [10/20], Step [11300/12299],final_Loss: 0.7474\n",
      "Epoch [10/20], Step [11400/12299],final_Loss: 0.3372\n",
      "Epoch [10/20], Step [11500/12299],final_Loss: 0.5038\n",
      "Epoch [10/20], Step [11600/12299],final_Loss: 0.4036\n",
      "Epoch [10/20], Step [11700/12299],final_Loss: 0.2075\n",
      "Epoch [10/20], Step [11800/12299],final_Loss: 6.6367\n",
      "Epoch [10/20], Step [11900/12299],final_Loss: 0.2682\n",
      "Epoch [10/20], Step [12000/12299],final_Loss: 0.6778\n",
      "Epoch [10/20], Step [12100/12299],final_Loss: 0.2728\n",
      "Epoch [10/20], Step [12200/12299],final_Loss: 0.2522\n",
      "Epoch [11/20], Step [100/12299],final_Loss: 0.2569\n",
      "Epoch [11/20], Step [200/12299],final_Loss: 2.9913\n",
      "Epoch [11/20], Step [300/12299],final_Loss: 0.2115\n",
      "Epoch [11/20], Step [400/12299],final_Loss: 0.4412\n",
      "Epoch [11/20], Step [500/12299],final_Loss: 0.1965\n",
      "Epoch [11/20], Step [600/12299],final_Loss: 0.1589\n",
      "Epoch [11/20], Step [700/12299],final_Loss: 19.3810\n",
      "Epoch [11/20], Step [800/12299],final_Loss: 0.1494\n",
      "Epoch [11/20], Step [900/12299],final_Loss: 0.1072\n",
      "Epoch [11/20], Step [1000/12299],final_Loss: 0.4364\n",
      "Epoch [11/20], Step [1100/12299],final_Loss: 0.1833\n",
      "Epoch [11/20], Step [1200/12299],final_Loss: 0.3168\n",
      "Epoch [11/20], Step [1300/12299],final_Loss: 0.4918\n",
      "Epoch [11/20], Step [1400/12299],final_Loss: 0.1863\n",
      "Epoch [11/20], Step [1500/12299],final_Loss: 1.7332\n",
      "Epoch [11/20], Step [1600/12299],final_Loss: 42.4091\n",
      "Epoch [11/20], Step [1700/12299],final_Loss: 17.1032\n",
      "Epoch [11/20], Step [1800/12299],final_Loss: 0.2029\n",
      "Epoch [11/20], Step [1900/12299],final_Loss: 0.5515\n",
      "Epoch [11/20], Step [2000/12299],final_Loss: 7.7221\n",
      "Epoch [11/20], Step [2100/12299],final_Loss: 1.0952\n",
      "Epoch [11/20], Step [2200/12299],final_Loss: 0.6193\n",
      "Epoch [11/20], Step [2300/12299],final_Loss: 0.7454\n",
      "Epoch [11/20], Step [2400/12299],final_Loss: 0.5341\n",
      "Epoch [11/20], Step [2500/12299],final_Loss: 0.6457\n",
      "Epoch [11/20], Step [2600/12299],final_Loss: 0.3334\n",
      "Epoch [11/20], Step [2700/12299],final_Loss: 0.5782\n",
      "Epoch [11/20], Step [2800/12299],final_Loss: 0.4586\n",
      "Epoch [11/20], Step [2900/12299],final_Loss: 0.3997\n",
      "Epoch [11/20], Step [3000/12299],final_Loss: 0.5939\n",
      "Epoch [11/20], Step [3100/12299],final_Loss: 1.0881\n",
      "Epoch [11/20], Step [3200/12299],final_Loss: 0.8251\n",
      "Epoch [11/20], Step [3300/12299],final_Loss: 8.5055\n",
      "Epoch [11/20], Step [3400/12299],final_Loss: 0.2370\n",
      "Epoch [11/20], Step [3500/12299],final_Loss: 0.2783\n",
      "Epoch [11/20], Step [3600/12299],final_Loss: 1.6272\n",
      "Epoch [11/20], Step [3700/12299],final_Loss: 0.5375\n",
      "Epoch [11/20], Step [3800/12299],final_Loss: 0.3249\n",
      "Epoch [11/20], Step [3900/12299],final_Loss: 0.3812\n",
      "Epoch [11/20], Step [4000/12299],final_Loss: 5.0564\n",
      "Epoch [11/20], Step [4100/12299],final_Loss: 0.3856\n",
      "Epoch [11/20], Step [4200/12299],final_Loss: 32.6800\n",
      "Epoch [11/20], Step [4300/12299],final_Loss: 1.0534\n",
      "Epoch [11/20], Step [4400/12299],final_Loss: 0.6858\n",
      "Epoch [11/20], Step [4500/12299],final_Loss: 0.3831\n",
      "Epoch [11/20], Step [4600/12299],final_Loss: 0.7420\n",
      "Epoch [11/20], Step [4700/12299],final_Loss: 1.5325\n",
      "Epoch [11/20], Step [4800/12299],final_Loss: 4.4765\n",
      "Epoch [11/20], Step [4900/12299],final_Loss: 7.6284\n",
      "Epoch [11/20], Step [5000/12299],final_Loss: 0.3578\n",
      "Epoch [11/20], Step [5100/12299],final_Loss: 4.8036\n",
      "Epoch [11/20], Step [5200/12299],final_Loss: 0.8976\n",
      "Epoch [11/20], Step [5300/12299],final_Loss: 0.7588\n",
      "Epoch [11/20], Step [5400/12299],final_Loss: 14.3802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/20], Step [5500/12299],final_Loss: 1.6428\n",
      "Epoch [11/20], Step [5600/12299],final_Loss: 1.1020\n",
      "Epoch [11/20], Step [5700/12299],final_Loss: 12.6784\n",
      "Epoch [11/20], Step [5800/12299],final_Loss: 0.9075\n",
      "Epoch [11/20], Step [5900/12299],final_Loss: 1.4808\n",
      "Epoch [11/20], Step [6000/12299],final_Loss: 2.3319\n",
      "Epoch [11/20], Step [6100/12299],final_Loss: 1.9745\n",
      "Epoch [11/20], Step [6200/12299],final_Loss: 1.2605\n",
      "Epoch [11/20], Step [6300/12299],final_Loss: 0.3493\n",
      "Epoch [11/20], Step [6400/12299],final_Loss: 0.7421\n",
      "Epoch [11/20], Step [6500/12299],final_Loss: 0.7744\n",
      "Epoch [11/20], Step [6600/12299],final_Loss: 1.0581\n",
      "Epoch [11/20], Step [6700/12299],final_Loss: 1.3093\n",
      "Epoch [11/20], Step [6800/12299],final_Loss: 0.5367\n",
      "Epoch [11/20], Step [6900/12299],final_Loss: 0.5994\n",
      "Epoch [11/20], Step [7000/12299],final_Loss: 0.6356\n",
      "Epoch [11/20], Step [7100/12299],final_Loss: 0.5711\n",
      "Epoch [11/20], Step [7200/12299],final_Loss: 0.2001\n",
      "Epoch [11/20], Step [7300/12299],final_Loss: 0.6424\n",
      "Epoch [11/20], Step [7400/12299],final_Loss: 3.2739\n",
      "Epoch [11/20], Step [7500/12299],final_Loss: 3.8594\n",
      "Epoch [11/20], Step [7600/12299],final_Loss: 1.2706\n",
      "Epoch [11/20], Step [7700/12299],final_Loss: 2.4562\n",
      "Epoch [11/20], Step [7800/12299],final_Loss: 21.1183\n",
      "Epoch [11/20], Step [7900/12299],final_Loss: 0.4950\n",
      "Epoch [11/20], Step [8000/12299],final_Loss: 0.3868\n",
      "Epoch [11/20], Step [8100/12299],final_Loss: 0.4673\n",
      "Epoch [11/20], Step [8200/12299],final_Loss: 1.1574\n",
      "Epoch [11/20], Step [8300/12299],final_Loss: 0.5974\n",
      "Epoch [11/20], Step [8400/12299],final_Loss: 0.7200\n",
      "Epoch [11/20], Step [8500/12299],final_Loss: 0.1501\n",
      "Epoch [11/20], Step [8600/12299],final_Loss: 0.3590\n",
      "Epoch [11/20], Step [8700/12299],final_Loss: 0.1756\n",
      "Epoch [11/20], Step [8800/12299],final_Loss: 0.2946\n",
      "Epoch [11/20], Step [8900/12299],final_Loss: 0.7497\n",
      "Epoch [11/20], Step [9000/12299],final_Loss: 20.8796\n",
      "Epoch [11/20], Step [9100/12299],final_Loss: 2.7936\n",
      "Epoch [11/20], Step [9200/12299],final_Loss: 0.6516\n",
      "Epoch [11/20], Step [9300/12299],final_Loss: 2.5276\n",
      "Epoch [11/20], Step [9400/12299],final_Loss: 17.5900\n",
      "Epoch [11/20], Step [9500/12299],final_Loss: 0.2351\n",
      "Epoch [11/20], Step [9600/12299],final_Loss: 0.7443\n",
      "Epoch [11/20], Step [9700/12299],final_Loss: 0.1886\n",
      "Epoch [11/20], Step [9800/12299],final_Loss: 0.1419\n",
      "Epoch [11/20], Step [9900/12299],final_Loss: 0.0916\n",
      "Epoch [11/20], Step [10000/12299],final_Loss: 1.0018\n",
      "Epoch [11/20], Step [10100/12299],final_Loss: 18.3805\n",
      "Epoch [11/20], Step [10200/12299],final_Loss: 0.3604\n",
      "Epoch [11/20], Step [10300/12299],final_Loss: 6.5532\n",
      "Epoch [11/20], Step [10400/12299],final_Loss: 0.2763\n",
      "Epoch [11/20], Step [10500/12299],final_Loss: 17.1873\n",
      "Epoch [11/20], Step [10600/12299],final_Loss: 0.2333\n",
      "Epoch [11/20], Step [10700/12299],final_Loss: 1.5867\n",
      "Epoch [11/20], Step [10800/12299],final_Loss: 0.1721\n",
      "Epoch [11/20], Step [10900/12299],final_Loss: 1.2424\n",
      "Epoch [11/20], Step [11000/12299],final_Loss: 4.7904\n",
      "Epoch [11/20], Step [11100/12299],final_Loss: 0.6659\n",
      "Epoch [11/20], Step [11200/12299],final_Loss: 3.4617\n",
      "Epoch [11/20], Step [11300/12299],final_Loss: 0.8395\n",
      "Epoch [11/20], Step [11400/12299],final_Loss: 0.4211\n",
      "Epoch [11/20], Step [11500/12299],final_Loss: 0.4693\n",
      "Epoch [11/20], Step [11600/12299],final_Loss: 0.4339\n",
      "Epoch [11/20], Step [11700/12299],final_Loss: 0.2414\n",
      "Epoch [11/20], Step [11800/12299],final_Loss: 7.1981\n",
      "Epoch [11/20], Step [11900/12299],final_Loss: 0.2953\n",
      "Epoch [11/20], Step [12000/12299],final_Loss: 0.5191\n",
      "Epoch [11/20], Step [12100/12299],final_Loss: 0.2331\n",
      "Epoch [11/20], Step [12200/12299],final_Loss: 0.3231\n",
      "Epoch [12/20], Step [100/12299],final_Loss: 0.2692\n",
      "Epoch [12/20], Step [200/12299],final_Loss: 2.7515\n",
      "Epoch [12/20], Step [300/12299],final_Loss: 0.2098\n",
      "Epoch [12/20], Step [400/12299],final_Loss: 0.5082\n",
      "Epoch [12/20], Step [500/12299],final_Loss: 0.1999\n",
      "Epoch [12/20], Step [600/12299],final_Loss: 0.1598\n",
      "Epoch [12/20], Step [700/12299],final_Loss: 21.7203\n",
      "Epoch [12/20], Step [800/12299],final_Loss: 0.1507\n",
      "Epoch [12/20], Step [900/12299],final_Loss: 0.1020\n",
      "Epoch [12/20], Step [1000/12299],final_Loss: 0.4168\n",
      "Epoch [12/20], Step [1100/12299],final_Loss: 0.1690\n",
      "Epoch [12/20], Step [1200/12299],final_Loss: 0.3180\n",
      "Epoch [12/20], Step [1300/12299],final_Loss: 0.6185\n",
      "Epoch [12/20], Step [1400/12299],final_Loss: 0.2056\n",
      "Epoch [12/20], Step [1500/12299],final_Loss: 2.1031\n",
      "Epoch [12/20], Step [1600/12299],final_Loss: 45.7468\n",
      "Epoch [12/20], Step [1700/12299],final_Loss: 15.3588\n",
      "Epoch [12/20], Step [1800/12299],final_Loss: 0.2897\n",
      "Epoch [12/20], Step [1900/12299],final_Loss: 0.6663\n",
      "Epoch [12/20], Step [2000/12299],final_Loss: 6.5915\n",
      "Epoch [12/20], Step [2100/12299],final_Loss: 0.8215\n",
      "Epoch [12/20], Step [2200/12299],final_Loss: 0.4053\n",
      "Epoch [12/20], Step [2300/12299],final_Loss: 0.7129\n",
      "Epoch [12/20], Step [2400/12299],final_Loss: 0.3482\n",
      "Epoch [12/20], Step [2500/12299],final_Loss: 0.4832\n",
      "Epoch [12/20], Step [2600/12299],final_Loss: 0.2741\n",
      "Epoch [12/20], Step [2700/12299],final_Loss: 0.6269\n",
      "Epoch [12/20], Step [2800/12299],final_Loss: 0.5282\n",
      "Epoch [12/20], Step [2900/12299],final_Loss: 0.3908\n",
      "Epoch [12/20], Step [3000/12299],final_Loss: 0.7205\n",
      "Epoch [12/20], Step [3100/12299],final_Loss: 1.1383\n",
      "Epoch [12/20], Step [3200/12299],final_Loss: 0.8984\n",
      "Epoch [12/20], Step [3300/12299],final_Loss: 8.7142\n",
      "Epoch [12/20], Step [3400/12299],final_Loss: 0.2179\n",
      "Epoch [12/20], Step [3500/12299],final_Loss: 0.3502\n",
      "Epoch [12/20], Step [3600/12299],final_Loss: 0.7366\n",
      "Epoch [12/20], Step [3700/12299],final_Loss: 0.6572\n",
      "Epoch [12/20], Step [3800/12299],final_Loss: 0.2578\n",
      "Epoch [12/20], Step [3900/12299],final_Loss: 0.2908\n",
      "Epoch [12/20], Step [4000/12299],final_Loss: 5.4344\n",
      "Epoch [12/20], Step [4100/12299],final_Loss: 0.4131\n",
      "Epoch [12/20], Step [4200/12299],final_Loss: 32.2323\n",
      "Epoch [12/20], Step [4300/12299],final_Loss: 1.2565\n",
      "Epoch [12/20], Step [4400/12299],final_Loss: 0.6502\n",
      "Epoch [12/20], Step [4500/12299],final_Loss: 0.4221\n",
      "Epoch [12/20], Step [4600/12299],final_Loss: 0.9236\n",
      "Epoch [12/20], Step [4700/12299],final_Loss: 1.6945\n",
      "Epoch [12/20], Step [4800/12299],final_Loss: 4.0640\n",
      "Epoch [12/20], Step [4900/12299],final_Loss: 4.4219\n",
      "Epoch [12/20], Step [5000/12299],final_Loss: 0.2605\n",
      "Epoch [12/20], Step [5100/12299],final_Loss: 6.1067\n",
      "Epoch [12/20], Step [5200/12299],final_Loss: 0.8270\n",
      "Epoch [12/20], Step [5300/12299],final_Loss: 0.6740\n",
      "Epoch [12/20], Step [5400/12299],final_Loss: 14.8280\n",
      "Epoch [12/20], Step [5500/12299],final_Loss: 1.8899\n",
      "Epoch [12/20], Step [5600/12299],final_Loss: 0.8992\n",
      "Epoch [12/20], Step [5700/12299],final_Loss: 10.2128\n",
      "Epoch [12/20], Step [5800/12299],final_Loss: 1.0633\n",
      "Epoch [12/20], Step [5900/12299],final_Loss: 1.1109\n",
      "Epoch [12/20], Step [6000/12299],final_Loss: 3.7850\n",
      "Epoch [12/20], Step [6100/12299],final_Loss: 1.9194\n",
      "Epoch [12/20], Step [6200/12299],final_Loss: 1.6472\n",
      "Epoch [12/20], Step [6300/12299],final_Loss: 0.4015\n",
      "Epoch [12/20], Step [6400/12299],final_Loss: 0.7828\n",
      "Epoch [12/20], Step [6500/12299],final_Loss: 0.6048\n",
      "Epoch [12/20], Step [6600/12299],final_Loss: 0.9903\n",
      "Epoch [12/20], Step [6700/12299],final_Loss: 1.5749\n",
      "Epoch [12/20], Step [6800/12299],final_Loss: 0.6843\n",
      "Epoch [12/20], Step [6900/12299],final_Loss: 0.6595\n",
      "Epoch [12/20], Step [7000/12299],final_Loss: 0.6269\n",
      "Epoch [12/20], Step [7100/12299],final_Loss: 0.8169\n",
      "Epoch [12/20], Step [7200/12299],final_Loss: 0.2581\n",
      "Epoch [12/20], Step [7300/12299],final_Loss: 0.8340\n",
      "Epoch [12/20], Step [7400/12299],final_Loss: 2.3018\n",
      "Epoch [12/20], Step [7500/12299],final_Loss: 4.2103\n",
      "Epoch [12/20], Step [7600/12299],final_Loss: 1.3812\n",
      "Epoch [12/20], Step [7700/12299],final_Loss: 4.6213\n",
      "Epoch [12/20], Step [7800/12299],final_Loss: 28.2153\n",
      "Epoch [12/20], Step [7900/12299],final_Loss: 0.5316\n",
      "Epoch [12/20], Step [8000/12299],final_Loss: 0.4599\n",
      "Epoch [12/20], Step [8100/12299],final_Loss: 0.3684\n",
      "Epoch [12/20], Step [8200/12299],final_Loss: 1.2660\n",
      "Epoch [12/20], Step [8300/12299],final_Loss: 0.8801\n",
      "Epoch [12/20], Step [8400/12299],final_Loss: 0.6334\n",
      "Epoch [12/20], Step [8500/12299],final_Loss: 0.1597\n",
      "Epoch [12/20], Step [8600/12299],final_Loss: 0.4109\n",
      "Epoch [12/20], Step [8700/12299],final_Loss: 0.1589\n",
      "Epoch [12/20], Step [8800/12299],final_Loss: 0.2519\n",
      "Epoch [12/20], Step [8900/12299],final_Loss: 0.6062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/20], Step [9000/12299],final_Loss: 30.6567\n",
      "Epoch [12/20], Step [9100/12299],final_Loss: 4.1420\n",
      "Epoch [12/20], Step [9200/12299],final_Loss: 0.8972\n",
      "Epoch [12/20], Step [9300/12299],final_Loss: 2.4284\n",
      "Epoch [12/20], Step [9400/12299],final_Loss: 14.7757\n",
      "Epoch [12/20], Step [9500/12299],final_Loss: 0.3622\n",
      "Epoch [12/20], Step [9600/12299],final_Loss: 0.9350\n",
      "Epoch [12/20], Step [9700/12299],final_Loss: 0.2270\n",
      "Epoch [12/20], Step [9800/12299],final_Loss: 0.1905\n",
      "Epoch [12/20], Step [9900/12299],final_Loss: 0.1035\n",
      "Epoch [12/20], Step [10000/12299],final_Loss: 1.1075\n",
      "Epoch [12/20], Step [10100/12299],final_Loss: 16.5820\n",
      "Epoch [12/20], Step [10200/12299],final_Loss: 0.3906\n",
      "Epoch [12/20], Step [10300/12299],final_Loss: 4.5192\n",
      "Epoch [12/20], Step [10400/12299],final_Loss: 0.2297\n",
      "Epoch [12/20], Step [10500/12299],final_Loss: 16.0148\n",
      "Epoch [12/20], Step [10600/12299],final_Loss: 0.3119\n",
      "Epoch [12/20], Step [10700/12299],final_Loss: 2.6839\n",
      "Epoch [12/20], Step [10800/12299],final_Loss: 0.2073\n",
      "Epoch [12/20], Step [10900/12299],final_Loss: 1.6247\n",
      "Epoch [12/20], Step [11000/12299],final_Loss: 6.6284\n",
      "Epoch [12/20], Step [11100/12299],final_Loss: 0.8249\n",
      "Epoch [12/20], Step [11200/12299],final_Loss: 6.3070\n",
      "Epoch [12/20], Step [11300/12299],final_Loss: 1.1130\n",
      "Epoch [12/20], Step [11400/12299],final_Loss: 0.4563\n",
      "Epoch [12/20], Step [11500/12299],final_Loss: 0.5065\n",
      "Epoch [12/20], Step [11600/12299],final_Loss: 0.4759\n",
      "Epoch [12/20], Step [11700/12299],final_Loss: 0.2550\n",
      "Epoch [12/20], Step [11800/12299],final_Loss: 6.7469\n",
      "Epoch [12/20], Step [11900/12299],final_Loss: 0.2584\n",
      "Epoch [12/20], Step [12000/12299],final_Loss: 0.5744\n",
      "Epoch [12/20], Step [12100/12299],final_Loss: 0.2782\n",
      "Epoch [12/20], Step [12200/12299],final_Loss: 0.4070\n",
      "Epoch [13/20], Step [100/12299],final_Loss: 0.2864\n",
      "Epoch [13/20], Step [200/12299],final_Loss: 3.6394\n",
      "Epoch [13/20], Step [300/12299],final_Loss: 0.2049\n",
      "Epoch [13/20], Step [400/12299],final_Loss: 0.5054\n",
      "Epoch [13/20], Step [500/12299],final_Loss: 0.1902\n",
      "Epoch [13/20], Step [600/12299],final_Loss: 0.1834\n",
      "Epoch [13/20], Step [700/12299],final_Loss: 17.3297\n",
      "Epoch [13/20], Step [800/12299],final_Loss: 0.1757\n",
      "Epoch [13/20], Step [900/12299],final_Loss: 0.1109\n",
      "Epoch [13/20], Step [1000/12299],final_Loss: 0.3913\n",
      "Epoch [13/20], Step [1100/12299],final_Loss: 0.1689\n",
      "Epoch [13/20], Step [1200/12299],final_Loss: 0.3237\n",
      "Epoch [13/20], Step [1300/12299],final_Loss: 0.5390\n",
      "Epoch [13/20], Step [1400/12299],final_Loss: 0.1818\n",
      "Epoch [13/20], Step [1500/12299],final_Loss: 1.4227\n",
      "Epoch [13/20], Step [1600/12299],final_Loss: 42.4344\n",
      "Epoch [13/20], Step [1700/12299],final_Loss: 13.7747\n",
      "Epoch [13/20], Step [1800/12299],final_Loss: 0.1964\n",
      "Epoch [13/20], Step [1900/12299],final_Loss: 0.6112\n",
      "Epoch [13/20], Step [2000/12299],final_Loss: 6.2066\n",
      "Epoch [13/20], Step [2100/12299],final_Loss: 1.5054\n",
      "Epoch [13/20], Step [2200/12299],final_Loss: 0.5124\n",
      "Epoch [13/20], Step [2300/12299],final_Loss: 0.7739\n",
      "Epoch [13/20], Step [2400/12299],final_Loss: 0.4235\n",
      "Epoch [13/20], Step [2500/12299],final_Loss: 0.5379\n",
      "Epoch [13/20], Step [2600/12299],final_Loss: 0.3028\n",
      "Epoch [13/20], Step [2700/12299],final_Loss: 0.7487\n",
      "Epoch [13/20], Step [2800/12299],final_Loss: 0.3429\n",
      "Epoch [13/20], Step [2900/12299],final_Loss: 0.2987\n",
      "Epoch [13/20], Step [3000/12299],final_Loss: 0.6331\n",
      "Epoch [13/20], Step [3100/12299],final_Loss: 0.7785\n",
      "Epoch [13/20], Step [3200/12299],final_Loss: 0.6884\n",
      "Epoch [13/20], Step [3300/12299],final_Loss: 8.5848\n",
      "Epoch [13/20], Step [3400/12299],final_Loss: 0.2553\n",
      "Epoch [13/20], Step [3500/12299],final_Loss: 0.2740\n",
      "Epoch [13/20], Step [3600/12299],final_Loss: 1.2266\n",
      "Epoch [13/20], Step [3700/12299],final_Loss: 0.6763\n",
      "Epoch [13/20], Step [3800/12299],final_Loss: 0.3065\n",
      "Epoch [13/20], Step [3900/12299],final_Loss: 0.3627\n",
      "Epoch [13/20], Step [4000/12299],final_Loss: 3.8131\n",
      "Epoch [13/20], Step [4100/12299],final_Loss: 0.3772\n",
      "Epoch [13/20], Step [4200/12299],final_Loss: 31.8109\n",
      "Epoch [13/20], Step [4300/12299],final_Loss: 1.1299\n",
      "Epoch [13/20], Step [4400/12299],final_Loss: 0.6556\n",
      "Epoch [13/20], Step [4500/12299],final_Loss: 0.3990\n",
      "Epoch [13/20], Step [4600/12299],final_Loss: 0.6190\n",
      "Epoch [13/20], Step [4700/12299],final_Loss: 1.6627\n",
      "Epoch [13/20], Step [4800/12299],final_Loss: 4.9482\n",
      "Epoch [13/20], Step [4900/12299],final_Loss: 7.9439\n",
      "Epoch [13/20], Step [5000/12299],final_Loss: 0.4039\n",
      "Epoch [13/20], Step [5100/12299],final_Loss: 6.1906\n",
      "Epoch [13/20], Step [5200/12299],final_Loss: 0.9455\n",
      "Epoch [13/20], Step [5300/12299],final_Loss: 0.7368\n",
      "Epoch [13/20], Step [5400/12299],final_Loss: 14.6026\n",
      "Epoch [13/20], Step [5500/12299],final_Loss: 2.0619\n",
      "Epoch [13/20], Step [5600/12299],final_Loss: 0.8664\n",
      "Epoch [13/20], Step [5700/12299],final_Loss: 10.3402\n",
      "Epoch [13/20], Step [5800/12299],final_Loss: 1.5697\n",
      "Epoch [13/20], Step [5900/12299],final_Loss: 1.4560\n",
      "Epoch [13/20], Step [6000/12299],final_Loss: 3.1197\n",
      "Epoch [13/20], Step [6100/12299],final_Loss: 2.1797\n",
      "Epoch [13/20], Step [6200/12299],final_Loss: 1.6206\n",
      "Epoch [13/20], Step [6300/12299],final_Loss: 0.3553\n",
      "Epoch [13/20], Step [6400/12299],final_Loss: 0.8028\n",
      "Epoch [13/20], Step [6500/12299],final_Loss: 0.6430\n",
      "Epoch [13/20], Step [6600/12299],final_Loss: 0.9942\n",
      "Epoch [13/20], Step [6700/12299],final_Loss: 1.1102\n",
      "Epoch [13/20], Step [6800/12299],final_Loss: 0.5969\n",
      "Epoch [13/20], Step [6900/12299],final_Loss: 0.6776\n",
      "Epoch [13/20], Step [7000/12299],final_Loss: 0.6145\n",
      "Epoch [13/20], Step [7100/12299],final_Loss: 0.7691\n",
      "Epoch [13/20], Step [7200/12299],final_Loss: 0.2595\n",
      "Epoch [13/20], Step [7300/12299],final_Loss: 0.9908\n",
      "Epoch [13/20], Step [7400/12299],final_Loss: 2.3625\n",
      "Epoch [13/20], Step [7500/12299],final_Loss: 2.9445\n",
      "Epoch [13/20], Step [7600/12299],final_Loss: 1.2855\n",
      "Epoch [13/20], Step [7700/12299],final_Loss: 4.8528\n",
      "Epoch [13/20], Step [7800/12299],final_Loss: 18.1138\n",
      "Epoch [13/20], Step [7900/12299],final_Loss: 0.7376\n",
      "Epoch [13/20], Step [8000/12299],final_Loss: 0.5503\n",
      "Epoch [13/20], Step [8100/12299],final_Loss: 0.4292\n",
      "Epoch [13/20], Step [8200/12299],final_Loss: 1.3582\n",
      "Epoch [13/20], Step [8300/12299],final_Loss: 0.7177\n",
      "Epoch [13/20], Step [8400/12299],final_Loss: 0.5092\n",
      "Epoch [13/20], Step [8500/12299],final_Loss: 0.1736\n",
      "Epoch [13/20], Step [8600/12299],final_Loss: 0.4305\n",
      "Epoch [13/20], Step [8700/12299],final_Loss: 0.1638\n",
      "Epoch [13/20], Step [8800/12299],final_Loss: 0.2764\n",
      "Epoch [13/20], Step [8900/12299],final_Loss: 0.6741\n",
      "Epoch [13/20], Step [9000/12299],final_Loss: 21.9369\n",
      "Epoch [13/20], Step [9100/12299],final_Loss: 2.8804\n",
      "Epoch [13/20], Step [9200/12299],final_Loss: 0.8558\n",
      "Epoch [13/20], Step [9300/12299],final_Loss: 2.6445\n",
      "Epoch [13/20], Step [9400/12299],final_Loss: 17.6838\n",
      "Epoch [13/20], Step [9500/12299],final_Loss: 0.3226\n",
      "Epoch [13/20], Step [9600/12299],final_Loss: 0.9613\n",
      "Epoch [13/20], Step [9700/12299],final_Loss: 0.2521\n",
      "Epoch [13/20], Step [9800/12299],final_Loss: 0.1858\n",
      "Epoch [13/20], Step [9900/12299],final_Loss: 0.1275\n",
      "Epoch [13/20], Step [10000/12299],final_Loss: 1.3727\n",
      "Epoch [13/20], Step [10100/12299],final_Loss: 18.5561\n",
      "Epoch [13/20], Step [10200/12299],final_Loss: 0.4775\n",
      "Epoch [13/20], Step [10300/12299],final_Loss: 6.3070\n",
      "Epoch [13/20], Step [10400/12299],final_Loss: 0.2558\n",
      "Epoch [13/20], Step [10500/12299],final_Loss: 14.6695\n",
      "Epoch [13/20], Step [10600/12299],final_Loss: 0.3546\n",
      "Epoch [13/20], Step [10700/12299],final_Loss: 1.8317\n",
      "Epoch [13/20], Step [10800/12299],final_Loss: 0.2264\n",
      "Epoch [13/20], Step [10900/12299],final_Loss: 1.5866\n",
      "Epoch [13/20], Step [11000/12299],final_Loss: 6.2387\n",
      "Epoch [13/20], Step [11100/12299],final_Loss: 0.7592\n",
      "Epoch [13/20], Step [11200/12299],final_Loss: 4.6147\n",
      "Epoch [13/20], Step [11300/12299],final_Loss: 0.8852\n",
      "Epoch [13/20], Step [11400/12299],final_Loss: 0.4594\n",
      "Epoch [13/20], Step [11500/12299],final_Loss: 0.5838\n",
      "Epoch [13/20], Step [11600/12299],final_Loss: 0.6357\n",
      "Epoch [13/20], Step [11700/12299],final_Loss: 0.2799\n",
      "Epoch [13/20], Step [11800/12299],final_Loss: 8.2216\n",
      "Epoch [13/20], Step [11900/12299],final_Loss: 0.2727\n",
      "Epoch [13/20], Step [12000/12299],final_Loss: 0.6888\n",
      "Epoch [13/20], Step [12100/12299],final_Loss: 0.2888\n",
      "Epoch [13/20], Step [12200/12299],final_Loss: 0.6228\n",
      "Epoch [14/20], Step [100/12299],final_Loss: 0.2900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/20], Step [200/12299],final_Loss: 2.5539\n",
      "Epoch [14/20], Step [300/12299],final_Loss: 0.1958\n",
      "Epoch [14/20], Step [400/12299],final_Loss: 0.5494\n",
      "Epoch [14/20], Step [500/12299],final_Loss: 0.2371\n",
      "Epoch [14/20], Step [600/12299],final_Loss: 0.1697\n",
      "Epoch [14/20], Step [700/12299],final_Loss: 23.1569\n",
      "Epoch [14/20], Step [800/12299],final_Loss: 0.1758\n",
      "Epoch [14/20], Step [900/12299],final_Loss: 0.1180\n",
      "Epoch [14/20], Step [1000/12299],final_Loss: 0.4277\n",
      "Epoch [14/20], Step [1100/12299],final_Loss: 0.1900\n",
      "Epoch [14/20], Step [1200/12299],final_Loss: 0.3184\n",
      "Epoch [14/20], Step [1300/12299],final_Loss: 0.5996\n",
      "Epoch [14/20], Step [1400/12299],final_Loss: 0.1773\n",
      "Epoch [14/20], Step [1500/12299],final_Loss: 1.6955\n",
      "Epoch [14/20], Step [1600/12299],final_Loss: 34.6408\n",
      "Epoch [14/20], Step [1700/12299],final_Loss: 16.3299\n",
      "Epoch [14/20], Step [1800/12299],final_Loss: 0.2313\n",
      "Epoch [14/20], Step [1900/12299],final_Loss: 0.6576\n",
      "Epoch [14/20], Step [2000/12299],final_Loss: 10.8911\n",
      "Epoch [14/20], Step [2100/12299],final_Loss: 1.0224\n",
      "Epoch [14/20], Step [2200/12299],final_Loss: 0.5165\n",
      "Epoch [14/20], Step [2300/12299],final_Loss: 0.8019\n",
      "Epoch [14/20], Step [2400/12299],final_Loss: 0.4968\n",
      "Epoch [14/20], Step [2500/12299],final_Loss: 0.6212\n",
      "Epoch [14/20], Step [2600/12299],final_Loss: 0.2957\n",
      "Epoch [14/20], Step [2700/12299],final_Loss: 0.5865\n",
      "Epoch [14/20], Step [2800/12299],final_Loss: 0.5540\n",
      "Epoch [14/20], Step [2900/12299],final_Loss: 0.3203\n",
      "Epoch [14/20], Step [3000/12299],final_Loss: 0.4395\n",
      "Epoch [14/20], Step [3100/12299],final_Loss: 1.0159\n",
      "Epoch [14/20], Step [3200/12299],final_Loss: 0.7943\n",
      "Epoch [14/20], Step [3300/12299],final_Loss: 8.0286\n",
      "Epoch [14/20], Step [3400/12299],final_Loss: 0.2408\n",
      "Epoch [14/20], Step [3500/12299],final_Loss: 0.3168\n",
      "Epoch [14/20], Step [3600/12299],final_Loss: 1.2139\n",
      "Epoch [14/20], Step [3700/12299],final_Loss: 0.5752\n",
      "Epoch [14/20], Step [3800/12299],final_Loss: 0.2759\n",
      "Epoch [14/20], Step [3900/12299],final_Loss: 0.3996\n",
      "Epoch [14/20], Step [4000/12299],final_Loss: 3.4499\n",
      "Epoch [14/20], Step [4100/12299],final_Loss: 0.3194\n",
      "Epoch [14/20], Step [4200/12299],final_Loss: 39.6720\n",
      "Epoch [14/20], Step [4300/12299],final_Loss: 0.7135\n",
      "Epoch [14/20], Step [4400/12299],final_Loss: 0.6189\n",
      "Epoch [14/20], Step [4500/12299],final_Loss: 0.4968\n",
      "Epoch [14/20], Step [4600/12299],final_Loss: 0.6894\n",
      "Epoch [14/20], Step [4700/12299],final_Loss: 1.9631\n",
      "Epoch [14/20], Step [4800/12299],final_Loss: 4.0633\n",
      "Epoch [14/20], Step [4900/12299],final_Loss: 4.3925\n",
      "Epoch [14/20], Step [5000/12299],final_Loss: 0.3264\n",
      "Epoch [14/20], Step [5100/12299],final_Loss: 5.7851\n",
      "Epoch [14/20], Step [5200/12299],final_Loss: 0.8432\n",
      "Epoch [14/20], Step [5300/12299],final_Loss: 0.7027\n",
      "Epoch [14/20], Step [5400/12299],final_Loss: 8.9873\n",
      "Epoch [14/20], Step [5500/12299],final_Loss: 2.0570\n",
      "Epoch [14/20], Step [5600/12299],final_Loss: 0.9145\n",
      "Epoch [14/20], Step [5700/12299],final_Loss: 9.0115\n",
      "Epoch [14/20], Step [5800/12299],final_Loss: 1.0821\n",
      "Epoch [14/20], Step [5900/12299],final_Loss: 1.4214\n",
      "Epoch [14/20], Step [6000/12299],final_Loss: 3.4082\n",
      "Epoch [14/20], Step [6100/12299],final_Loss: 2.1668\n",
      "Epoch [14/20], Step [6200/12299],final_Loss: 1.3654\n",
      "Epoch [14/20], Step [6300/12299],final_Loss: 0.3558\n",
      "Epoch [14/20], Step [6400/12299],final_Loss: 0.6056\n",
      "Epoch [14/20], Step [6500/12299],final_Loss: 0.6522\n",
      "Epoch [14/20], Step [6600/12299],final_Loss: 1.1569\n",
      "Epoch [14/20], Step [6700/12299],final_Loss: 1.1696\n",
      "Epoch [14/20], Step [6800/12299],final_Loss: 0.5554\n",
      "Epoch [14/20], Step [6900/12299],final_Loss: 0.5561\n",
      "Epoch [14/20], Step [7000/12299],final_Loss: 0.5667\n",
      "Epoch [14/20], Step [7100/12299],final_Loss: 0.7467\n",
      "Epoch [14/20], Step [7200/12299],final_Loss: 0.2445\n",
      "Epoch [14/20], Step [7300/12299],final_Loss: 0.9419\n",
      "Epoch [14/20], Step [7400/12299],final_Loss: 2.8250\n",
      "Epoch [14/20], Step [7500/12299],final_Loss: 3.3093\n",
      "Epoch [14/20], Step [7600/12299],final_Loss: 1.6033\n",
      "Epoch [14/20], Step [7700/12299],final_Loss: 3.7678\n",
      "Epoch [14/20], Step [7800/12299],final_Loss: 21.8555\n",
      "Epoch [14/20], Step [7900/12299],final_Loss: 0.6639\n",
      "Epoch [14/20], Step [8000/12299],final_Loss: 0.3929\n",
      "Epoch [14/20], Step [8100/12299],final_Loss: 0.4816\n",
      "Epoch [14/20], Step [8200/12299],final_Loss: 1.2213\n",
      "Epoch [14/20], Step [8300/12299],final_Loss: 0.7471\n",
      "Epoch [14/20], Step [8400/12299],final_Loss: 0.7227\n",
      "Epoch [14/20], Step [8500/12299],final_Loss: 0.1719\n",
      "Epoch [14/20], Step [8600/12299],final_Loss: 0.4612\n",
      "Epoch [14/20], Step [8700/12299],final_Loss: 0.1810\n",
      "Epoch [14/20], Step [8800/12299],final_Loss: 0.2538\n",
      "Epoch [14/20], Step [8900/12299],final_Loss: 0.6230\n",
      "Epoch [14/20], Step [9000/12299],final_Loss: 30.4250\n",
      "Epoch [14/20], Step [9100/12299],final_Loss: 2.5937\n",
      "Epoch [14/20], Step [9200/12299],final_Loss: 0.6615\n",
      "Epoch [14/20], Step [9300/12299],final_Loss: 1.9067\n",
      "Epoch [14/20], Step [9400/12299],final_Loss: 17.9551\n",
      "Epoch [14/20], Step [9500/12299],final_Loss: 0.3061\n",
      "Epoch [14/20], Step [9600/12299],final_Loss: 0.9513\n",
      "Epoch [14/20], Step [9700/12299],final_Loss: 0.2476\n",
      "Epoch [14/20], Step [9800/12299],final_Loss: 0.1186\n",
      "Epoch [14/20], Step [9900/12299],final_Loss: 0.1007\n",
      "Epoch [14/20], Step [10000/12299],final_Loss: 0.7960\n",
      "Epoch [14/20], Step [10100/12299],final_Loss: 18.7215\n",
      "Epoch [14/20], Step [10200/12299],final_Loss: 0.3547\n",
      "Epoch [14/20], Step [10300/12299],final_Loss: 4.9598\n",
      "Epoch [14/20], Step [10400/12299],final_Loss: 0.2505\n",
      "Epoch [14/20], Step [10500/12299],final_Loss: 16.2197\n",
      "Epoch [14/20], Step [10600/12299],final_Loss: 0.3324\n",
      "Epoch [14/20], Step [10700/12299],final_Loss: 1.5206\n",
      "Epoch [14/20], Step [10800/12299],final_Loss: 0.2227\n",
      "Epoch [14/20], Step [10900/12299],final_Loss: 2.0500\n",
      "Epoch [14/20], Step [11000/12299],final_Loss: 4.5984\n",
      "Epoch [14/20], Step [11100/12299],final_Loss: 0.7245\n",
      "Epoch [14/20], Step [11200/12299],final_Loss: 2.9163\n",
      "Epoch [14/20], Step [11300/12299],final_Loss: 0.7880\n",
      "Epoch [14/20], Step [11400/12299],final_Loss: 0.3475\n",
      "Epoch [14/20], Step [11500/12299],final_Loss: 0.5907\n",
      "Epoch [14/20], Step [11600/12299],final_Loss: 0.5455\n",
      "Epoch [14/20], Step [11700/12299],final_Loss: 0.2167\n",
      "Epoch [14/20], Step [11800/12299],final_Loss: 8.4262\n",
      "Epoch [14/20], Step [11900/12299],final_Loss: 0.2450\n",
      "Epoch [14/20], Step [12000/12299],final_Loss: 0.5507\n",
      "Epoch [14/20], Step [12100/12299],final_Loss: 0.2709\n",
      "Epoch [14/20], Step [12200/12299],final_Loss: 0.2957\n",
      "Epoch [15/20], Step [100/12299],final_Loss: 0.2735\n",
      "Epoch [15/20], Step [200/12299],final_Loss: 2.3721\n",
      "Epoch [15/20], Step [300/12299],final_Loss: 0.2032\n",
      "Epoch [15/20], Step [400/12299],final_Loss: 0.5459\n",
      "Epoch [15/20], Step [500/12299],final_Loss: 0.2310\n",
      "Epoch [15/20], Step [600/12299],final_Loss: 0.1760\n",
      "Epoch [15/20], Step [700/12299],final_Loss: 21.9259\n",
      "Epoch [15/20], Step [800/12299],final_Loss: 0.1636\n",
      "Epoch [15/20], Step [900/12299],final_Loss: 0.1128\n",
      "Epoch [15/20], Step [1000/12299],final_Loss: 0.4693\n",
      "Epoch [15/20], Step [1100/12299],final_Loss: 0.1994\n",
      "Epoch [15/20], Step [1200/12299],final_Loss: 0.3089\n",
      "Epoch [15/20], Step [1300/12299],final_Loss: 0.6389\n",
      "Epoch [15/20], Step [1400/12299],final_Loss: 0.1515\n",
      "Epoch [15/20], Step [1500/12299],final_Loss: 1.8569\n",
      "Epoch [15/20], Step [1600/12299],final_Loss: 34.7752\n",
      "Epoch [15/20], Step [1700/12299],final_Loss: 17.6949\n",
      "Epoch [15/20], Step [1800/12299],final_Loss: 0.2872\n",
      "Epoch [15/20], Step [1900/12299],final_Loss: 0.6189\n",
      "Epoch [15/20], Step [2000/12299],final_Loss: 10.8092\n",
      "Epoch [15/20], Step [2100/12299],final_Loss: 1.6347\n",
      "Epoch [15/20], Step [2200/12299],final_Loss: 0.4456\n",
      "Epoch [15/20], Step [2300/12299],final_Loss: 0.8105\n",
      "Epoch [15/20], Step [2400/12299],final_Loss: 0.4271\n",
      "Epoch [15/20], Step [2500/12299],final_Loss: 0.5169\n",
      "Epoch [15/20], Step [2600/12299],final_Loss: 0.3776\n",
      "Epoch [15/20], Step [2700/12299],final_Loss: 0.5999\n",
      "Epoch [15/20], Step [2800/12299],final_Loss: 0.6021\n",
      "Epoch [15/20], Step [2900/12299],final_Loss: 0.3478\n",
      "Epoch [15/20], Step [3000/12299],final_Loss: 0.6649\n",
      "Epoch [15/20], Step [3100/12299],final_Loss: 1.7071\n",
      "Epoch [15/20], Step [3200/12299],final_Loss: 1.0075\n",
      "Epoch [15/20], Step [3300/12299],final_Loss: 7.3362\n",
      "Epoch [15/20], Step [3400/12299],final_Loss: 0.3200\n",
      "Epoch [15/20], Step [3500/12299],final_Loss: 0.3075\n",
      "Epoch [15/20], Step [3600/12299],final_Loss: 1.1586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/20], Step [3700/12299],final_Loss: 0.5450\n",
      "Epoch [15/20], Step [3800/12299],final_Loss: 0.2686\n",
      "Epoch [15/20], Step [3900/12299],final_Loss: 0.3613\n",
      "Epoch [15/20], Step [4000/12299],final_Loss: 4.3313\n",
      "Epoch [15/20], Step [4100/12299],final_Loss: 0.3820\n",
      "Epoch [15/20], Step [4200/12299],final_Loss: 27.5758\n",
      "Epoch [15/20], Step [4300/12299],final_Loss: 1.0340\n",
      "Epoch [15/20], Step [4400/12299],final_Loss: 0.6805\n",
      "Epoch [15/20], Step [4500/12299],final_Loss: 0.4364\n",
      "Epoch [15/20], Step [4600/12299],final_Loss: 0.5354\n",
      "Epoch [15/20], Step [4700/12299],final_Loss: 1.5105\n",
      "Epoch [15/20], Step [4800/12299],final_Loss: 4.7368\n",
      "Epoch [15/20], Step [4900/12299],final_Loss: 5.9439\n",
      "Epoch [15/20], Step [5000/12299],final_Loss: 0.3110\n",
      "Epoch [15/20], Step [5100/12299],final_Loss: 5.5065\n",
      "Epoch [15/20], Step [5200/12299],final_Loss: 0.9268\n",
      "Epoch [15/20], Step [5300/12299],final_Loss: 0.7194\n",
      "Epoch [15/20], Step [5400/12299],final_Loss: 7.5854\n",
      "Epoch [15/20], Step [5500/12299],final_Loss: 1.6756\n",
      "Epoch [15/20], Step [5600/12299],final_Loss: 0.8234\n",
      "Epoch [15/20], Step [5700/12299],final_Loss: 9.0935\n",
      "Epoch [15/20], Step [5800/12299],final_Loss: 1.5745\n",
      "Epoch [15/20], Step [5900/12299],final_Loss: 1.0101\n",
      "Epoch [15/20], Step [6000/12299],final_Loss: 3.2045\n",
      "Epoch [15/20], Step [6100/12299],final_Loss: 1.9850\n",
      "Epoch [15/20], Step [6200/12299],final_Loss: 1.0191\n",
      "Epoch [15/20], Step [6300/12299],final_Loss: 0.3211\n",
      "Epoch [15/20], Step [6400/12299],final_Loss: 0.6277\n",
      "Epoch [15/20], Step [6500/12299],final_Loss: 0.4057\n",
      "Epoch [15/20], Step [6600/12299],final_Loss: 0.7974\n",
      "Epoch [15/20], Step [6700/12299],final_Loss: 0.8995\n",
      "Epoch [15/20], Step [6800/12299],final_Loss: 0.8342\n",
      "Epoch [15/20], Step [6900/12299],final_Loss: 0.6351\n",
      "Epoch [15/20], Step [7000/12299],final_Loss: 0.5733\n",
      "Epoch [15/20], Step [7100/12299],final_Loss: 0.6755\n",
      "Epoch [15/20], Step [7200/12299],final_Loss: 0.2374\n",
      "Epoch [15/20], Step [7300/12299],final_Loss: 0.7555\n",
      "Epoch [15/20], Step [7400/12299],final_Loss: 2.5676\n",
      "Epoch [15/20], Step [7500/12299],final_Loss: 2.6431\n",
      "Epoch [15/20], Step [7600/12299],final_Loss: 1.5708\n",
      "Epoch [15/20], Step [7700/12299],final_Loss: 3.3475\n",
      "Epoch [15/20], Step [7800/12299],final_Loss: 28.8489\n",
      "Epoch [15/20], Step [7900/12299],final_Loss: 0.5890\n",
      "Epoch [15/20], Step [8000/12299],final_Loss: 0.6671\n",
      "Epoch [15/20], Step [8100/12299],final_Loss: 0.4433\n",
      "Epoch [15/20], Step [8200/12299],final_Loss: 1.2344\n",
      "Epoch [15/20], Step [8300/12299],final_Loss: 0.6754\n",
      "Epoch [15/20], Step [8400/12299],final_Loss: 0.9162\n",
      "Epoch [15/20], Step [8500/12299],final_Loss: 0.1842\n",
      "Epoch [15/20], Step [8600/12299],final_Loss: 0.3679\n",
      "Epoch [15/20], Step [8700/12299],final_Loss: 0.1630\n",
      "Epoch [15/20], Step [8800/12299],final_Loss: 0.2134\n",
      "Epoch [15/20], Step [8900/12299],final_Loss: 0.5927\n",
      "Epoch [15/20], Step [9000/12299],final_Loss: 29.4993\n",
      "Epoch [15/20], Step [9100/12299],final_Loss: 2.6161\n",
      "Epoch [15/20], Step [9200/12299],final_Loss: 0.8091\n",
      "Epoch [15/20], Step [9300/12299],final_Loss: 2.1265\n",
      "Epoch [15/20], Step [9400/12299],final_Loss: 17.5147\n",
      "Epoch [15/20], Step [9500/12299],final_Loss: 0.4048\n",
      "Epoch [15/20], Step [9600/12299],final_Loss: 0.8063\n",
      "Epoch [15/20], Step [9700/12299],final_Loss: 0.2090\n",
      "Epoch [15/20], Step [9800/12299],final_Loss: 0.1510\n",
      "Epoch [15/20], Step [9900/12299],final_Loss: 0.1062\n",
      "Epoch [15/20], Step [10000/12299],final_Loss: 1.0141\n",
      "Epoch [15/20], Step [10100/12299],final_Loss: 19.2471\n",
      "Epoch [15/20], Step [10200/12299],final_Loss: 0.3354\n",
      "Epoch [15/20], Step [10300/12299],final_Loss: 5.0978\n",
      "Epoch [15/20], Step [10400/12299],final_Loss: 0.2357\n",
      "Epoch [15/20], Step [10500/12299],final_Loss: 16.4333\n",
      "Epoch [15/20], Step [10600/12299],final_Loss: 0.2811\n",
      "Epoch [15/20], Step [10700/12299],final_Loss: 2.0547\n",
      "Epoch [15/20], Step [10800/12299],final_Loss: 0.1811\n",
      "Epoch [15/20], Step [10900/12299],final_Loss: 1.1723\n",
      "Epoch [15/20], Step [11000/12299],final_Loss: 5.0052\n",
      "Epoch [15/20], Step [11100/12299],final_Loss: 0.9879\n",
      "Epoch [15/20], Step [11200/12299],final_Loss: 4.3389\n",
      "Epoch [15/20], Step [11300/12299],final_Loss: 0.7279\n",
      "Epoch [15/20], Step [11400/12299],final_Loss: 0.3721\n",
      "Epoch [15/20], Step [11500/12299],final_Loss: 0.4943\n",
      "Epoch [15/20], Step [11600/12299],final_Loss: 0.5886\n",
      "Epoch [15/20], Step [11700/12299],final_Loss: 0.2772\n",
      "Epoch [15/20], Step [11800/12299],final_Loss: 5.2391\n",
      "Epoch [15/20], Step [11900/12299],final_Loss: 0.2181\n",
      "Epoch [15/20], Step [12000/12299],final_Loss: 0.6577\n",
      "Epoch [15/20], Step [12100/12299],final_Loss: 0.2560\n",
      "Epoch [15/20], Step [12200/12299],final_Loss: 0.3431\n",
      "Epoch [16/20], Step [100/12299],final_Loss: 0.2274\n",
      "Epoch [16/20], Step [200/12299],final_Loss: 2.3831\n",
      "Epoch [16/20], Step [300/12299],final_Loss: 0.2178\n",
      "Epoch [16/20], Step [400/12299],final_Loss: 0.5092\n",
      "Epoch [16/20], Step [500/12299],final_Loss: 0.2170\n",
      "Epoch [16/20], Step [600/12299],final_Loss: 0.1538\n",
      "Epoch [16/20], Step [700/12299],final_Loss: 18.1742\n",
      "Epoch [16/20], Step [800/12299],final_Loss: 0.1255\n",
      "Epoch [16/20], Step [900/12299],final_Loss: 0.0869\n",
      "Epoch [16/20], Step [1000/12299],final_Loss: 0.4253\n",
      "Epoch [16/20], Step [1100/12299],final_Loss: 0.2458\n",
      "Epoch [16/20], Step [1200/12299],final_Loss: 0.2861\n",
      "Epoch [16/20], Step [1300/12299],final_Loss: 0.5093\n",
      "Epoch [16/20], Step [1400/12299],final_Loss: 0.2499\n",
      "Epoch [16/20], Step [1500/12299],final_Loss: 1.9120\n",
      "Epoch [16/20], Step [1600/12299],final_Loss: 36.5107\n",
      "Epoch [16/20], Step [1700/12299],final_Loss: 16.1865\n",
      "Epoch [16/20], Step [1800/12299],final_Loss: 0.1498\n",
      "Epoch [16/20], Step [1900/12299],final_Loss: 0.5115\n",
      "Epoch [16/20], Step [2000/12299],final_Loss: 9.9448\n",
      "Epoch [16/20], Step [2100/12299],final_Loss: 1.3486\n",
      "Epoch [16/20], Step [2200/12299],final_Loss: 0.3464\n",
      "Epoch [16/20], Step [2300/12299],final_Loss: 0.6377\n",
      "Epoch [16/20], Step [2400/12299],final_Loss: 0.3668\n",
      "Epoch [16/20], Step [2500/12299],final_Loss: 0.5618\n",
      "Epoch [16/20], Step [2600/12299],final_Loss: 0.2709\n",
      "Epoch [16/20], Step [2700/12299],final_Loss: 0.6125\n",
      "Epoch [16/20], Step [2800/12299],final_Loss: 0.6153\n",
      "Epoch [16/20], Step [2900/12299],final_Loss: 0.2828\n",
      "Epoch [16/20], Step [3000/12299],final_Loss: 0.6037\n",
      "Epoch [16/20], Step [3100/12299],final_Loss: 0.9350\n",
      "Epoch [16/20], Step [3200/12299],final_Loss: 0.7006\n",
      "Epoch [16/20], Step [3300/12299],final_Loss: 7.2088\n",
      "Epoch [16/20], Step [3400/12299],final_Loss: 0.2462\n",
      "Epoch [16/20], Step [3500/12299],final_Loss: 0.2865\n",
      "Epoch [16/20], Step [3600/12299],final_Loss: 1.3222\n",
      "Epoch [16/20], Step [3700/12299],final_Loss: 0.5913\n",
      "Epoch [16/20], Step [3800/12299],final_Loss: 0.2347\n",
      "Epoch [16/20], Step [3900/12299],final_Loss: 0.3034\n",
      "Epoch [16/20], Step [4000/12299],final_Loss: 4.2670\n",
      "Epoch [16/20], Step [4100/12299],final_Loss: 0.3354\n",
      "Epoch [16/20], Step [4200/12299],final_Loss: 32.1722\n",
      "Epoch [16/20], Step [4300/12299],final_Loss: 1.2080\n",
      "Epoch [16/20], Step [4400/12299],final_Loss: 0.8497\n",
      "Epoch [16/20], Step [4500/12299],final_Loss: 0.3807\n",
      "Epoch [16/20], Step [4600/12299],final_Loss: 0.5353\n",
      "Epoch [16/20], Step [4700/12299],final_Loss: 1.1885\n",
      "Epoch [16/20], Step [4800/12299],final_Loss: 4.6129\n",
      "Epoch [16/20], Step [4900/12299],final_Loss: 4.2112\n",
      "Epoch [16/20], Step [5000/12299],final_Loss: 0.3007\n",
      "Epoch [16/20], Step [5100/12299],final_Loss: 4.1397\n",
      "Epoch [16/20], Step [5200/12299],final_Loss: 0.8263\n",
      "Epoch [16/20], Step [5300/12299],final_Loss: 0.6558\n",
      "Epoch [16/20], Step [5400/12299],final_Loss: 8.9217\n",
      "Epoch [16/20], Step [5500/12299],final_Loss: 1.7010\n",
      "Epoch [16/20], Step [5600/12299],final_Loss: 0.5636\n",
      "Epoch [16/20], Step [5700/12299],final_Loss: 9.6830\n",
      "Epoch [16/20], Step [5800/12299],final_Loss: 0.9233\n",
      "Epoch [16/20], Step [5900/12299],final_Loss: 1.4800\n",
      "Epoch [16/20], Step [6000/12299],final_Loss: 3.4720\n",
      "Epoch [16/20], Step [6100/12299],final_Loss: 1.2439\n",
      "Epoch [16/20], Step [6200/12299],final_Loss: 1.1237\n",
      "Epoch [16/20], Step [6300/12299],final_Loss: 0.3273\n",
      "Epoch [16/20], Step [6400/12299],final_Loss: 0.5346\n",
      "Epoch [16/20], Step [6500/12299],final_Loss: 0.4728\n",
      "Epoch [16/20], Step [6600/12299],final_Loss: 0.7025\n",
      "Epoch [16/20], Step [6700/12299],final_Loss: 0.8736\n",
      "Epoch [16/20], Step [6800/12299],final_Loss: 0.5500\n",
      "Epoch [16/20], Step [6900/12299],final_Loss: 0.6650\n",
      "Epoch [16/20], Step [7000/12299],final_Loss: 0.5895\n",
      "Epoch [16/20], Step [7100/12299],final_Loss: 0.7188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/20], Step [7200/12299],final_Loss: 0.2529\n",
      "Epoch [16/20], Step [7300/12299],final_Loss: 0.7186\n",
      "Epoch [16/20], Step [7400/12299],final_Loss: 2.7800\n",
      "Epoch [16/20], Step [7500/12299],final_Loss: 1.8750\n",
      "Epoch [16/20], Step [7600/12299],final_Loss: 1.1943\n",
      "Epoch [16/20], Step [7700/12299],final_Loss: 3.1917\n",
      "Epoch [16/20], Step [7800/12299],final_Loss: 17.5734\n",
      "Epoch [16/20], Step [7900/12299],final_Loss: 0.5262\n",
      "Epoch [16/20], Step [8000/12299],final_Loss: 0.4006\n",
      "Epoch [16/20], Step [8100/12299],final_Loss: 0.3899\n",
      "Epoch [16/20], Step [8200/12299],final_Loss: 0.9592\n",
      "Epoch [16/20], Step [8300/12299],final_Loss: 0.7822\n",
      "Epoch [16/20], Step [8400/12299],final_Loss: 0.8128\n",
      "Epoch [16/20], Step [8500/12299],final_Loss: 0.1824\n",
      "Epoch [16/20], Step [8600/12299],final_Loss: 0.2906\n",
      "Epoch [16/20], Step [8700/12299],final_Loss: 0.1296\n",
      "Epoch [16/20], Step [8800/12299],final_Loss: 0.2497\n",
      "Epoch [16/20], Step [8900/12299],final_Loss: 0.4938\n",
      "Epoch [16/20], Step [9000/12299],final_Loss: 29.3797\n",
      "Epoch [16/20], Step [9100/12299],final_Loss: 2.2880\n",
      "Epoch [16/20], Step [9200/12299],final_Loss: 0.7002\n",
      "Epoch [16/20], Step [9300/12299],final_Loss: 1.9912\n",
      "Epoch [16/20], Step [9400/12299],final_Loss: 15.3163\n",
      "Epoch [16/20], Step [9500/12299],final_Loss: 0.3222\n",
      "Epoch [16/20], Step [9600/12299],final_Loss: 0.6733\n",
      "Epoch [16/20], Step [9700/12299],final_Loss: 0.2426\n",
      "Epoch [16/20], Step [9800/12299],final_Loss: 0.1257\n",
      "Epoch [16/20], Step [9900/12299],final_Loss: 0.0978\n",
      "Epoch [16/20], Step [10000/12299],final_Loss: 0.8941\n",
      "Epoch [16/20], Step [10100/12299],final_Loss: 16.7401\n",
      "Epoch [16/20], Step [10200/12299],final_Loss: 0.3116\n",
      "Epoch [16/20], Step [10300/12299],final_Loss: 4.6946\n",
      "Epoch [16/20], Step [10400/12299],final_Loss: 0.2367\n",
      "Epoch [16/20], Step [10500/12299],final_Loss: 17.4475\n",
      "Epoch [16/20], Step [10600/12299],final_Loss: 0.2937\n",
      "Epoch [16/20], Step [10700/12299],final_Loss: 2.2809\n",
      "Epoch [16/20], Step [10800/12299],final_Loss: 0.2162\n",
      "Epoch [16/20], Step [10900/12299],final_Loss: 2.2803\n",
      "Epoch [16/20], Step [11000/12299],final_Loss: 6.1819\n",
      "Epoch [16/20], Step [11100/12299],final_Loss: 0.6478\n",
      "Epoch [16/20], Step [11200/12299],final_Loss: 2.4439\n",
      "Epoch [16/20], Step [11300/12299],final_Loss: 0.6204\n",
      "Epoch [16/20], Step [11400/12299],final_Loss: 0.3543\n",
      "Epoch [16/20], Step [11500/12299],final_Loss: 0.4206\n",
      "Epoch [16/20], Step [11600/12299],final_Loss: 0.5984\n",
      "Epoch [16/20], Step [11700/12299],final_Loss: 0.2752\n",
      "Epoch [16/20], Step [11800/12299],final_Loss: 5.0374\n",
      "Epoch [16/20], Step [11900/12299],final_Loss: 0.2016\n",
      "Epoch [16/20], Step [12000/12299],final_Loss: 0.5305\n",
      "Epoch [16/20], Step [12100/12299],final_Loss: 0.2369\n",
      "Epoch [16/20], Step [12200/12299],final_Loss: 0.3337\n",
      "Epoch [17/20], Step [100/12299],final_Loss: 0.2479\n",
      "Epoch [17/20], Step [200/12299],final_Loss: 2.5650\n",
      "Epoch [17/20], Step [300/12299],final_Loss: 0.2025\n",
      "Epoch [17/20], Step [400/12299],final_Loss: 0.5177\n",
      "Epoch [17/20], Step [500/12299],final_Loss: 0.2313\n",
      "Epoch [17/20], Step [600/12299],final_Loss: 0.1904\n",
      "Epoch [17/20], Step [700/12299],final_Loss: 18.9301\n",
      "Epoch [17/20], Step [800/12299],final_Loss: 0.1366\n",
      "Epoch [17/20], Step [900/12299],final_Loss: 0.1088\n",
      "Epoch [17/20], Step [1000/12299],final_Loss: 0.3968\n",
      "Epoch [17/20], Step [1100/12299],final_Loss: 0.1556\n",
      "Epoch [17/20], Step [1200/12299],final_Loss: 0.3000\n",
      "Epoch [17/20], Step [1300/12299],final_Loss: 0.5085\n",
      "Epoch [17/20], Step [1400/12299],final_Loss: 0.2616\n",
      "Epoch [17/20], Step [1500/12299],final_Loss: 1.3237\n",
      "Epoch [17/20], Step [1600/12299],final_Loss: 47.0899\n",
      "Epoch [17/20], Step [1700/12299],final_Loss: 17.2817\n",
      "Epoch [17/20], Step [1800/12299],final_Loss: 0.1797\n",
      "Epoch [17/20], Step [1900/12299],final_Loss: 0.4672\n",
      "Epoch [17/20], Step [2000/12299],final_Loss: 10.9939\n",
      "Epoch [17/20], Step [2100/12299],final_Loss: 1.1765\n",
      "Epoch [17/20], Step [2200/12299],final_Loss: 0.3037\n",
      "Epoch [17/20], Step [2300/12299],final_Loss: 0.6982\n",
      "Epoch [17/20], Step [2400/12299],final_Loss: 0.2874\n",
      "Epoch [17/20], Step [2500/12299],final_Loss: 0.3439\n",
      "Epoch [17/20], Step [2600/12299],final_Loss: 0.2453\n",
      "Epoch [17/20], Step [2700/12299],final_Loss: 0.6322\n",
      "Epoch [17/20], Step [2800/12299],final_Loss: 0.5593\n",
      "Epoch [17/20], Step [2900/12299],final_Loss: 0.2313\n",
      "Epoch [17/20], Step [3000/12299],final_Loss: 0.5877\n",
      "Epoch [17/20], Step [3100/12299],final_Loss: 1.1299\n",
      "Epoch [17/20], Step [3200/12299],final_Loss: 0.5395\n",
      "Epoch [17/20], Step [3300/12299],final_Loss: 8.6838\n",
      "Epoch [17/20], Step [3400/12299],final_Loss: 0.2132\n",
      "Epoch [17/20], Step [3500/12299],final_Loss: 0.4313\n",
      "Epoch [17/20], Step [3600/12299],final_Loss: 1.0870\n",
      "Epoch [17/20], Step [3700/12299],final_Loss: 0.5797\n",
      "Epoch [17/20], Step [3800/12299],final_Loss: 0.2566\n",
      "Epoch [17/20], Step [3900/12299],final_Loss: 0.3310\n",
      "Epoch [17/20], Step [4000/12299],final_Loss: 4.2494\n",
      "Epoch [17/20], Step [4100/12299],final_Loss: 0.3244\n",
      "Epoch [17/20], Step [4200/12299],final_Loss: 32.1101\n",
      "Epoch [17/20], Step [4300/12299],final_Loss: 1.1019\n",
      "Epoch [17/20], Step [4400/12299],final_Loss: 0.6535\n",
      "Epoch [17/20], Step [4500/12299],final_Loss: 0.4654\n",
      "Epoch [17/20], Step [4600/12299],final_Loss: 0.5217\n",
      "Epoch [17/20], Step [4700/12299],final_Loss: 1.5096\n",
      "Epoch [17/20], Step [4800/12299],final_Loss: 4.2003\n",
      "Epoch [17/20], Step [4900/12299],final_Loss: 6.0390\n",
      "Epoch [17/20], Step [5000/12299],final_Loss: 0.3370\n",
      "Epoch [17/20], Step [5100/12299],final_Loss: 4.7466\n",
      "Epoch [17/20], Step [5200/12299],final_Loss: 0.9002\n",
      "Epoch [17/20], Step [5300/12299],final_Loss: 0.6272\n",
      "Epoch [17/20], Step [5400/12299],final_Loss: 11.5273\n",
      "Epoch [17/20], Step [5500/12299],final_Loss: 1.6027\n",
      "Epoch [17/20], Step [5600/12299],final_Loss: 0.7865\n",
      "Epoch [17/20], Step [5700/12299],final_Loss: 12.0621\n",
      "Epoch [17/20], Step [5800/12299],final_Loss: 1.3080\n",
      "Epoch [17/20], Step [5900/12299],final_Loss: 1.2869\n",
      "Epoch [17/20], Step [6000/12299],final_Loss: 4.3522\n",
      "Epoch [17/20], Step [6100/12299],final_Loss: 1.8881\n",
      "Epoch [17/20], Step [6200/12299],final_Loss: 1.5214\n",
      "Epoch [17/20], Step [6300/12299],final_Loss: 0.4130\n",
      "Epoch [17/20], Step [6400/12299],final_Loss: 0.6586\n",
      "Epoch [17/20], Step [6500/12299],final_Loss: 0.5785\n",
      "Epoch [17/20], Step [6600/12299],final_Loss: 0.9002\n",
      "Epoch [17/20], Step [6700/12299],final_Loss: 0.7974\n",
      "Epoch [17/20], Step [6800/12299],final_Loss: 0.7010\n",
      "Epoch [17/20], Step [6900/12299],final_Loss: 0.6336\n",
      "Epoch [17/20], Step [7000/12299],final_Loss: 0.4365\n",
      "Epoch [17/20], Step [7100/12299],final_Loss: 0.5924\n",
      "Epoch [17/20], Step [7200/12299],final_Loss: 0.2242\n",
      "Epoch [17/20], Step [7300/12299],final_Loss: 0.6339\n",
      "Epoch [17/20], Step [7400/12299],final_Loss: 2.1760\n",
      "Epoch [17/20], Step [7500/12299],final_Loss: 2.0508\n",
      "Epoch [17/20], Step [7600/12299],final_Loss: 1.1584\n",
      "Epoch [17/20], Step [7700/12299],final_Loss: 3.4497\n",
      "Epoch [17/20], Step [7800/12299],final_Loss: 19.4694\n",
      "Epoch [17/20], Step [7900/12299],final_Loss: 0.5849\n",
      "Epoch [17/20], Step [8000/12299],final_Loss: 0.4284\n",
      "Epoch [17/20], Step [8100/12299],final_Loss: 0.4196\n",
      "Epoch [17/20], Step [8200/12299],final_Loss: 1.1786\n",
      "Epoch [17/20], Step [8300/12299],final_Loss: 1.0569\n",
      "Epoch [17/20], Step [8400/12299],final_Loss: 1.8082\n",
      "Epoch [17/20], Step [8500/12299],final_Loss: 0.1637\n",
      "Epoch [17/20], Step [8600/12299],final_Loss: 0.3870\n",
      "Epoch [17/20], Step [8700/12299],final_Loss: 0.1481\n",
      "Epoch [17/20], Step [8800/12299],final_Loss: 0.2233\n",
      "Epoch [17/20], Step [8900/12299],final_Loss: 0.8592\n",
      "Epoch [17/20], Step [9000/12299],final_Loss: 26.9803\n",
      "Epoch [17/20], Step [9100/12299],final_Loss: 2.9643\n",
      "Epoch [17/20], Step [9200/12299],final_Loss: 0.8745\n",
      "Epoch [17/20], Step [9300/12299],final_Loss: 2.2621\n",
      "Epoch [17/20], Step [9400/12299],final_Loss: 16.0491\n",
      "Epoch [17/20], Step [9500/12299],final_Loss: 0.3142\n",
      "Epoch [17/20], Step [9600/12299],final_Loss: 0.8853\n",
      "Epoch [17/20], Step [9700/12299],final_Loss: 0.2529\n",
      "Epoch [17/20], Step [9800/12299],final_Loss: 0.1402\n",
      "Epoch [17/20], Step [9900/12299],final_Loss: 0.1021\n",
      "Epoch [17/20], Step [10000/12299],final_Loss: 0.8502\n",
      "Epoch [17/20], Step [10100/12299],final_Loss: 19.6779\n",
      "Epoch [17/20], Step [10200/12299],final_Loss: 0.3739\n",
      "Epoch [17/20], Step [10300/12299],final_Loss: 5.6898\n",
      "Epoch [17/20], Step [10400/12299],final_Loss: 0.2391\n",
      "Epoch [17/20], Step [10500/12299],final_Loss: 18.8967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/20], Step [10600/12299],final_Loss: 0.3110\n",
      "Epoch [17/20], Step [10700/12299],final_Loss: 2.3739\n",
      "Epoch [17/20], Step [10800/12299],final_Loss: 0.2373\n",
      "Epoch [17/20], Step [10900/12299],final_Loss: 1.4205\n",
      "Epoch [17/20], Step [11000/12299],final_Loss: 4.3432\n",
      "Epoch [17/20], Step [11100/12299],final_Loss: 0.6547\n",
      "Epoch [17/20], Step [11200/12299],final_Loss: 3.4599\n",
      "Epoch [17/20], Step [11300/12299],final_Loss: 0.6316\n",
      "Epoch [17/20], Step [11400/12299],final_Loss: 0.3532\n",
      "Epoch [17/20], Step [11500/12299],final_Loss: 0.4699\n",
      "Epoch [17/20], Step [11600/12299],final_Loss: 0.6010\n",
      "Epoch [17/20], Step [11700/12299],final_Loss: 0.2611\n",
      "Epoch [17/20], Step [11800/12299],final_Loss: 5.1428\n",
      "Epoch [17/20], Step [11900/12299],final_Loss: 0.2247\n",
      "Epoch [17/20], Step [12000/12299],final_Loss: 0.5790\n",
      "Epoch [17/20], Step [12100/12299],final_Loss: 0.2473\n",
      "Epoch [17/20], Step [12200/12299],final_Loss: 0.2480\n",
      "Epoch [18/20], Step [100/12299],final_Loss: 0.2393\n",
      "Epoch [18/20], Step [200/12299],final_Loss: 1.8988\n",
      "Epoch [18/20], Step [300/12299],final_Loss: 0.2407\n",
      "Epoch [18/20], Step [400/12299],final_Loss: 0.6065\n",
      "Epoch [18/20], Step [500/12299],final_Loss: 0.2063\n",
      "Epoch [18/20], Step [600/12299],final_Loss: 0.1576\n",
      "Epoch [18/20], Step [700/12299],final_Loss: 16.6741\n",
      "Epoch [18/20], Step [800/12299],final_Loss: 0.1509\n",
      "Epoch [18/20], Step [900/12299],final_Loss: 0.1127\n",
      "Epoch [18/20], Step [1000/12299],final_Loss: 0.4234\n",
      "Epoch [18/20], Step [1100/12299],final_Loss: 0.1718\n",
      "Epoch [18/20], Step [1200/12299],final_Loss: 0.3181\n",
      "Epoch [18/20], Step [1300/12299],final_Loss: 0.4822\n",
      "Epoch [18/20], Step [1400/12299],final_Loss: 0.1858\n",
      "Epoch [18/20], Step [1500/12299],final_Loss: 1.2608\n",
      "Epoch [18/20], Step [1600/12299],final_Loss: 35.9092\n",
      "Epoch [18/20], Step [1700/12299],final_Loss: 17.4729\n",
      "Epoch [18/20], Step [1800/12299],final_Loss: 0.2267\n",
      "Epoch [18/20], Step [1900/12299],final_Loss: 0.5150\n",
      "Epoch [18/20], Step [2000/12299],final_Loss: 8.8471\n",
      "Epoch [18/20], Step [2100/12299],final_Loss: 1.3650\n",
      "Epoch [18/20], Step [2200/12299],final_Loss: 0.2999\n",
      "Epoch [18/20], Step [2300/12299],final_Loss: 0.8884\n",
      "Epoch [18/20], Step [2400/12299],final_Loss: 0.2599\n",
      "Epoch [18/20], Step [2500/12299],final_Loss: 0.3731\n",
      "Epoch [18/20], Step [2600/12299],final_Loss: 0.2882\n",
      "Epoch [18/20], Step [2700/12299],final_Loss: 0.6466\n",
      "Epoch [18/20], Step [2800/12299],final_Loss: 0.6677\n",
      "Epoch [18/20], Step [2900/12299],final_Loss: 0.2470\n",
      "Epoch [18/20], Step [3000/12299],final_Loss: 0.5732\n",
      "Epoch [18/20], Step [3100/12299],final_Loss: 1.1317\n",
      "Epoch [18/20], Step [3200/12299],final_Loss: 0.5721\n",
      "Epoch [18/20], Step [3300/12299],final_Loss: 9.5143\n",
      "Epoch [18/20], Step [3400/12299],final_Loss: 0.2386\n",
      "Epoch [18/20], Step [3500/12299],final_Loss: 0.2757\n",
      "Epoch [18/20], Step [3600/12299],final_Loss: 1.3218\n",
      "Epoch [18/20], Step [3700/12299],final_Loss: 0.6550\n",
      "Epoch [18/20], Step [3800/12299],final_Loss: 0.2366\n",
      "Epoch [18/20], Step [3900/12299],final_Loss: 0.3223\n",
      "Epoch [18/20], Step [4000/12299],final_Loss: 2.9657\n",
      "Epoch [18/20], Step [4100/12299],final_Loss: 0.3548\n",
      "Epoch [18/20], Step [4200/12299],final_Loss: 34.6263\n",
      "Epoch [18/20], Step [4300/12299],final_Loss: 0.9912\n",
      "Epoch [18/20], Step [4400/12299],final_Loss: 0.6267\n",
      "Epoch [18/20], Step [4500/12299],final_Loss: 0.3716\n",
      "Epoch [18/20], Step [4600/12299],final_Loss: 0.4535\n",
      "Epoch [18/20], Step [4700/12299],final_Loss: 1.4468\n",
      "Epoch [18/20], Step [4800/12299],final_Loss: 3.1582\n",
      "Epoch [18/20], Step [4900/12299],final_Loss: 3.8423\n",
      "Epoch [18/20], Step [5000/12299],final_Loss: 0.3481\n",
      "Epoch [18/20], Step [5100/12299],final_Loss: 4.6963\n",
      "Epoch [18/20], Step [5200/12299],final_Loss: 0.8607\n",
      "Epoch [18/20], Step [5300/12299],final_Loss: 0.5825\n",
      "Epoch [18/20], Step [5400/12299],final_Loss: 11.6009\n",
      "Epoch [18/20], Step [5500/12299],final_Loss: 1.5790\n",
      "Epoch [18/20], Step [5600/12299],final_Loss: 1.0002\n",
      "Epoch [18/20], Step [5700/12299],final_Loss: 9.2214\n",
      "Epoch [18/20], Step [5800/12299],final_Loss: 2.3732\n",
      "Epoch [18/20], Step [5900/12299],final_Loss: 1.5153\n",
      "Epoch [18/20], Step [6000/12299],final_Loss: 3.5808\n",
      "Epoch [18/20], Step [6100/12299],final_Loss: 1.6763\n",
      "Epoch [18/20], Step [6200/12299],final_Loss: 1.5563\n",
      "Epoch [18/20], Step [6300/12299],final_Loss: 0.4695\n",
      "Epoch [18/20], Step [6400/12299],final_Loss: 0.5505\n",
      "Epoch [18/20], Step [6500/12299],final_Loss: 0.5424\n",
      "Epoch [18/20], Step [6600/12299],final_Loss: 1.0362\n",
      "Epoch [18/20], Step [6700/12299],final_Loss: 0.8043\n",
      "Epoch [18/20], Step [6800/12299],final_Loss: 0.6581\n",
      "Epoch [18/20], Step [6900/12299],final_Loss: 0.7451\n",
      "Epoch [18/20], Step [7000/12299],final_Loss: 0.4625\n",
      "Epoch [18/20], Step [7100/12299],final_Loss: 0.6597\n",
      "Epoch [18/20], Step [7200/12299],final_Loss: 0.2470\n",
      "Epoch [18/20], Step [7300/12299],final_Loss: 0.6053\n",
      "Epoch [18/20], Step [7400/12299],final_Loss: 2.1704\n",
      "Epoch [18/20], Step [7500/12299],final_Loss: 2.6421\n",
      "Epoch [18/20], Step [7600/12299],final_Loss: 1.4745\n",
      "Epoch [18/20], Step [7700/12299],final_Loss: 2.9645\n",
      "Epoch [18/20], Step [7800/12299],final_Loss: 11.4747\n",
      "Epoch [18/20], Step [7900/12299],final_Loss: 0.6337\n",
      "Epoch [18/20], Step [8000/12299],final_Loss: 0.3817\n",
      "Epoch [18/20], Step [8100/12299],final_Loss: 0.4158\n",
      "Epoch [18/20], Step [8200/12299],final_Loss: 1.1320\n",
      "Epoch [18/20], Step [8300/12299],final_Loss: 0.8447\n",
      "Epoch [18/20], Step [8400/12299],final_Loss: 1.1025\n",
      "Epoch [18/20], Step [8500/12299],final_Loss: 0.1998\n",
      "Epoch [18/20], Step [8600/12299],final_Loss: 0.4268\n",
      "Epoch [18/20], Step [8700/12299],final_Loss: 0.1705\n",
      "Epoch [18/20], Step [8800/12299],final_Loss: 0.2206\n",
      "Epoch [18/20], Step [8900/12299],final_Loss: 0.7525\n",
      "Epoch [18/20], Step [9000/12299],final_Loss: 28.1752\n",
      "Epoch [18/20], Step [9100/12299],final_Loss: 4.7946\n",
      "Epoch [18/20], Step [9200/12299],final_Loss: 0.9903\n",
      "Epoch [18/20], Step [9300/12299],final_Loss: 2.9559\n",
      "Epoch [18/20], Step [9400/12299],final_Loss: 19.7063\n",
      "Epoch [18/20], Step [9500/12299],final_Loss: 0.3001\n",
      "Epoch [18/20], Step [9600/12299],final_Loss: 0.9732\n",
      "Epoch [18/20], Step [9700/12299],final_Loss: 0.2374\n",
      "Epoch [18/20], Step [9800/12299],final_Loss: 0.1475\n",
      "Epoch [18/20], Step [9900/12299],final_Loss: 0.1200\n",
      "Epoch [18/20], Step [10000/12299],final_Loss: 0.9410\n",
      "Epoch [18/20], Step [10100/12299],final_Loss: 19.8453\n",
      "Epoch [18/20], Step [10200/12299],final_Loss: 0.4786\n",
      "Epoch [18/20], Step [10300/12299],final_Loss: 5.1427\n",
      "Epoch [18/20], Step [10400/12299],final_Loss: 0.2733\n",
      "Epoch [18/20], Step [10500/12299],final_Loss: 18.0746\n",
      "Epoch [18/20], Step [10600/12299],final_Loss: 0.3225\n",
      "Epoch [18/20], Step [10700/12299],final_Loss: 3.2106\n",
      "Epoch [18/20], Step [10800/12299],final_Loss: 0.2768\n",
      "Epoch [18/20], Step [10900/12299],final_Loss: 1.8326\n",
      "Epoch [18/20], Step [11000/12299],final_Loss: 6.8873\n",
      "Epoch [18/20], Step [11100/12299],final_Loss: 0.8513\n",
      "Epoch [18/20], Step [11200/12299],final_Loss: 4.5269\n",
      "Epoch [18/20], Step [11300/12299],final_Loss: 0.8757\n",
      "Epoch [18/20], Step [11400/12299],final_Loss: 0.4811\n",
      "Epoch [18/20], Step [11500/12299],final_Loss: 0.6289\n",
      "Epoch [18/20], Step [11600/12299],final_Loss: 0.5458\n",
      "Epoch [18/20], Step [11700/12299],final_Loss: 0.2895\n",
      "Epoch [18/20], Step [11800/12299],final_Loss: 6.2423\n",
      "Epoch [18/20], Step [11900/12299],final_Loss: 0.2347\n",
      "Epoch [18/20], Step [12000/12299],final_Loss: 0.6306\n",
      "Epoch [18/20], Step [12100/12299],final_Loss: 0.2602\n",
      "Epoch [18/20], Step [12200/12299],final_Loss: 0.2413\n",
      "Epoch [19/20], Step [100/12299],final_Loss: 0.3178\n",
      "Epoch [19/20], Step [200/12299],final_Loss: 3.2945\n",
      "Epoch [19/20], Step [300/12299],final_Loss: 0.2850\n",
      "Epoch [19/20], Step [400/12299],final_Loss: 0.7122\n",
      "Epoch [19/20], Step [500/12299],final_Loss: 0.2719\n",
      "Epoch [19/20], Step [600/12299],final_Loss: 0.1657\n",
      "Epoch [19/20], Step [700/12299],final_Loss: 14.8019\n",
      "Epoch [19/20], Step [800/12299],final_Loss: 0.1698\n",
      "Epoch [19/20], Step [900/12299],final_Loss: 0.1216\n",
      "Epoch [19/20], Step [1000/12299],final_Loss: 0.4291\n",
      "Epoch [19/20], Step [1100/12299],final_Loss: 0.1813\n",
      "Epoch [19/20], Step [1200/12299],final_Loss: 0.3416\n",
      "Epoch [19/20], Step [1300/12299],final_Loss: 0.3233\n",
      "Epoch [19/20], Step [1400/12299],final_Loss: 0.2162\n",
      "Epoch [19/20], Step [1500/12299],final_Loss: 1.5106\n",
      "Epoch [19/20], Step [1600/12299],final_Loss: 27.8896\n",
      "Epoch [19/20], Step [1700/12299],final_Loss: 15.5134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/20], Step [1800/12299],final_Loss: 0.1669\n",
      "Epoch [19/20], Step [1900/12299],final_Loss: 0.5334\n",
      "Epoch [19/20], Step [2000/12299],final_Loss: 6.4588\n",
      "Epoch [19/20], Step [2100/12299],final_Loss: 1.2257\n",
      "Epoch [19/20], Step [2200/12299],final_Loss: 0.4694\n",
      "Epoch [19/20], Step [2300/12299],final_Loss: 0.8742\n",
      "Epoch [19/20], Step [2400/12299],final_Loss: 0.2840\n",
      "Epoch [19/20], Step [2500/12299],final_Loss: 0.4558\n",
      "Epoch [19/20], Step [2600/12299],final_Loss: 0.2883\n",
      "Epoch [19/20], Step [2700/12299],final_Loss: 0.6238\n",
      "Epoch [19/20], Step [2800/12299],final_Loss: 0.9627\n",
      "Epoch [19/20], Step [2900/12299],final_Loss: 0.2761\n",
      "Epoch [19/20], Step [3000/12299],final_Loss: 0.7493\n",
      "Epoch [19/20], Step [3100/12299],final_Loss: 1.1298\n",
      "Epoch [19/20], Step [3200/12299],final_Loss: 0.6613\n",
      "Epoch [19/20], Step [3300/12299],final_Loss: 8.6395\n",
      "Epoch [19/20], Step [3400/12299],final_Loss: 0.2169\n",
      "Epoch [19/20], Step [3500/12299],final_Loss: 0.3296\n",
      "Epoch [19/20], Step [3600/12299],final_Loss: 1.0623\n",
      "Epoch [19/20], Step [3700/12299],final_Loss: 0.9712\n",
      "Epoch [19/20], Step [3800/12299],final_Loss: 0.2767\n",
      "Epoch [19/20], Step [3900/12299],final_Loss: 0.3490\n",
      "Epoch [19/20], Step [4000/12299],final_Loss: 4.1259\n",
      "Epoch [19/20], Step [4100/12299],final_Loss: 0.3876\n",
      "Epoch [19/20], Step [4200/12299],final_Loss: 27.3211\n",
      "Epoch [19/20], Step [4300/12299],final_Loss: 1.1350\n",
      "Epoch [19/20], Step [4400/12299],final_Loss: 0.5865\n",
      "Epoch [19/20], Step [4500/12299],final_Loss: 0.3076\n",
      "Epoch [19/20], Step [4600/12299],final_Loss: 0.4497\n",
      "Epoch [19/20], Step [4700/12299],final_Loss: 1.3116\n",
      "Epoch [19/20], Step [4800/12299],final_Loss: 5.8830\n",
      "Epoch [19/20], Step [4900/12299],final_Loss: 6.0670\n",
      "Epoch [19/20], Step [5000/12299],final_Loss: 0.3037\n",
      "Epoch [19/20], Step [5100/12299],final_Loss: 4.0072\n",
      "Epoch [19/20], Step [5200/12299],final_Loss: 0.8757\n",
      "Epoch [19/20], Step [5300/12299],final_Loss: 0.9615\n",
      "Epoch [19/20], Step [5400/12299],final_Loss: 11.7116\n",
      "Epoch [19/20], Step [5500/12299],final_Loss: 2.6576\n",
      "Epoch [19/20], Step [5600/12299],final_Loss: 0.6418\n",
      "Epoch [19/20], Step [5700/12299],final_Loss: 9.3943\n",
      "Epoch [19/20], Step [5800/12299],final_Loss: 1.5525\n",
      "Epoch [19/20], Step [5900/12299],final_Loss: 1.3066\n",
      "Epoch [19/20], Step [6000/12299],final_Loss: 3.9782\n",
      "Epoch [19/20], Step [6100/12299],final_Loss: 1.2787\n",
      "Epoch [19/20], Step [6200/12299],final_Loss: 0.9656\n",
      "Epoch [19/20], Step [6300/12299],final_Loss: 0.3813\n",
      "Epoch [19/20], Step [6400/12299],final_Loss: 0.6536\n",
      "Epoch [19/20], Step [6500/12299],final_Loss: 0.4003\n",
      "Epoch [19/20], Step [6600/12299],final_Loss: 0.7496\n",
      "Epoch [19/20], Step [6700/12299],final_Loss: 0.5729\n",
      "Epoch [19/20], Step [6800/12299],final_Loss: 0.5959\n",
      "Epoch [19/20], Step [6900/12299],final_Loss: 0.7404\n",
      "Epoch [19/20], Step [7000/12299],final_Loss: 0.5855\n",
      "Epoch [19/20], Step [7100/12299],final_Loss: 0.5733\n",
      "Epoch [19/20], Step [7200/12299],final_Loss: 0.2081\n",
      "Epoch [19/20], Step [7300/12299],final_Loss: 0.6157\n",
      "Epoch [19/20], Step [7400/12299],final_Loss: 2.6932\n",
      "Epoch [19/20], Step [7500/12299],final_Loss: 3.0752\n",
      "Epoch [19/20], Step [7600/12299],final_Loss: 1.2899\n",
      "Epoch [19/20], Step [7700/12299],final_Loss: 2.7112\n",
      "Epoch [19/20], Step [7800/12299],final_Loss: 29.5175\n",
      "Epoch [19/20], Step [7900/12299],final_Loss: 0.6361\n",
      "Epoch [19/20], Step [8000/12299],final_Loss: 0.5051\n",
      "Epoch [19/20], Step [8100/12299],final_Loss: 0.5333\n",
      "Epoch [19/20], Step [8200/12299],final_Loss: 1.4191\n",
      "Epoch [19/20], Step [8300/12299],final_Loss: 0.8351\n",
      "Epoch [19/20], Step [8400/12299],final_Loss: 1.4718\n",
      "Epoch [19/20], Step [8500/12299],final_Loss: 0.1811\n",
      "Epoch [19/20], Step [8600/12299],final_Loss: 0.5104\n",
      "Epoch [19/20], Step [8700/12299],final_Loss: 0.1694\n",
      "Epoch [19/20], Step [8800/12299],final_Loss: 0.2179\n",
      "Epoch [19/20], Step [8900/12299],final_Loss: 0.9177\n",
      "Epoch [19/20], Step [9000/12299],final_Loss: 27.0263\n",
      "Epoch [19/20], Step [9100/12299],final_Loss: 3.4336\n",
      "Epoch [19/20], Step [9200/12299],final_Loss: 0.8700\n",
      "Epoch [19/20], Step [9300/12299],final_Loss: 3.1571\n",
      "Epoch [19/20], Step [9400/12299],final_Loss: 16.5639\n",
      "Epoch [19/20], Step [9500/12299],final_Loss: 0.3353\n",
      "Epoch [19/20], Step [9600/12299],final_Loss: 0.8460\n",
      "Epoch [19/20], Step [9700/12299],final_Loss: 0.2225\n",
      "Epoch [19/20], Step [9800/12299],final_Loss: 0.1413\n",
      "Epoch [19/20], Step [9900/12299],final_Loss: 0.1117\n",
      "Epoch [19/20], Step [10000/12299],final_Loss: 0.9841\n",
      "Epoch [19/20], Step [10100/12299],final_Loss: 18.2750\n",
      "Epoch [19/20], Step [10200/12299],final_Loss: 0.3822\n",
      "Epoch [19/20], Step [10300/12299],final_Loss: 6.2451\n",
      "Epoch [19/20], Step [10400/12299],final_Loss: 0.2698\n",
      "Epoch [19/20], Step [10500/12299],final_Loss: 17.4260\n",
      "Epoch [19/20], Step [10600/12299],final_Loss: 0.2751\n",
      "Epoch [19/20], Step [10700/12299],final_Loss: 2.5320\n",
      "Epoch [19/20], Step [10800/12299],final_Loss: 0.3018\n",
      "Epoch [19/20], Step [10900/12299],final_Loss: 1.2985\n",
      "Epoch [19/20], Step [11000/12299],final_Loss: 5.8176\n",
      "Epoch [19/20], Step [11100/12299],final_Loss: 0.8916\n",
      "Epoch [19/20], Step [11200/12299],final_Loss: 2.6885\n",
      "Epoch [19/20], Step [11300/12299],final_Loss: 0.6828\n",
      "Epoch [19/20], Step [11400/12299],final_Loss: 0.3974\n",
      "Epoch [19/20], Step [11500/12299],final_Loss: 0.4424\n",
      "Epoch [19/20], Step [11600/12299],final_Loss: 0.5586\n",
      "Epoch [19/20], Step [11700/12299],final_Loss: 0.2631\n",
      "Epoch [19/20], Step [11800/12299],final_Loss: 4.6357\n",
      "Epoch [19/20], Step [11900/12299],final_Loss: 0.1721\n",
      "Epoch [19/20], Step [12000/12299],final_Loss: 0.4680\n",
      "Epoch [19/20], Step [12100/12299],final_Loss: 0.2097\n",
      "Epoch [19/20], Step [12200/12299],final_Loss: 0.3233\n",
      "Epoch [20/20], Step [100/12299],final_Loss: 0.2649\n",
      "Epoch [20/20], Step [200/12299],final_Loss: 1.8656\n",
      "Epoch [20/20], Step [300/12299],final_Loss: 0.2244\n",
      "Epoch [20/20], Step [400/12299],final_Loss: 0.4987\n",
      "Epoch [20/20], Step [500/12299],final_Loss: 0.2148\n",
      "Epoch [20/20], Step [600/12299],final_Loss: 0.1735\n",
      "Epoch [20/20], Step [700/12299],final_Loss: 19.2064\n",
      "Epoch [20/20], Step [800/12299],final_Loss: 0.1485\n",
      "Epoch [20/20], Step [900/12299],final_Loss: 0.1241\n",
      "Epoch [20/20], Step [1000/12299],final_Loss: 0.6430\n",
      "Epoch [20/20], Step [1100/12299],final_Loss: 0.1919\n",
      "Epoch [20/20], Step [1200/12299],final_Loss: 0.4532\n",
      "Epoch [20/20], Step [1300/12299],final_Loss: 0.3858\n",
      "Epoch [20/20], Step [1400/12299],final_Loss: 0.1869\n",
      "Epoch [20/20], Step [1500/12299],final_Loss: 1.3464\n",
      "Epoch [20/20], Step [1600/12299],final_Loss: 34.5730\n",
      "Epoch [20/20], Step [1700/12299],final_Loss: 17.0509\n",
      "Epoch [20/20], Step [1800/12299],final_Loss: 0.1336\n",
      "Epoch [20/20], Step [1900/12299],final_Loss: 0.5943\n",
      "Epoch [20/20], Step [2000/12299],final_Loss: 8.4422\n",
      "Epoch [20/20], Step [2100/12299],final_Loss: 1.4906\n",
      "Epoch [20/20], Step [2200/12299],final_Loss: 0.4018\n",
      "Epoch [20/20], Step [2300/12299],final_Loss: 0.8635\n",
      "Epoch [20/20], Step [2400/12299],final_Loss: 0.3262\n",
      "Epoch [20/20], Step [2500/12299],final_Loss: 0.3901\n",
      "Epoch [20/20], Step [2600/12299],final_Loss: 0.3531\n",
      "Epoch [20/20], Step [2700/12299],final_Loss: 0.7829\n",
      "Epoch [20/20], Step [2800/12299],final_Loss: 0.7483\n",
      "Epoch [20/20], Step [2900/12299],final_Loss: 0.2471\n",
      "Epoch [20/20], Step [3000/12299],final_Loss: 0.6976\n",
      "Epoch [20/20], Step [3100/12299],final_Loss: 1.2894\n",
      "Epoch [20/20], Step [3200/12299],final_Loss: 0.6626\n",
      "Epoch [20/20], Step [3300/12299],final_Loss: 8.6236\n",
      "Epoch [20/20], Step [3400/12299],final_Loss: 0.2395\n",
      "Epoch [20/20], Step [3500/12299],final_Loss: 0.4179\n",
      "Epoch [20/20], Step [3600/12299],final_Loss: 1.2967\n",
      "Epoch [20/20], Step [3700/12299],final_Loss: 0.8841\n",
      "Epoch [20/20], Step [3800/12299],final_Loss: 0.2847\n",
      "Epoch [20/20], Step [3900/12299],final_Loss: 0.3124\n",
      "Epoch [20/20], Step [4000/12299],final_Loss: 3.9352\n",
      "Epoch [20/20], Step [4100/12299],final_Loss: 0.4151\n",
      "Epoch [20/20], Step [4200/12299],final_Loss: 28.9246\n",
      "Epoch [20/20], Step [4300/12299],final_Loss: 1.1005\n",
      "Epoch [20/20], Step [4400/12299],final_Loss: 0.5581\n",
      "Epoch [20/20], Step [4500/12299],final_Loss: 0.3899\n",
      "Epoch [20/20], Step [4600/12299],final_Loss: 0.5339\n",
      "Epoch [20/20], Step [4700/12299],final_Loss: 1.3101\n",
      "Epoch [20/20], Step [4800/12299],final_Loss: 4.6465\n",
      "Epoch [20/20], Step [4900/12299],final_Loss: 5.5894\n",
      "Epoch [20/20], Step [5000/12299],final_Loss: 0.2831\n",
      "Epoch [20/20], Step [5100/12299],final_Loss: 5.4448\n",
      "Epoch [20/20], Step [5200/12299],final_Loss: 1.2852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/20], Step [5300/12299],final_Loss: 0.7105\n",
      "Epoch [20/20], Step [5400/12299],final_Loss: 10.2931\n",
      "Epoch [20/20], Step [5500/12299],final_Loss: 2.0117\n",
      "Epoch [20/20], Step [5600/12299],final_Loss: 0.6150\n",
      "Epoch [20/20], Step [5700/12299],final_Loss: 9.7616\n",
      "Epoch [20/20], Step [5800/12299],final_Loss: 1.1260\n",
      "Epoch [20/20], Step [5900/12299],final_Loss: 1.1717\n",
      "Epoch [20/20], Step [6000/12299],final_Loss: 3.9286\n",
      "Epoch [20/20], Step [6100/12299],final_Loss: 2.1518\n",
      "Epoch [20/20], Step [6200/12299],final_Loss: 0.9949\n",
      "Epoch [20/20], Step [6300/12299],final_Loss: 0.3033\n",
      "Epoch [20/20], Step [6400/12299],final_Loss: 0.4534\n",
      "Epoch [20/20], Step [6500/12299],final_Loss: 0.3875\n",
      "Epoch [20/20], Step [6600/12299],final_Loss: 0.9518\n",
      "Epoch [20/20], Step [6700/12299],final_Loss: 0.7758\n",
      "Epoch [20/20], Step [6800/12299],final_Loss: 0.5367\n",
      "Epoch [20/20], Step [6900/12299],final_Loss: 0.9311\n",
      "Epoch [20/20], Step [7000/12299],final_Loss: 0.4114\n",
      "Epoch [20/20], Step [7100/12299],final_Loss: 0.6382\n",
      "Epoch [20/20], Step [7200/12299],final_Loss: 0.2288\n",
      "Epoch [20/20], Step [7300/12299],final_Loss: 0.6614\n",
      "Epoch [20/20], Step [7400/12299],final_Loss: 2.3016\n",
      "Epoch [20/20], Step [7500/12299],final_Loss: 2.5115\n",
      "Epoch [20/20], Step [7600/12299],final_Loss: 1.2767\n",
      "Epoch [20/20], Step [7700/12299],final_Loss: 2.8306\n",
      "Epoch [20/20], Step [7800/12299],final_Loss: 27.6773\n",
      "Epoch [20/20], Step [7900/12299],final_Loss: 0.5659\n",
      "Epoch [20/20], Step [8000/12299],final_Loss: 0.5698\n",
      "Epoch [20/20], Step [8100/12299],final_Loss: 0.3724\n",
      "Epoch [20/20], Step [8200/12299],final_Loss: 1.0359\n",
      "Epoch [20/20], Step [8300/12299],final_Loss: 0.8491\n",
      "Epoch [20/20], Step [8400/12299],final_Loss: 1.1463\n",
      "Epoch [20/20], Step [8500/12299],final_Loss: 0.1680\n",
      "Epoch [20/20], Step [8600/12299],final_Loss: 0.4755\n",
      "Epoch [20/20], Step [8700/12299],final_Loss: 0.1936\n",
      "Epoch [20/20], Step [8800/12299],final_Loss: 0.2030\n",
      "Epoch [20/20], Step [8900/12299],final_Loss: 0.6239\n",
      "Epoch [20/20], Step [9000/12299],final_Loss: 23.1386\n",
      "Epoch [20/20], Step [9100/12299],final_Loss: 2.5959\n",
      "Epoch [20/20], Step [9200/12299],final_Loss: 0.8680\n",
      "Epoch [20/20], Step [9300/12299],final_Loss: 2.8708\n",
      "Epoch [20/20], Step [9400/12299],final_Loss: 17.2695\n",
      "Epoch [20/20], Step [9500/12299],final_Loss: 0.2455\n",
      "Epoch [20/20], Step [9600/12299],final_Loss: 0.8440\n",
      "Epoch [20/20], Step [9700/12299],final_Loss: 0.2180\n",
      "Epoch [20/20], Step [9800/12299],final_Loss: 0.1186\n",
      "Epoch [20/20], Step [9900/12299],final_Loss: 0.0937\n",
      "Epoch [20/20], Step [10000/12299],final_Loss: 0.7592\n",
      "Epoch [20/20], Step [10100/12299],final_Loss: 18.3568\n",
      "Epoch [20/20], Step [10200/12299],final_Loss: 0.3444\n",
      "Epoch [20/20], Step [10300/12299],final_Loss: 4.9376\n",
      "Epoch [20/20], Step [10400/12299],final_Loss: 0.3556\n",
      "Epoch [20/20], Step [10500/12299],final_Loss: 19.6450\n",
      "Epoch [20/20], Step [10600/12299],final_Loss: 0.3449\n",
      "Epoch [20/20], Step [10700/12299],final_Loss: 2.5281\n",
      "Epoch [20/20], Step [10800/12299],final_Loss: 0.2895\n",
      "Epoch [20/20], Step [10900/12299],final_Loss: 1.0023\n",
      "Epoch [20/20], Step [11000/12299],final_Loss: 5.6726\n",
      "Epoch [20/20], Step [11100/12299],final_Loss: 0.8809\n",
      "Epoch [20/20], Step [11200/12299],final_Loss: 3.5596\n",
      "Epoch [20/20], Step [11300/12299],final_Loss: 0.6279\n",
      "Epoch [20/20], Step [11400/12299],final_Loss: 0.3475\n",
      "Epoch [20/20], Step [11500/12299],final_Loss: 0.5513\n",
      "Epoch [20/20], Step [11600/12299],final_Loss: 0.6308\n",
      "Epoch [20/20], Step [11700/12299],final_Loss: 0.2422\n",
      "Epoch [20/20], Step [11800/12299],final_Loss: 6.7019\n",
      "Epoch [20/20], Step [11900/12299],final_Loss: 0.2165\n",
      "Epoch [20/20], Step [12000/12299],final_Loss: 0.4488\n",
      "Epoch [20/20], Step [12100/12299],final_Loss: 0.2044\n",
      "Epoch [20/20], Step [12200/12299],final_Loss: 0.2246\n"
     ]
    }
   ],
   "source": [
    "npz_x=[]\n",
    "npz_loss=[]\n",
    "#training step\n",
    "total_step=len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i,(image,label) in enumerate(train_loader):\n",
    "        image=image.cuda()\n",
    "        label=label.cuda()\n",
    "        ss_output,is_output1,is_output2,ds_output,output=model(image)\n",
    "      \n",
    "        ss_loss=criterion(ss_output,label)\n",
    "        is_loss1=criterion(is_output1,label)\n",
    "        is_loss2=criterion(is_output2,label)\n",
    "        ds_loss=criterion(ds_output,label)\n",
    "        final_loss=ss_loss+is_loss1+is_loss2+ds_loss+criterion(output,label)\n",
    "        #backward and optimize\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        final_loss.backward()\n",
    "        optimizer.step()\n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}],final_Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, final_loss.item()))\n",
    "    if(epoch%3==0):\n",
    "        #save training result\n",
    "        torch.save(model.state_dict(), './save_data/total_model.ckpt')\n",
    "        npz_x.append(epoch)\n",
    "        npz_loss.append(final_loss)\n",
    "        np.savez(\"./save_data/total_model_loss.npz\",x=npz_x,y=npz_loss)\n",
    "        \n",
    "np.savez(\"./save_data/total_model_loss.npz\",x=npz_x,y=npz_loss)\n",
    "#save training result\n",
    "torch.save(model.state_dict(), './save_data/total_model.ckpt')     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nhs\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\nhs\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 21.22123749898366 %\n"
     ]
    }
   ],
   "source": [
    "#image accuracy \n",
    "model.load_state_dict(torch.load('./save_data/total_model.ckpt'))\n",
    "correct = 0\n",
    "total = 0\n",
    "predict=[]\n",
    "for images, labels in test_loader:\n",
    "    images = images.cuda()\n",
    "    labels = labels.cuda()\n",
    "    _,_,_,_,outputs = model(images)\n",
    "    predicted = torch.max(softmax(outputs), 1)\n",
    "    predict_tensor=softmax(outputs)\n",
    "    predict_numpy=predict_tensor.cpu().detach().numpy()\n",
    "    predict.append(predict_numpy)\n",
    "#     print(predicted[1],labels)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted[1] == labels).sum().item()\n",
    "print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "dirr=\"D:/emotion_dataset/mmidataset/VideoWithImageLabels/face_detected/test/labels.csv\"\n",
    "f_read=open(dirr,'r',encoding='utf-8')\n",
    "reader=csv.reader(f_read)\n",
    "image_number=[]\n",
    "image_label=[]\n",
    "for line in reader:\n",
    "    \n",
    "    indexNo_start = line[0].find('image')\n",
    "    image_start=line[0][indexNo_start+5:]\n",
    "    image_label.append(line[1])\n",
    "    indexNo_finish = image_start.find('_')\n",
    "    image_number.append(int(image_start[:indexNo_finish]))\n",
    "f_read.close()\n",
    "image_number.pop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12299\n",
      "155\n",
      "155\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# \n",
    "cnt=1\n",
    "for i in range(len(image_number)):\n",
    "    if(i<len(image_number)-1):\n",
    "        if(image_number[i]!=image_number[i+1]):\n",
    "            \n",
    "            cnt+=1\n",
    "cnt+=1\n",
    "print(len(image_number))\n",
    "print(cnt)\n",
    "label_cnt=[]\n",
    "#image label,    csv \n",
    "# f = open('predict_label_cnt.csv', 'w', encoding='utf-8', newline='')\n",
    "# wr = csv.writer(f)\n",
    "# \n",
    "avicnt=1\n",
    "for i in range(len(image_number)):\n",
    "    if(i<len(image_number)):\n",
    "#         wr.writerow([image_label[i],avicnt])\n",
    "        label_cnt.append([image_label[i],avicnt])\n",
    "        if(i!=(len(image_number)-1) and image_number[i]!=image_number[i+1]):\n",
    "            avicnt+=1\n",
    "        \n",
    "avicnt+=1\n",
    "# f.close()\n",
    "print(avicnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#csv   predict score    \n",
    "#    ,   .\n",
    "# f = open('predict_label_cnt.csv', 'r', encoding='utf-8')\n",
    "# rdr=csv.reader(f)\n",
    "# \n",
    "prediction_score=0\n",
    "# lines=[]\n",
    "# for line in rdr:\n",
    "#     lines.append([int(line[0]),int(line[1])])\n",
    "# f.close()\n",
    "arr=[]\n",
    "for i in range(len(predict)):\n",
    "    arr.append([predict[i],label_cnt[i][0],label_cnt[i][1]])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not Correct!! -->avi_predict :  3 ,label :  1 ,cnt :  1\n",
      "not Correct!! -->avi_predict :  3 ,label :  1 ,cnt :  2\n",
      "not Correct!! -->avi_predict :  4 ,label :  2 ,cnt :  3\n",
      "not Correct!! -->avi_predict :  4 ,label :  3 ,cnt :  4\n",
      "Correct!! -->avi_predict :  3 ,label :  3 ,cnt :  5\n",
      "not Correct!! -->avi_predict :  3 ,label :  4 ,cnt :  6\n",
      "Correct!! -->avi_predict :  4 ,label :  4 ,cnt :  7\n",
      "not Correct!! -->avi_predict :  4 ,label :  5 ,cnt :  8\n",
      "not Correct!! -->avi_predict :  3 ,label :  5 ,cnt :  9\n",
      "not Correct!! -->avi_predict :  3 ,label :  6 ,cnt :  10\n",
      "not Correct!! -->avi_predict :  4 ,label :  6 ,cnt :  11\n",
      "not Correct!! -->avi_predict :  4 ,label :  2 ,cnt :  12\n",
      "not Correct!! -->avi_predict :  4 ,label :  2 ,cnt :  13\n",
      "not Correct!! -->avi_predict :  4 ,label :  3 ,cnt :  14\n",
      "not Correct!! -->avi_predict :  5 ,label :  4 ,cnt :  15\n",
      "Correct!! -->avi_predict :  4 ,label :  4 ,cnt :  16\n",
      "not Correct!! -->avi_predict :  4 ,label :  1 ,cnt :  17\n",
      "not Correct!! -->avi_predict :  4 ,label :  3 ,cnt :  18\n",
      "not Correct!! -->avi_predict :  5 ,label :  3 ,cnt :  19\n",
      "not Correct!! -->avi_predict :  4 ,label :  6 ,cnt :  20\n",
      "not Correct!! -->avi_predict :  4 ,label :  6 ,cnt :  21\n",
      "not Correct!! -->avi_predict :  4 ,label :  2 ,cnt :  22\n",
      "not Correct!! -->avi_predict :  4 ,label :  3 ,cnt :  23\n",
      "Correct!! -->avi_predict :  4 ,label :  4 ,cnt :  24\n",
      "Correct!! -->avi_predict :  4 ,label :  4 ,cnt :  25\n",
      "not Correct!! -->avi_predict :  4 ,label :  6 ,cnt :  26\n",
      "not Correct!! -->avi_predict :  4 ,label :  6 ,cnt :  27\n",
      "not Correct!! -->avi_predict :  4 ,label :  1 ,cnt :  28\n",
      "not Correct!! -->avi_predict :  4 ,label :  2 ,cnt :  29\n",
      "not Correct!! -->avi_predict :  4 ,label :  2 ,cnt :  30\n",
      "not Correct!! -->avi_predict :  4 ,label :  3 ,cnt :  31\n",
      "Correct!! -->avi_predict :  4 ,label :  4 ,cnt :  32\n",
      "Correct!! -->avi_predict :  4 ,label :  4 ,cnt :  33\n",
      "not Correct!! -->avi_predict :  4 ,label :  5 ,cnt :  34\n",
      "not Correct!! -->avi_predict :  4 ,label :  5 ,cnt :  35\n",
      "not Correct!! -->avi_predict :  4 ,label :  5 ,cnt :  36\n",
      "not Correct!! -->avi_predict :  4 ,label :  6 ,cnt :  37\n",
      "not Correct!! -->avi_predict :  4 ,label :  6 ,cnt :  38\n",
      "not Correct!! -->avi_predict :  4 ,label :  1 ,cnt :  39\n",
      "not Correct!! -->avi_predict :  4 ,label :  1 ,cnt :  40\n",
      "not Correct!! -->avi_predict :  4 ,label :  2 ,cnt :  41\n",
      "not Correct!! -->avi_predict :  4 ,label :  3 ,cnt :  42\n",
      "not Correct!! -->avi_predict :  4 ,label :  3 ,cnt :  43\n",
      "Correct!! -->avi_predict :  4 ,label :  4 ,cnt :  44\n",
      "Correct!! -->avi_predict :  4 ,label :  4 ,cnt :  45\n",
      "not Correct!! -->avi_predict :  4 ,label :  5 ,cnt :  46\n",
      "not Correct!! -->avi_predict :  4 ,label :  6 ,cnt :  47\n",
      "not Correct!! -->avi_predict :  4 ,label :  1 ,cnt :  48\n",
      "not Correct!! -->avi_predict :  4 ,label :  2 ,cnt :  49\n",
      "not Correct!! -->avi_predict :  4 ,label :  6 ,cnt :  50\n",
      "not Correct!! -->avi_predict :  4 ,label :  1 ,cnt :  51\n",
      "not Correct!! -->avi_predict :  4 ,label :  6 ,cnt :  52\n",
      "Correct!! -->avi_predict :  4 ,label :  4 ,cnt :  53\n",
      "not Correct!! -->avi_predict :  4 ,label :  2 ,cnt :  54\n",
      "Correct!! -->avi_predict :  4 ,label :  4 ,cnt :  55\n",
      "not Correct!! -->avi_predict :  4 ,label :  5 ,cnt :  56\n",
      "not Correct!! -->avi_predict :  4 ,label :  5 ,cnt :  57\n",
      "not Correct!! -->avi_predict :  4 ,label :  6 ,cnt :  58\n",
      "not Correct!! -->avi_predict :  4 ,label :  6 ,cnt :  59\n",
      "not Correct!! -->avi_predict :  4 ,label :  6 ,cnt :  60\n",
      "not Correct!! -->avi_predict :  4 ,label :  1 ,cnt :  61\n",
      "not Correct!! -->avi_predict :  4 ,label :  3 ,cnt :  62\n",
      "Correct!! -->avi_predict :  4 ,label :  4 ,cnt :  63\n",
      "Correct!! -->avi_predict :  4 ,label :  4 ,cnt :  64\n",
      "not Correct!! -->avi_predict :  4 ,label :  5 ,cnt :  65\n",
      "not Correct!! -->avi_predict :  4 ,label :  5 ,cnt :  66\n",
      "not Correct!! -->avi_predict :  4 ,label :  6 ,cnt :  67\n",
      "not Correct!! -->avi_predict :  4 ,label :  1 ,cnt :  68\n",
      "not Correct!! -->avi_predict :  4 ,label :  2 ,cnt :  69\n",
      "not Correct!! -->avi_predict :  4 ,label :  3 ,cnt :  70\n",
      "not Correct!! -->avi_predict :  4 ,label :  3 ,cnt :  71\n",
      "Correct!! -->avi_predict :  4 ,label :  4 ,cnt :  72\n",
      "not Correct!! -->avi_predict :  4 ,label :  5 ,cnt :  73\n",
      "not Correct!! -->avi_predict :  4 ,label :  6 ,cnt :  74\n",
      "not Correct!! -->avi_predict :  4 ,label :  6 ,cnt :  75\n",
      "not Correct!! -->avi_predict :  4 ,label :  2 ,cnt :  76\n",
      "not Correct!! -->avi_predict :  4 ,label :  1 ,cnt :  77\n",
      "not Correct!! -->avi_predict :  4 ,label :  2 ,cnt :  78\n",
      "not Correct!! -->avi_predict :  4 ,label :  3 ,cnt :  79\n",
      "not Correct!! -->avi_predict :  4 ,label :  3 ,cnt :  80\n",
      "Correct!! -->avi_predict :  4 ,label :  4 ,cnt :  81\n",
      "not Correct!! -->avi_predict :  4 ,label :  5 ,cnt :  82\n",
      "not Correct!! -->avi_predict :  4 ,label :  1 ,cnt :  83\n",
      "not Correct!! -->avi_predict :  3 ,label :  1 ,cnt :  84\n",
      "not Correct!! -->avi_predict :  3 ,label :  1 ,cnt :  85\n",
      "not Correct!! -->avi_predict :  4 ,label :  6 ,cnt :  86\n",
      "not Correct!! -->avi_predict :  4 ,label :  1 ,cnt :  87\n",
      "not Correct!! -->avi_predict :  3 ,label :  1 ,cnt :  88\n",
      "not Correct!! -->avi_predict :  4 ,label :  6 ,cnt :  89\n",
      "not Correct!! -->avi_predict :  4 ,label :  1 ,cnt :  90\n",
      "Correct!! -->avi_predict :  4 ,label :  4 ,cnt :  91\n",
      "not Correct!! -->avi_predict :  4 ,label :  5 ,cnt :  92\n",
      "not Correct!! -->avi_predict :  4 ,label :  6 ,cnt :  93\n",
      "Correct!! -->avi_predict :  4 ,label :  4 ,cnt :  94\n",
      "Correct!! -->avi_predict :  4 ,label :  4 ,cnt :  95\n",
      "not Correct!! -->avi_predict :  4 ,label :  5 ,cnt :  96\n",
      "not Correct!! -->avi_predict :  4 ,label :  2 ,cnt :  97\n",
      "not Correct!! -->avi_predict :  4 ,label :  2 ,cnt :  98\n",
      "not Correct!! -->avi_predict :  4 ,label :  6 ,cnt :  99\n",
      "not Correct!! -->avi_predict :  4 ,label :  3 ,cnt :  100\n",
      "Correct!! -->avi_predict :  4 ,label :  4 ,cnt :  101\n",
      "not Correct!! -->avi_predict :  4 ,label :  1 ,cnt :  102\n",
      "not Correct!! -->avi_predict :  4 ,label :  1 ,cnt :  103\n",
      "not Correct!! -->avi_predict :  4 ,label :  2 ,cnt :  104\n",
      "not Correct!! -->avi_predict :  4 ,label :  2 ,cnt :  105\n",
      "not Correct!! -->avi_predict :  4 ,label :  3 ,cnt :  106\n",
      "Correct!! -->avi_predict :  4 ,label :  4 ,cnt :  107\n",
      "Correct!! -->avi_predict :  4 ,label :  4 ,cnt :  108\n",
      "not Correct!! -->avi_predict :  4 ,label :  5 ,cnt :  109\n",
      "not Correct!! -->avi_predict :  4 ,label :  5 ,cnt :  110\n",
      "not Correct!! -->avi_predict :  4 ,label :  6 ,cnt :  111\n",
      "not Correct!! -->avi_predict :  4 ,label :  1 ,cnt :  112\n",
      "not Correct!! -->avi_predict :  4 ,label :  1 ,cnt :  113\n",
      "not Correct!! -->avi_predict :  4 ,label :  1 ,cnt :  114\n",
      "not Correct!! -->avi_predict :  4 ,label :  1 ,cnt :  115\n",
      "not Correct!! -->avi_predict :  4 ,label :  3 ,cnt :  116\n",
      "not Correct!! -->avi_predict :  4 ,label :  6 ,cnt :  117\n",
      "not Correct!! -->avi_predict :  4 ,label :  3 ,cnt :  118\n",
      "Correct!! -->avi_predict :  4 ,label :  4 ,cnt :  119\n",
      "not Correct!! -->avi_predict :  5 ,label :  4 ,cnt :  120\n",
      "Correct!! -->avi_predict :  5 ,label :  5 ,cnt :  121\n",
      "Correct!! -->avi_predict :  5 ,label :  5 ,cnt :  122\n",
      "Correct!! -->avi_predict :  5 ,label :  5 ,cnt :  123\n",
      "not Correct!! -->avi_predict :  4 ,label :  5 ,cnt :  124\n",
      "not Correct!! -->avi_predict :  4 ,label :  5 ,cnt :  125\n",
      "not Correct!! -->avi_predict :  4 ,label :  6 ,cnt :  126\n",
      "not Correct!! -->avi_predict :  4 ,label :  6 ,cnt :  127\n",
      "not Correct!! -->avi_predict :  5 ,label :  1 ,cnt :  128\n",
      "not Correct!! -->avi_predict :  5 ,label :  1 ,cnt :  129\n",
      "not Correct!! -->avi_predict :  4 ,label :  2 ,cnt :  130\n",
      "Correct!! -->avi_predict :  4 ,label :  4 ,cnt :  131\n",
      "not Correct!! -->avi_predict :  5 ,label :  4 ,cnt :  132\n",
      "Correct!! -->avi_predict :  5 ,label :  5 ,cnt :  133\n",
      "not Correct!! -->avi_predict :  1 ,label :  5 ,cnt :  134\n",
      "Correct!! -->avi_predict :  6 ,label :  6 ,cnt :  135\n",
      "not Correct!! -->avi_predict :  4 ,label :  3 ,cnt :  136\n",
      "not Correct!! -->avi_predict :  4 ,label :  1 ,cnt :  137\n",
      "not Correct!! -->avi_predict :  4 ,label :  2 ,cnt :  138\n",
      "not Correct!! -->avi_predict :  4 ,label :  2 ,cnt :  139\n",
      "not Correct!! -->avi_predict :  4 ,label :  1 ,cnt :  140\n",
      "Correct!! -->avi_predict :  1 ,label :  1 ,cnt :  141\n",
      "Correct!! -->avi_predict :  1 ,label :  1 ,cnt :  142\n",
      "not Correct!! -->avi_predict :  4 ,label :  6 ,cnt :  143\n",
      "not Correct!! -->avi_predict :  4 ,label :  6 ,cnt :  144\n",
      "not Correct!! -->avi_predict :  4 ,label :  1 ,cnt :  145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not Correct!! -->avi_predict :  4 ,label :  1 ,cnt :  146\n",
      "not Correct!! -->avi_predict :  4 ,label :  1 ,cnt :  147\n",
      "not Correct!! -->avi_predict :  4 ,label :  1 ,cnt :  148\n",
      "not Correct!! -->avi_predict :  4 ,label :  2 ,cnt :  149\n",
      "not Correct!! -->avi_predict :  4 ,label :  3 ,cnt :  150\n",
      "not Correct!! -->avi_predict :  4 ,label :  3 ,cnt :  151\n",
      "Correct!! -->avi_predict :  4 ,label :  4 ,cnt :  152\n",
      "Correct!! -->avi_predict :  4 ,label :  4 ,cnt :  153\n",
      "Correct!! -->avi_predict :  4 ,label :  4 ,cnt :  154\n",
      "total :  155 ,correct :  33 ,not correct :  121\n",
      "total avi files :  154\n",
      "accuracy :  21.29032258064516\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "cnt=1\n",
    "frame_cnt=0\n",
    "avi_sum=0\n",
    "avi_predict=[]\n",
    "pt_correct=0\n",
    "pt_notcorrect=0\n",
    "for i in range(len(arr)):\n",
    "    \n",
    "    if(cnt!=arr[i][2] and cnt==arr[i-1][2]):\n",
    "        avi_sum=avi_sum/frame_cnt\n",
    "        avi_predict.append(avi_sum)\n",
    "        if(np.argmax(avi_sum)==int(arr[i-1][1])):\n",
    "            print(\"Correct!! -->avi_predict : \",np.argmax(avi_sum),\",label : \",arr[i-1][1],\",cnt : \",cnt)\n",
    "            pt_correct+=1\n",
    "        else:\n",
    "            print(\"not Correct!! -->avi_predict : \",np.argmax(avi_sum),\",label : \",arr[i-1][1],\",cnt : \",cnt)\n",
    "            pt_notcorrect+=1\n",
    "        avi_sum=0\n",
    "        cnt+=1\n",
    "        frame_cnt=0\n",
    "        \n",
    "    else:\n",
    "        avi_sum+=arr[i][0]\n",
    "        frame_cnt+=1\n",
    "frame_cnt=0\n",
    "avi_sum=0\n",
    "for i in range(len(arr)):\n",
    "    if (arr[i][2]==avicnt-1):\n",
    "        avi_sum+=arr[i][0]\n",
    "        frame_cnt+=1\n",
    "avi_sum=avi_sum/frame_cnt\n",
    "avi_predict.append(avi_sum)\n",
    "if(np.argmax(avi_sum)==int(arr[len(arr)-1][1])):\n",
    "    print(\"Correct!! -->avi_predict : \",np.argmax(avi_sum),\",label : \",arr[len(arr)-1][1],\",cnt : \",cnt)\n",
    "    pt_correct+=1\n",
    "else:\n",
    "    print(\"not Correct!! -->avi_predict : \",np.argmax(avi_sum),\",label : \",arr[len(arr)-1][1],\",cnt : \",cnt)\n",
    "    pt_notcorrect+=1\n",
    "# #  predict \n",
    "# cnt=1\n",
    "# correct = 0\n",
    "# for i in range(len(arr)):\n",
    "#     if(cnt!=arr[i][2] and cnt==arr[i-1][2]):\n",
    "# #         print(np.argmax(avi_predict[cnt]),arr[i-1][1])\n",
    "#         if(np.argmax(avi_predict[cnt])==arr[i-1][1]):\n",
    "#              correct+=1\n",
    "#         cnt+=1\n",
    "\n",
    "# for i in range(len(arr)):\n",
    "#     if (arr[i][2]==208):\n",
    "# #         print((np.argmax(avi_predict[cnt-1]),arr[i][1]))\n",
    "#         if(np.argmax(avi_predict[cnt-1])==arr[i][1]):\n",
    "#             correct+=1\n",
    "#         break\n",
    "total = avicnt\n",
    "print(\"total : \",total,\",correct : \",pt_correct,\",not correct : \",pt_notcorrect)\n",
    "print(\"total avi files : \",cnt)\n",
    "print(\"accuracy : \",pt_correct/total*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
